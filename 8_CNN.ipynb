{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "Convolutional Neural Network takes the features within convolutional layers and pooling layers, so it could presume more features and information.\n",
    "\n",
    "## Convolutional Layer\n",
    "In a convolutional layer, a window would travel through the data, which is in the matrix form. Such window, which is also called __Kernel__, has weights on each unit. And there are usually several kernels at one layer, producing a group of new matrices or convolutions. This is the well known __Tensor__. We usually choose the kernels which have the same shape.  \n",
    "Here, we should point out that the convolution of two matrix shall be\n",
    "$$\n",
    "    [A*K]_{ij} = \\sum\\limits_{m=0}^{p-1}\\sum\\limits_{n=0}^{q-1} A_{i+m,j+n}K_{p-m,q-n}\n",
    "$$\n",
    "where we need to flip $K$ and do the element-wise multiplication, then sum them.\n",
    "\n",
    "## Pooling Layer\n",
    "This layer reduces the spatial size but remains main features. A cat is still a cat if we move the picture right for several pixels. Classical pooling layers are max pooling layer and average pooling layer.\n",
    "\n",
    "## Fully Connected Layer\n",
    "After processed with convolutional layer and pooling layers, we would use the fully connected layer, which is what we have in the former learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propagation and Backpropagation\n",
    "\n",
    "CNN is also a feedforwad NN. So we need to process two main parts in one epoch, which is forward propagation and backward propagation. We choose a specific network here, whose structure is:\n",
    "<img src=\"pic/CNN.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propagation\n",
    "We mainly analyse two layers here:\n",
    " - Convolutional Layer  \n",
    "     Let $*$ denote the convolution operator. We could define the multiplication over the tensors\n",
    "     $$\n",
    "         \\mathrm{vec}(K*A) \\triangleq [K_T^{(1)}, \\cdots, K_T^{(n)}]A \\triangleq [K_T^{(1)}A, \\cdots, K_T^{(n)}A]\n",
    "     $$\n",
    "     here $K_T^{(1)}$ is $K$ filled with 0 in other position which looks like $\\left[\\begin{matrix}K & O\\\\ O & O\\end{matrix}\\right]$. And $K_T^{(i)}A$ is the sum of elementwise multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Pooling Layer  \n",
    " Pooling layer could be regarded as a special convolutional layer with a stride length. We usually use the _max pooling_ and _average pooling_. Formally, we could have\n",
    "$$\n",
    "    a = z = A * K\n",
    "$$\n",
    "without a bias $b$ and activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "- Convolutional Layer  \n",
    "In the former analysis of normal fully connected NN, we use matrix form to describe the derivatives, without facing too much trouble. This is because that $w_{ij}$ in every layer is independent to each other, and our chain rules have no intersection. However, in CNN, the kernel would be used for several times, that the derivative of $K$ would be a sum of several other derivatives\n",
    "$$\n",
    "    \\frac{\\partial L}{\\partial K_{ij}} = \\sum\\limits_k \\frac{\\partial L}{\\partial z_k}\\frac{\\partial z_k}{\\partial K_{ij}} = \\mathrm{dot}\\left(\\frac{\\partial L}{\\partial z}, \\frac{\\partial z}{\\partial K_{ij}}\\right) \\\\\n",
    "    \\frac{\\partial L}{\\partial K} = A[*]\\frac{\\partial L}{\\partial z} \\\\\n",
    "    \\nabla_K L = \\mathrm{flip}\\left(\\frac{\\partial L}{\\partial K}\\right) = \\mathrm{flip}\\left(A[*]\\frac{\\partial L}{\\partial z}\\right) = A * \\mathrm{flip}\\left(\\frac{\\partial L}{\\partial z}\\right)\n",
    "$$\n",
    "here\n",
    "$$\n",
    "    z = A*K + b,\\ a = \\sigma(z)\n",
    "$$\n",
    "then\n",
    "$$\n",
    "    \\frac{\\partial L}{\\partial b} = \\mathrm{dot}\\left(\\frac{\\partial L}{\\partial z}, J\\right)\n",
    "$$\n",
    "where $J$ is matrix of ones.  \n",
    "Also, with our need to backpropagate the derivative, we need $\\frac{\\partial L}{\\partial A}$  \n",
    "in conclusion, we have\n",
    "$$\n",
    "    \\frac{\\partial L}{\\partial A} = \n",
    "        \\left[\\begin{matrix}\n",
    "            0 & O & 0 \\\\\n",
    "            O & \\frac{\\partial L}{\\partial z} & O \\\\\n",
    "            0 & O & 0\n",
    "        \\end{matrix}\\right] [*] K \\\\\n",
    "        \\triangleq \\frac{\\partial L}{\\partial z} \\left<*\\right> K\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pooling Layer  \n",
    " Because we have the similar structure as traditional convolutional layers, we could take even the same backpropagations for common pooling layer. While we only use such few pooling layers, so I'd like to get the detail of max pooling and average pooling, in backpropagation.\n",
    "    - max pooling  \n",
    "    Assume we have\n",
    "    $$\n",
    "        [A*K]_{ij} = A_{p_iq_j}\n",
    "    $$\n",
    "    then, we would not have to compute $\\frac{\\partial L}{\\partial K}$, which is meaningless, but only need $\\frac{\\partial L}{\\partial A}$. So\n",
    "    $$\n",
    "        \\nabla_A L = \\nabla_{z} L\\nabla_A {z} \\\\\n",
    "        \\nabla_{A_{p_iq_j}} L = \\nabla_{z_{ij}} L \n",
    "    $$\n",
    "    - average pooling  \n",
    "    because of the averaging, $K=J$, which is exactly what we get in convolutional layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that in our codes implementation, _dot()_ get the element-wise multiplication and sum them, and we wouldn't need to write the convolution function to compute, so here we adjust our content to fit the codes implementation.\n",
    "\n",
    "__In forward propagation__  \n",
    "$$\n",
    "z = A*K + b,\\ a = \\sigma(z)\n",
    "$$\n",
    "where the convolution is replaced by what we have said above.\n",
    "\n",
    "__In backward propagation__\n",
    "$$\n",
    "\\nabla_K L = A*\\nabla_z L\\\\\n",
    "\\nabla_b L = \\mathrm{dot}\\left(\\frac{\\partial L}{\\partial z}, J\\right) \\\\\n",
    "\\nabla_A L = \\left(\\frac{\\partial L}{\\partial z}, J\\right) * \\mathrm{flip}(K)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.2",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
