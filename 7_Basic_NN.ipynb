{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic NN\n",
    "\n",
    "This IJulia shows the basic concept and application with neural network, which has some matrix calculus.\n",
    "\n",
    "Firstly we set $$X = [x^{(1)}, x^{(2)}, ..., x^{(m)}]$$ as the inputted data, where $x^{i}$ is an $n$ by $1$ vector with $n$ features. Then we have an $N_{i-1}$ by $N_{i}$ vector $w^{[i]}$ to represent the _weight matrix_ at the $i$th layer and an $N_i$ by $1$ bias vector $b^{[i]}$.  \n",
    "\n",
    "Thus at each layer we could have the calculation:\n",
    "$$z^{[i]} = w^{[i]\\mathrm{T}}a^{[i-1]} + b^{[i]}\\\\a^{[i]} = \\sigma(z^{[i]})$$\n",
    "where $a^{[i]}$ is the activation ouput from layer $i$. \n",
    "\n",
    "It is all known that there are a lot of vector and matrix derivatives in NN. So we need to analyze them completely in order to get a deep comprehenson of NN.\n",
    "\n",
    "We construct a fully connected NN here: \n",
    "<center><img src=\"pic/342NN.png\" width=80%/></center>\n",
    "with 4 layers which have N, 3, 4 and 2 nodes in each layer. This may look like a binary classification applied in tasks such as picture classification(However CNN performs much better in these problems)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost/Loss Function\n",
    "\n",
    "To train a NN, we need some rules and directions to point out that how to adjust our $w$s and $b$s. And that is __Cost Funcion__ and __Loss Function__.<br/>\n",
    "Loss function is used to measure the performance of NN describing the \"distance\" bewteen the output of our NN and the true labels from dataset. Usually we would like to minimize the loss function to make NN work better. In the former NN we constructed, we have two output values. And we need a loss function $$L: \\mathcal{R}^2 \\rightarrow \\mathcal{R}$$ Here $L$ must be a functional, or it would generate a couple of conflicting NNs.\n",
    "\n",
    "Because in each layer, the weight edges are independent to each other, it's same to calculate $\\frac{\\partial L}{\\partial w^{[i]}}$, $\\frac{\\partial L}{\\partial w^{[i]}_j}$ and $\\frac{\\partial L}{\\partial w^{[i]}_{jk}}$ for updating the weights. For convience, we will using the matrix derivative to calculate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients\n",
    "\n",
    "With loss function, what we need to do in training NN is using the gradient to update weights and bias\n",
    "$$\n",
    "w^{[i]} := w^{[i]} - \\alpha\\nabla_{w^{[i]}}L \\\\\n",
    "b^{[i]} := b^{[i]} - \\alpha\\nabla_{b^{[i]}}L\n",
    "$$\n",
    "and with BGD, we could have\n",
    "$$\n",
    "w^{[i]} := w^{[i]} - \\alpha\\frac{1}{m}\\sum_{j=1}^m\\nabla_{w^{[i]}}L^{(j)} \\\\\n",
    "b^{[i]} := b^{[i]} - \\alpha\\frac{1}{m}\\sum_{j=1}^m\\nabla_{b^{[i]}}L^{(j)}\n",
    "$$\n",
    "So all we need to do is to calculate $\\nabla_{w^{[i]}}L$ and $\\nabla_{b^{[i]}}L$. With [chain rule](https://en.wikipedia.org/wiki/Chain_rule) applied, we have\n",
    "$$\\begin{align*}\n",
    "\\left(\\nabla_{w^{[3]}_j}L\\right)^\\mathrm{T}\n",
    "    &= \\frac{\\partial L}{\\partial w^{[3]}_j} \\\\\n",
    "    &= \\left(\\frac{\\partial L}{\\partial a^{[3]}}\\right)_{1\\times2}\n",
    "       \\left(\\frac{\\partial a^{[3]}}{\\partial z^{[3]}}\\right)_{2\\times2}\n",
    "       \\left(\\frac{\\partial z^{[3]}}{\\partial w^{[3]}_j}\\right)_{2\\times4} \\\\\n",
    "\\end{align*}$$\n",
    "where $w^{[3]}_j = w^{[3]}_{:,j}$. And because of independency, we could have\n",
    "$$\\begin{align*}\n",
    "\\left(\\nabla_{w^{[3]}}L\\right)^\\mathrm{T}\n",
    "    &= \\frac{\\partial L}{\\partial w^{[3]}} \\\\\n",
    "    &= \\sum_{j=1}^2(E_j)_{2\\times1}\\left(\\frac{\\partial L}{\\partial a^{[3]}}\\right)_{1\\times2}\n",
    "       \\left(\\frac{\\partial a^{[3]}}{\\partial z^{[3]}}\\right)_{2\\times2}\n",
    "       \\left(\\frac{\\partial z^{[3]}}{\\partial w^{[3]}_j}\\right)_{2\\times4} \\\\\n",
    "\\end{align*}$$\n",
    "where $E_1 = [1,0]^\\mathrm{T}, E_2 = [0,1]^\\mathrm{T}$. Samely, we could easily calculate $\\nabla_{b^{[3]}}L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a more suitable form, we need to inspect $\\nabla_{w^{[2]}}L$,\n",
    "$$\\begin{align*}\n",
    "\\left(\\nabla_{w^{[2]}_j}L\\right)^\\mathrm{T}\n",
    "    &= \\frac{\\partial L}{\\partial w^{[2]}_j} \\\\\n",
    "    &= \\left(\\frac{\\partial L}{\\partial a^{[3]}}\\right)_{1\\times2}\n",
    "       \\left(\\frac{\\partial a^{[3]}}{\\partial z^{[3]}}\\right)_{2\\times2}\n",
    "       \\left(\\frac{\\partial z^{[3]}}{\\partial a^{[2]}}\\right)_{2\\times4}\n",
    "       \\left(\\frac{\\partial a^{[2]}}{\\partial z^{[2]}}\\right)_{4\\times4}\n",
    "       \\left(\\frac{\\partial z^{[2]}}{\\partial w^{[2]}_j}\\right)_{4\\times3}\n",
    "\\end{align*}$$\n",
    "and \n",
    "$$\n",
    "\\left(\\nabla_{w^{[2]}}L\\right)^\\mathrm{T}\n",
    "    =  \\sum_{j=1}^4\\left(E_j\\right)_{4\\times1}\\left(\\frac{\\partial L}{\\partial a^{[3]}}\\right)_{1\\times2}\n",
    "       \\left(\\frac{\\partial a^{[3]}}{\\partial z^{[3]}}\\right)_{2\\times2}\n",
    "       \\left(\\frac{\\partial z^{[3]}}{\\partial a^{[2]}}\\right)_{2\\times4}\n",
    "       \\left(\\frac{\\partial a^{[2]}}{\\partial z^{[2]}}\\right)_{4\\times4}\n",
    "       \\left(\\frac{\\partial z^{[2]}}{\\partial w^{[2]}_j}\\right)_{4\\times3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization & Broadcasting\n",
    "\n",
    "We have known that the built in functions for matrix calculations perform much better than explicit loop operation in Julia, as well as Python. So we'd like to find a more matrix tasty calculation, where we need the [Vectorization](https://en.wikipedia.org/wiki/Vectorization_%28mathematics%29).\n",
    "\n",
    "Because of the independency between $w_j$ in the same layer, instead of calculating $\\nabla_{w}L$, we can calculate\n",
    "$$\\begin{align*}\n",
    "\\left(\\nabla_{\\mathrm{vec}(w^{[3]})}L\\right)^\\mathrm{T}\n",
    "    &= \\frac{\\partial L}{\\partial \\mathrm{vec}({w^{[3]}})} \\\\\n",
    "    &= \\left(\\frac{\\partial L}{\\partial a^{[3]}}\\right)_{1\\times2}\n",
    "       \\left(\\frac{\\partial a^{[3]}}{\\partial z^{[3]}}\\right)_{2\\times2}\n",
    "       \\left(\\frac{\\partial z^{[3]}}{\\partial \\mathrm{vec}(w^{[3]})}\\right)_{2\\times8} \\\\\n",
    "    &= \\left[\\frac{\\partial L}{\\partial {w^{[3]}_1}}, \\frac{\\partial L}{\\partial {w^{[3]}_2}}\\right]_{1\\times8} \\\\\n",
    "    &= \\left(\\mathrm{vec}(\\nabla_{w^{[3]}}L)\\right)^\\mathrm{T}\n",
    "\\end{align*}$$\n",
    "And with _reshape_ function, we could transform it easily.\n",
    "\n",
    "Now let's look at the most complicated gradient\n",
    "$$\\begin{align*}\n",
    "\\left(\\nabla_{\\mathrm{vec}(w^{[1]})}L\\right)^\\mathrm{T}\n",
    "    &= \\left(\\frac{\\partial L}{\\partial a^{[3]}}\\right)_{1\\times2}\n",
    "       \\left(\\frac{\\partial a^{[3]}}{\\partial z^{[3]}}\\right)_{2\\times2}\n",
    "       \\left(\\frac{\\partial z^{[3]}}{\\partial a^{[2]}}\\right)_{2\\times4}\n",
    "       \\left(\\frac{\\partial a^{[2]}}{\\partial z^{[2]}}\\right)_{4\\times4}\n",
    "       \\left(\\frac{\\partial z^{[2]}}{\\partial a^{[1]}}\\right)_{4\\times3}\n",
    "       \\left(\\frac{\\partial a^{[1]}}{\\partial z^{[1]}}\\right)_{3\\times3}\n",
    "       \\left(\\frac{\\partial z^{[1]}}{\\partial \\mathrm{vec}(w^{[1]})}\\right)_{3\\times3N} \\\\\n",
    "    &= \\left(\\nabla L(a^{[3]\\mathrm{T}})\\right)\n",
    "       \\left(diag\\left(\\nabla\\sigma(z^{[3]}_j)\\right)\\right)\n",
    "       w^{[3]\\mathrm{T}}\n",
    "       \\left(diag\\left(\\nabla\\sigma(z^{[2]}_j)\\right)\\right)\n",
    "       w^{[2]\\mathrm{T}}\n",
    "       \\left(diag\\left(\\nabla\\sigma(z^{[1]}_j)\\right)\\right)\n",
    "       \\left(diag\\left(x^{(i)\\mathrm{T}},x^{(i)\\mathrm{T}},x^{(i)\\mathrm{T}}\\right)\\right)\n",
    "\\end{align*}$$\n",
    "\n",
    "Here $\\nabla L,\\nabla\\sigma$ in the right are all map functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And because the diagonal matrix(and block matrix for $diag(x^{(i)\\mathrm{T}}, x^{(i)\\mathrm{T}}, x^{(i)\\mathrm{T}})$) has many zeros, we'd better use __broadcasting__ to save the space. So we have\n",
    "$$\\begin{align*}\n",
    "\\left(\\nabla_{w^{[1]}}L\\right)_{N\\times3}\n",
    "    &=  \\left(\\nabla L(a^{[3]\\mathrm{T}})\\right)_{1\\times2} .*\n",
    "       \\left(\\nabla\\sigma(z^{[3]\\mathrm{T}}_j)\\right)_{1\\times2} *\n",
    "       \\left(w^{[3]\\mathrm{T}}\\right)_{2\\times4} .*\n",
    "       \\left(\\nabla\\sigma(z^{[2]\\mathrm{T}}_j)\\right)_{1\\times4} *\n",
    "       \\left(w^{[2]\\mathrm{T}}\\right)_{4\\times3} .*\n",
    "       \\left(\\nabla\\sigma(z^{[1]\\mathrm{T}}_j)\\right)_{1\\times3} .*\n",
    "       \\left[x^{(i)},x^{(i)},x^{(i)}\\right]_{N\\times 3} \\\\\n",
    "    &=  \\left(\\nabla L(a^{[3]\\mathrm{T}})\\right)_{1\\times2} .*\n",
    "       \\left(\\nabla\\sigma(z^{[3]\\mathrm{T}}_j)\\right)_{1\\times2} *\n",
    "       \\left(w^{[3]\\mathrm{T}}\\right)_{2\\times4} .*\n",
    "       \\left(\\nabla\\sigma(z^{[2]\\mathrm{T}}_j)\\right)_{1\\times4} *\n",
    "       \\left(w^{[2]\\mathrm{T}}\\right)_{4\\times3} .*\n",
    "       \\left(\\nabla\\sigma(z^{[1]\\mathrm{T}}_j)\\right)_{1\\times3} .*\n",
    "       \\left[x^{(i)}\\right]_{N\\times 1}\n",
    "\\end{align*}$$\n",
    "where $.*$ is broadcasting in Julia and $*$ is the normal matrix multiplication. Thanks to broadcasting, we don't need to reshape the matrix. One example for broadcasting is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3; 2 4 6; 3 6 9; 4 8 12; 5 10 15; 6 12 18; 7 14 21; 8 16 24; 9 18 27; 10 20 30](10, 3)\n",
      "[1 4 9](1, 3)\n"
     ]
    }
   ],
   "source": [
    "a = [1 2 3]  # 1 by 3\n",
    "b = [1:10;]  # 10 by 1\n",
    "println(a .* b, size(a .* b))  # 10 by 3\n",
    "println(a .* a, size(a .* a))  # 1 by 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is same for $b^{[1]}$:\n",
    "$$\\begin{align*}\n",
    "\\left(\\nabla_{b^{[1]}}L\\right)^\\mathrm{T}\n",
    "    &=  \\left(\\nabla L(a^{[3]\\mathrm{T}})\\right)_{1\\times2} .*\n",
    "       \\left(\\nabla\\sigma(z^{[3]\\mathrm{T}}_j)\\right)_{1\\times2} *\n",
    "       \\left(w^{[3]\\mathrm{T}}\\right)_{2\\times4} .*\n",
    "       \\left(\\nabla\\sigma(z^{[2]\\mathrm{T}}_j)\\right)_{1\\times4} *\n",
    "       \\left(w^{[2]\\mathrm{T}}\\right)_{4\\times3} .*\n",
    "       \\left(\\nabla\\sigma(z^{[1]\\mathrm{T}}_j)\\right)_{1\\times3} * E_{3\\times3} \\\\\n",
    "    &= \\left(\\nabla L(a^{[3]\\mathrm{T}})\\right)_{1\\times2} .*\n",
    "       \\left(\\nabla\\sigma(z^{[3]\\mathrm{T}}_j)\\right)_{1\\times2} *\n",
    "       \\left(w^{[3]\\mathrm{T}}\\right)_{2\\times4} .*\n",
    "       \\left(\\nabla\\sigma(z^{[2]\\mathrm{T}}_j)\\right)_{1\\times4} *\n",
    "       \\left(w^{[2]\\mathrm{T}}\\right)_{4\\times3} .*\n",
    "       \\left(\\nabla\\sigma(z^{[1]\\mathrm{T}}_j)\\right)_{1\\times3}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we could have:\n",
    "$$\\begin{align*}\n",
    "\\delta^{[i]} \n",
    "    &=  \\left(\\nabla L(a^{[D]\\mathrm{T}})\\right)_{1\\times N_D} \\\\\n",
    "    &.* \\left(\\nabla\\sigma(z^{[D]\\mathrm{T}}_j)\\right)_{1\\times N_D} * \n",
    "        \\left(w^{[D]\\mathrm{T}}\\right)_{N_D\\times N_{D-1}} \\\\\n",
    "    &.*  \\left(\\nabla\\sigma(z^{[D-1]\\mathrm{T}}_j)\\right)_{1\\times N_{D-1}} *\n",
    "       \\left(w^{[D-1]\\mathrm{T}}\\right)_{N_{D-1}\\times N_{D-1}} \\\\\n",
    "    &.* \\cdots \\\\\n",
    "    &.*  \\left(\\nabla\\sigma(z^{[i+1]\\mathrm{T}}_j)\\right)_{1\\times N_{i+1}} *\n",
    "       \\left(w^{[i+1]\\mathrm{T}}\\right)_{N_{i+1}\\times N_{i}} \\\\\n",
    "    &.* \\left(\\nabla\\sigma(z^{[i]\\mathrm{T}}_j)\\right)_{1\\times N_{i}}\n",
    "\\end{align*}$$\n",
    "and\n",
    "$$\n",
    "\\nabla_{w^{[i]}}L = \\delta^{[i]} .* a^{[i-1]} \\\\\n",
    "\\nabla_{b^{[i]}}L = \\delta^{[i]\\mathrm{T}}\n",
    "$$\n",
    "also to save time and space, we would have\n",
    "$$\n",
    "\\delta^{[i]} = \\delta^{[i+1]} \n",
    "    * \\left(w^{[i+1]\\mathrm{T}}\\right)_{N_{i+1}\\times N_{i}}\n",
    "    .* \\left(\\nabla\\sigma(z^{[i]\\mathrm{T}}_j)\\right)_{1\\times N_{i}}\n",
    "$$\n",
    "So every time we get $z^{[i]}$, we could send it to calculate $w^{[i+1]\\mathrm{T}}.* \\nabla\\sigma(z^{[i]\\mathrm{T}}_j)$ and store it for the backpropagation. Note that \n",
    "$$A_{1\\times N}*B_{N\\times M}.*C_{1\\times M} = A*(B.*C)$$\n",
    "According to BGD, we need to calculate different $z^{[i](j)}$ with input $x^{(j)}$, and could we simplify it in matrix form rather than put then in a for-loop? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's think about $\\nabla_{w^{[1]}}L$ with $M$ samples.\n",
    "$$\\begin{align*}\n",
    "\\sum_{i=1}^M\\left(\\nabla_{w^{[1]}}L^{(i)}\\right)_{N\\times3}\n",
    "    &=  \\sum_{i=1}^M\\left\\{\\left(\\nabla L(a^{[3]\\mathrm{T}}_M)\\right)_{M\\times2} .*\n",
    "       \\left(\\nabla\\sigma(z^{[3]\\mathrm{T}}_M)\\right)_{M\\times2} *\n",
    "       \\left(w^{[3]\\mathrm{T}}\\right)_{2\\times4} .*\n",
    "       \\left(\\nabla\\sigma(z^{[2]\\mathrm{T}}_M)\\right)_{M\\times4} *\n",
    "       \\left(w^{[2]\\mathrm{T}}\\right)_{4\\times3} .*\n",
    "       \\left(\\nabla\\sigma(z^{[1]\\mathrm{T}}_M)\\right)_{M\\times3}\\right\\}_i .*\n",
    "       \\left[x^{(i)}\\right]_{N\\times 1} \\\\\n",
    "    &= X_{N\\times M}\n",
    "       \\left[\\left(\\nabla L(a^{[3]\\mathrm{T}}_M\\mathrm{T})\\right)_{M\\times2} .*\n",
    "       \\left(\\nabla\\sigma(z^{[3]\\mathrm{T}}_M)\\right)_{M\\times2} *\n",
    "       \\left(w^{[3]\\mathrm{T}}\\right)_{2\\times4} .*\n",
    "       \\left(\\nabla\\sigma(z^{[2]\\mathrm{T}}_M)\\right)_{M\\times4} *\n",
    "       \\left(w^{[2]\\mathrm{T}}\\right)_{4\\times3} .*\n",
    "       \\left(\\nabla\\sigma(z^{[1]\\mathrm{T}}_M)\\right)_{M\\times3}\\right]\\\\\n",
    "    &\\triangleq X\\delta^{[1]}_M\n",
    "\\end{align*}$$\n",
    "in the same way\n",
    "$$\n",
    "\\sum_{i=1}^M\\nabla_b^{[1]}L^{(i)} = \\delta^{[1]\\mathrm{T}}_M\\mathbf{1}_v\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\delta^{[i]}_M = \\delta^{[i+1]}_M\n",
    "    * \\left(w^{[i+1]\\mathrm{T}}\\right)_{N_{i+1}\\times N_{i}}\n",
    "    .* \\left(\\nabla\\sigma(z^{[i]})\\right)_{M\\times N_{i}}\n",
    "$$\n",
    "Thanks to broadcasting, it only needs a little difference to apply on matrix in BGD.\n",
    "However, in the matrix form, we could not have $A*B.*C = A*(B.*C)$, for $B_{P\\times Q}.*C_{M\\times Q}$ is not defined in broadcasting. So we could only calculate it in its origin order.\n",
    "\n",
    "## Example\n",
    "Let's see an example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
