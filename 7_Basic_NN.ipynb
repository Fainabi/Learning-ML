{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic NN\n",
    "\n",
    "This IJulia shows the basic concept and application with neural network, which has some matrix calculus.\n",
    "\n",
    "Firstly we set $$X = [x^{(1)}, x^{(2)}, ..., x^{(m)}]$$ as the inputted data, where $x^{i}$ is an $n$ by $1$ vector with $n$ features. Then we have an $N_{i-1}$ by $N_{i}$ vector $w^{[i]}$ to represent the _weight matrix_ at the $i$th layer and an $N_i$ by $1$ bias vector $b^{[i]}$.  \n",
    "\n",
    "Thus at each layer we could have the calculation:\n",
    "$$z^{[i]} = w^{[i]\\mathrm{T}}a^{[i-1]} + b^{[i]}\\\\a^{[i]} = \\sigma(z^{[i]})$$\n",
    "where $a^{[i]}$ is the activation ouput from layer $i$. \n",
    "\n",
    "It is all known that there are a lot of vector and matrix derivatives in NN. So we need to analyze them completely in order to get a deep comprehenson of NN.\n",
    "\n",
    "We construct a fully connected NN here: \n",
    "<center><img src=\"pic/342NN.png\" width=80%/></center>\n",
    "with 4 layers which have N, 3, 4 and 2 nodes in each layer. This may look like a binary classification applied in tasks such as picture classification(However CNN performs much better in these problems)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost/Loss Function\n",
    "\n",
    "To train a NN, we need some rules and directions to point out that how to adjust our $w$s and $b$s. And that is __Cost Funcion__ and __Loss Function__.<br/>\n",
    "Loss function is used to measure the performance of NN describing the \"distance\" bewteen the output of our NN and the true labels from dataset. Usually we would like to minimize the loss function to make NN work better. In the former NN we constructed, we have two output values. And we need a loss function $$L: \\mathcal{R}^2 \\rightarrow \\mathcal{R}$$ Here $L$ must be a functional, or it would generate a couple of conflicting NNs.\n",
    "\n",
    "Because in each layer, the weight edges are independent to each other, it's same to calculate $\\frac{\\partial L}{\\partial w^{[i]}}$, $\\frac{\\partial L}{\\partial w^{[i]}_j}$ and $\\frac{\\partial L}{\\partial w^{[i]}_{jk}}$ for updating the weights. For convience, we will using the matrix derivative to calculate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients\n",
    "\n",
    "With loss function, what we need to do in training NN is using the gradient to update weights and bias\n",
    "$$\n",
    "w^{[i]} := w^{[i]} - \\alpha\\nabla_{w^{[i]}}L \\\\\n",
    "b^{[i]} := b^{[i]} - \\alpha\\nabla_{b^{[i]}}L\n",
    "$$\n",
    "and with BGD, we could have\n",
    "$$\n",
    "w^{[i]} := w^{[i]} - \\alpha\\frac{1}{m}\\sum_{j=1}^m\\nabla_{w^{[i]}}L^{(j)} \\\\\n",
    "b^{[i]} := b^{[i]} - \\alpha\\frac{1}{m}\\sum_{j=1}^m\\nabla_{b^{[i]}}L^{(j)}\n",
    "$$\n",
    "So all we need to do is to calculate $\\nabla_{w^{[i]}}L$ and $\\nabla_{b^{[i]}}L$. With [chain rule](https://en.wikipedia.org/wiki/Chain_rule) applied, we have\n",
    "$$\\begin{align*}\n",
    "\\left(\\nabla_{w^{[3]}_j}L\\right)^\\mathrm{T}\n",
    "    &= \\frac{\\partial L}{\\partial w^{[3]}_j} \\\\\n",
    "    &= \\left(\\frac{\\partial L}{\\partial a^{[3]}}\\right)_{1\\times2}\n",
    "       \\left(\\frac{\\partial a^{[3]}}{\\partial z^{[3]}}\\right)_{2\\times2}\n",
    "       \\left(\\frac{\\partial z^{[3]}}{\\partial w^{[3]}_j}\\right)_{2\\times4} \\\\\n",
    "\\end{align*}$$\n",
    "where $w^{[3]}_j = w^{[3]}_{:,j}$. And because of independency, we could have\n",
    "$$\\begin{align*}\n",
    "\\left(\\nabla_{w^{[3]}}L\\right)^\\mathrm{T}\n",
    "    &= \\frac{\\partial L}{\\partial w^{[3]}} \\\\\n",
    "    &= \\sum_{j=1}^2(E_j)_{2\\times1}\\left(\\frac{\\partial L}{\\partial a^{[3]}}\\right)_{1\\times2}\n",
    "       \\left(\\frac{\\partial a^{[3]}}{\\partial z^{[3]}}\\right)_{2\\times2}\n",
    "       \\left(\\frac{\\partial z^{[3]}}{\\partial w^{[3]}_j}\\right)_{2\\times4} \\\\\n",
    "\\end{align*}$$\n",
    "where $E_1 = [1,0]^\\mathrm{T}, E_2 = [0,1]^\\mathrm{T}$. Samely, we could easily calculate $\\nabla_{b^{[3]}}L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a more suitable form, we need to inspect $\\nabla_{w^{[2]}}L$,\n",
    "$$\\begin{align*}\n",
    "\\left(\\nabla_{w^{[2]}_j}L\\right)^\\mathrm{T}\n",
    "    &= \\frac{\\partial L}{\\partial w^{[2]}_j} \\\\\n",
    "    &= \\left(\\frac{\\partial L}{\\partial a^{[3]}}\\right)_{1\\times2}\n",
    "       \\left(\\frac{\\partial a^{[3]}}{\\partial z^{[3]}}\\right)_{2\\times2}\n",
    "       \\left(\\frac{\\partial z^{[3]}}{\\partial a^{[2]}}\\right)_{2\\times4}\n",
    "       \\left(\\frac{\\partial a^{[2]}}{\\partial z^{[2]}}\\right)_{4\\times4}\n",
    "       \\left(\\frac{\\partial z^{[2]}}{\\partial w^{[2]}_j}\\right)_{4\\times3}\n",
    "\\end{align*}$$\n",
    "and \n",
    "$$\n",
    "\\left(\\nabla_{w^{[2]}}L\\right)^\\mathrm{T}\n",
    "    =  \\sum_{j=1}^4\\left(E_j\\right)_{4\\times1}\\left(\\frac{\\partial L}{\\partial a^{[3]}}\\right)_{1\\times2}\n",
    "       \\left(\\frac{\\partial a^{[3]}}{\\partial z^{[3]}}\\right)_{2\\times2}\n",
    "       \\left(\\frac{\\partial z^{[3]}}{\\partial a^{[2]}}\\right)_{2\\times4}\n",
    "       \\left(\\frac{\\partial a^{[2]}}{\\partial z^{[2]}}\\right)_{4\\times4}\n",
    "       \\left(\\frac{\\partial z^{[2]}}{\\partial w^{[2]}_j}\\right)_{4\\times3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization & Broadcasting\n",
    "\n",
    "We have known that the built in functions for matrix calculations perform much better than explicit loop operation in Julia, as well as Python. So we'd like to find a more matrix tasty calculation, where we need the [Vectorization](https://en.wikipedia.org/wiki/Vectorization_%28mathematics%29).\n",
    "\n",
    "Because of the independency between $w_j$ in the same layer, instead of calculating $\\nabla_{w}L$, we can calculate\n",
    "$$\\begin{align*}\n",
    "\\left(\\nabla_{\\mathrm{vec}(w^{[3]})}L\\right)^\\mathrm{T}\n",
    "    &= \\frac{\\partial L}{\\partial \\mathrm{vec}({w^{[3]}})} \\\\\n",
    "    &= \\left(\\frac{\\partial L}{\\partial a^{[3]}}\\right)_{1\\times2}\n",
    "       \\left(\\frac{\\partial a^{[3]}}{\\partial z^{[3]}}\\right)_{2\\times2}\n",
    "       \\left(\\frac{\\partial z^{[3]}}{\\partial \\mathrm{vec}(w^{[3]})}\\right)_{2\\times8} \\\\\n",
    "    &= \\left[\\frac{\\partial L}{\\partial {w^{[3]}_1}}, \\frac{\\partial L}{\\partial {w^{[3]}_2}}\\right]_{1\\times8} \\\\\n",
    "    &= \\left(\\mathrm{vec}(\\nabla_{w^{[3]}}L)\\right)^\\mathrm{T}\n",
    "\\end{align*}$$\n",
    "And with _reshape_ function, we could transform it easily.\n",
    "\n",
    "Now let's look at the most complicated gradient\n",
    "$$\\begin{align*}\n",
    "\\left(\\nabla_{\\mathrm{vec}(w^{[1]})}L\\right)^\\mathrm{T}\n",
    "    &= \\left(\\frac{\\partial L}{\\partial a^{[3]}}\\right)_{1\\times2}\n",
    "       \\left(\\frac{\\partial a^{[3]}}{\\partial z^{[3]}}\\right)_{2\\times2}\n",
    "       \\left(\\frac{\\partial z^{[3]}}{\\partial a^{[2]}}\\right)_{2\\times4}\n",
    "       \\left(\\frac{\\partial a^{[2]}}{\\partial z^{[2]}}\\right)_{4\\times4}\n",
    "       \\left(\\frac{\\partial z^{[2]}}{\\partial a^{[1]}}\\right)_{4\\times3}\n",
    "       \\left(\\frac{\\partial a^{[1]}}{\\partial z^{[1]}}\\right)_{3\\times3}\n",
    "       \\left(\\frac{\\partial z^{[1]}}{\\partial \\mathrm{vec}(w^{[1]})}\\right)_{3\\times3N} \\\\\n",
    "    &= \\left(\\nabla L(a^{[3]\\mathrm{T}})\\right)\n",
    "       \\left(diag\\left(\\nabla\\sigma(z^{[3]}_j)\\right)\\right)\n",
    "       w^{[3]\\mathrm{T}}\n",
    "       \\left(diag\\left(\\nabla\\sigma(z^{[2]}_j)\\right)\\right)\n",
    "       w^{[2]\\mathrm{T}}\n",
    "       \\left(diag\\left(\\nabla\\sigma(z^{[1]}_j)\\right)\\right)\n",
    "       \\left(diag\\left(x^{(i)\\mathrm{T}},x^{(i)\\mathrm{T}},x^{(i)\\mathrm{T}}\\right)\\right)\n",
    "\\end{align*}$$\n",
    "\n",
    "Here $\\nabla L,\\nabla\\sigma$ in the right are all map functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And because the diagonal matrix(and block matrix for $diag(x^{(i)\\mathrm{T}}, x^{(i)\\mathrm{T}}, x^{(i)\\mathrm{T}})$) has many zeros, we'd better use __broadcasting__ to save the space. So we have\n",
    "$$\\begin{align*}\n",
    "\\left(\\nabla_{w^{[1]}}L\\right)_{N\\times3}\n",
    "    &=  \\left(\\nabla L(a^{[3]\\mathrm{T}})\\right)_{1\\times2} .*\n",
    "       \\left(\\nabla\\sigma(z^{[3]\\mathrm{T}}_j)\\right)_{1\\times2} *\n",
    "       \\left(w^{[3]\\mathrm{T}}\\right)_{2\\times4} .*\n",
    "       \\left(\\nabla\\sigma(z^{[2]\\mathrm{T}}_j)\\right)_{1\\times4} *\n",
    "       \\left(w^{[2]\\mathrm{T}}\\right)_{4\\times3} .*\n",
    "       \\left(\\nabla\\sigma(z^{[1]\\mathrm{T}}_j)\\right)_{1\\times3} .*\n",
    "       \\left[x^{(i)},x^{(i)},x^{(i)}\\right]_{N\\times 3} \\\\\n",
    "    &=  \\left(\\nabla L(a^{[3]\\mathrm{T}})\\right)_{1\\times2} .*\n",
    "       \\left(\\nabla\\sigma(z^{[3]\\mathrm{T}}_j)\\right)_{1\\times2} *\n",
    "       \\left(w^{[3]\\mathrm{T}}\\right)_{2\\times4} .*\n",
    "       \\left(\\nabla\\sigma(z^{[2]\\mathrm{T}}_j)\\right)_{1\\times4} *\n",
    "       \\left(w^{[2]\\mathrm{T}}\\right)_{4\\times3} .*\n",
    "       \\left(\\nabla\\sigma(z^{[1]\\mathrm{T}}_j)\\right)_{1\\times3} .*\n",
    "       \\left[x^{(i)}\\right]_{N\\times 1}\n",
    "\\end{align*}$$\n",
    "where $.*$ is broadcasting in Julia and $*$ is the normal matrix multiplication. Thanks to broadcasting, we don't need to reshape the matrix. One example for broadcasting is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3; 2 4 6; 3 6 9; 4 8 12; 5 10 15; 6 12 18; 7 14 21; 8 16 24; 9 18 27; 10 20 30](10, 3)\n",
      "[1 4 9](1, 3)\n"
     ]
    }
   ],
   "source": [
    "a = [1 2 3]  # 1 by 3\n",
    "b = [1:10;]  # 10 by 1\n",
    "println(a .* b, size(a .* b))  # 10 by 3\n",
    "println(a .* a, size(a .* a))  # 1 by 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is same for $b^{[1]}$:\n",
    "$$\\begin{align*}\n",
    "\\left(\\nabla_{b^{[1]}}L\\right)^\\mathrm{T}\n",
    "    &=  \\left(\\nabla L(a^{[3]\\mathrm{T}})\\right)_{1\\times2} .*\n",
    "       \\left(\\nabla\\sigma(z^{[3]\\mathrm{T}}_j)\\right)_{1\\times2} *\n",
    "       \\left(w^{[3]\\mathrm{T}}\\right)_{2\\times4} .*\n",
    "       \\left(\\nabla\\sigma(z^{[2]\\mathrm{T}}_j)\\right)_{1\\times4} *\n",
    "       \\left(w^{[2]\\mathrm{T}}\\right)_{4\\times3} .*\n",
    "       \\left(\\nabla\\sigma(z^{[1]\\mathrm{T}}_j)\\right)_{1\\times3} * E_{3\\times3} \\\\\n",
    "    &= \\left(\\nabla L(a^{[3]\\mathrm{T}})\\right)_{1\\times2} .*\n",
    "       \\left(\\nabla\\sigma(z^{[3]\\mathrm{T}}_j)\\right)_{1\\times2} *\n",
    "       \\left(w^{[3]\\mathrm{T}}\\right)_{2\\times4} .*\n",
    "       \\left(\\nabla\\sigma(z^{[2]\\mathrm{T}}_j)\\right)_{1\\times4} *\n",
    "       \\left(w^{[2]\\mathrm{T}}\\right)_{4\\times3} .*\n",
    "       \\left(\\nabla\\sigma(z^{[1]\\mathrm{T}}_j)\\right)_{1\\times3}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we could have:\n",
    "$$\\begin{align*}\n",
    "\\delta^{[i]\\mathrm{T}} \n",
    "    &=  \\left(\\nabla L(a^{[D]\\mathrm{T}})\\right)_{1\\times N_D} \\\\\n",
    "    &.* \\left(\\nabla\\sigma(z^{[D]\\mathrm{T}}_j)\\right)_{1\\times N_D} * \n",
    "        \\left(w^{[D]\\mathrm{T}}\\right)_{N_D\\times N_{D-1}} \\\\\n",
    "    &.*  \\left(\\nabla\\sigma(z^{[D-1]\\mathrm{T}}_j)\\right)_{1\\times N_{D-1}} *\n",
    "       \\left(w^{[D-1]\\mathrm{T}}\\right)_{N_{D-1}\\times N_{D-2}} \\\\\n",
    "    &.* \\cdots \\\\\n",
    "    &.*  \\left(\\nabla\\sigma(z^{[i+1]\\mathrm{T}}_j)\\right)_{1\\times N_{i+1}} *\n",
    "       \\left(w^{[i+1]\\mathrm{T}}\\right)_{N_{i+1}\\times N_{i}} \\\\\n",
    "    &.* \\left(\\nabla\\sigma(z^{[i]\\mathrm{T}}_j)\\right)_{1\\times N_{i}}\n",
    "\\end{align*}$$\n",
    "and\n",
    "$$\n",
    "\\nabla_{w^{[i]}}L = \\delta^{[i]\\mathrm{T}} .* a^{[i-1]} \\\\\n",
    "\\nabla_{b^{[i]}}L = \\delta^{[i]}\n",
    "$$\n",
    "also to save time and space, we would have\n",
    "$$\n",
    "\\delta^{[i]\\mathrm{T}} = \\delta^{[i+1]\\mathrm{T}} \n",
    "    * \\left(w^{[i+1]\\mathrm{T}}\\right)_{N_{i+1}\\times N_{i}}\n",
    "    .* \\left(\\nabla\\sigma(z^{[i]\\mathrm{T}}_j)\\right)_{1\\times N_{i}}\n",
    "$$\n",
    "So every time we get $z^{[i]}$, we could send it to calculate $w^{[i+1]\\mathrm{T}}.* \\nabla\\sigma(z^{[i]\\mathrm{T}}_j)$ and store it for the backpropagation. Note that \n",
    "$$A_{1\\times N}*B_{N\\times M}.*C_{1\\times M} = A*(B.*C)$$\n",
    "According to BGD, we need to calculate different $z^{[i](j)}$ with input $x^{(j)}$, and could we simplify it in matrix form rather than put then in a for-loop? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's think about $\\nabla_{w^{[1]}}L$ with $M$ samples.\n",
    "$$\\begin{align*}\n",
    "\\sum_{i=1}^M\\left(\\nabla_{w^{[1]}}L^{(i)}\\right)_{N\\times3}\n",
    "    &=  \\sum_{i=1}^M\\left\\{\\left(\\nabla L(a^{[3]\\mathrm{T}}_M)\\right)_{M\\times2} .*\n",
    "       \\left(\\nabla\\sigma(z^{[3]\\mathrm{T}}_M)\\right)_{M\\times2} *\n",
    "       \\left(w^{[3]\\mathrm{T}}\\right)_{2\\times4} .*\n",
    "       \\left(\\nabla\\sigma(z^{[2]\\mathrm{T}}_M)\\right)_{M\\times4} *\n",
    "       \\left(w^{[2]\\mathrm{T}}\\right)_{4\\times3} .*\n",
    "       \\left(\\nabla\\sigma(z^{[1]\\mathrm{T}}_M)\\right)_{M\\times3}\\right\\}_i .*\n",
    "       \\left[x^{(i)}\\right]_{N\\times 1} \\\\\n",
    "    &= X_{N\\times M}\n",
    "       \\left[\\left(\\nabla L(a^{[3]\\mathrm{T}}_M\\mathrm{T})\\right)_{M\\times2} .*\n",
    "       \\left(\\nabla\\sigma(z^{[3]\\mathrm{T}}_M)\\right)_{M\\times2} *\n",
    "       \\left(w^{[3]\\mathrm{T}}\\right)_{2\\times4} .*\n",
    "       \\left(\\nabla\\sigma(z^{[2]\\mathrm{T}}_M)\\right)_{M\\times4} *\n",
    "       \\left(w^{[2]\\mathrm{T}}\\right)_{4\\times3} .*\n",
    "       \\left(\\nabla\\sigma(z^{[1]\\mathrm{T}}_M)\\right)_{M\\times3}\\right]\\\\\n",
    "    &\\triangleq X\\delta^{[1]\\mathrm{T}}_M\n",
    "\\end{align*}$$\n",
    "in the same way\n",
    "$$\n",
    "\\sum_{i=1}^M\\nabla_b^{[1]}L^{(i)} = \\delta^{[1]}_M\\mathbf{1}_v\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\delta^{[i]\\mathrm{T}}_M = \\delta^{[i+1]\\mathrm{T}}_M\n",
    "    * \\left(w^{[i+1]\\mathrm{T}}\\right)_{N_{i+1}\\times N_{i}}\n",
    "    .* \\left(\\nabla\\sigma(z^{[i]})\\right)_{M\\times N_{i}}\n",
    "$$\n",
    "Thanks to broadcasting, it only needs a little difference to apply on matrix in BGD.\n",
    "However, in the matrix form, we could not have $A*B.*C = A*(B.*C)$, for $B_{P\\times Q}.*C_{M\\times Q}$ is not defined in broadcasting. So we could only calculate it in its origin order.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "Before we practice with some examples, we first have some preparations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: replacing module NeuralNetwork.\n"
     ]
    }
   ],
   "source": [
    "using Logging, LinearAlgebra, Random, BenchmarkTools\n",
    "include(\"7_neural_network.jl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "backpropagation(X, Y; <keyword arguments>, max_iter=1e3, α=0.1)\n",
       "\\end{verbatim}\n",
       "Backpropagation NN. The accessible activation functions are \\texttt{logistic}, \\texttt{ReLU} and \\texttt{tanh} which could\n",
       "\n",
       "be inspected by NeuralNetwork.ActivationFunction.\n",
       "\n",
       "...\n",
       "\n",
       "\\section{Arguments}\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{X::Array}: samples\n",
       "\n",
       "\n",
       "\\item \\texttt{Y::Array}: labels\n",
       "\n",
       "\n",
       "\\item \\texttt{loss::Function}: loss function, this function just in normal vector-by-vector form\n",
       "\n",
       "\n",
       "\\item \\texttt{deri\\_loss::Function}: derivative of loss function, this function \\_\\_\\_must\\_\\_\\_ be in matrix form\n",
       "\n",
       "\n",
       "\\item \\texttt{activations::Array\\{ActivationFunction\\}}: arrays that contains activation function types for every layer. \n",
       "\n",
       "\n",
       "\\item \\texttt{nodes::Array}: number of nodes in every layer\n",
       "\n",
       "\n",
       "\\item \\texttt{max\\_iter::Number}: maximum time to iterate, default is 1e3\n",
       "\n",
       "\n",
       "\\item \\texttt{α::AbstractFloat}: learning rate, default is 0.001\n",
       "\n",
       "\\end{itemize}\n",
       "...\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "backpropagation(X, Y; <keyword arguments>, max_iter=1e3, α=0.1)\n",
       "```\n",
       "\n",
       "Backpropagation NN. The accessible activation functions are `logistic`, `ReLU` and `tanh` which could\n",
       "\n",
       "be inspected by NeuralNetwork.ActivationFunction.\n",
       "\n",
       "...\n",
       "\n",
       "# Arguments\n",
       "\n",
       "  * `X::Array`: samples\n",
       "  * `Y::Array`: labels\n",
       "  * `loss::Function`: loss function, this function just in normal vector-by-vector form\n",
       "  * `deri_loss::Function`: derivative of loss function, this function ___must___ be in matrix form\n",
       "  * `activations::Array{ActivationFunction}`: arrays that contains activation function types for every layer.\n",
       "  * `nodes::Array`: number of nodes in every layer\n",
       "  * `max_iter::Number`: maximum time to iterate, default is 1e3\n",
       "  * `α::AbstractFloat`: learning rate, default is 0.001\n",
       "\n",
       "...\n"
      ],
      "text/plain": [
       "\u001b[36m  backpropagation(X, Y; <keyword arguments>, max_iter=1e3, α=0.1)\u001b[39m\n",
       "\n",
       "  Backpropagation NN. The accessible activation functions are \u001b[36mlogistic\u001b[39m, \u001b[36mReLU\u001b[39m\n",
       "  and \u001b[36mtanh\u001b[39m which could\n",
       "\n",
       "  be inspected by NeuralNetwork.ActivationFunction.\n",
       "\n",
       "  ...\n",
       "\n",
       "\u001b[1m  Arguments\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "    •    \u001b[36mX::Array\u001b[39m: samples\n",
       "\n",
       "    •    \u001b[36mY::Array\u001b[39m: labels\n",
       "\n",
       "    •    \u001b[36mloss::Function\u001b[39m: loss function, this function just in normal\n",
       "        vector-by-vector form\n",
       "\n",
       "    •    \u001b[36mderi_loss::Function\u001b[39m: derivative of loss function, this function\n",
       "        ___must___ be in matrix form\n",
       "\n",
       "    •    \u001b[36mactivations::Array{ActivationFunction}\u001b[39m: arrays that contains\n",
       "        activation function types for every layer. \n",
       "\n",
       "    •    \u001b[36mnodes::Array\u001b[39m: number of nodes in every layer\n",
       "\n",
       "    •    \u001b[36mmax_iter::Number\u001b[39m: maximum time to iterate, default is 1e3\n",
       "\n",
       "    •    \u001b[36mα::AbstractFloat\u001b[39m: learning rate, default is 0.001\n",
       "\n",
       "  ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?NeuralNetwork.backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in every example, we need to define specific loss function and its derivative "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "Linear regression model is a $N\\times1$ NN whose activation functions are all identity function. Let us take a 3 dimension vector as the input\n",
    "$$\n",
    "y = w_1x_1 + w_2x_2 + w_3x_3 + b\n",
    "$$\n",
    "Then generate 1000 data with a random error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Main.LinearRegression"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"1_linear_regression.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of layers is 1\n",
      "number of training data is 1000\n",
      "scale of NN is 3 × [1]\n",
      "100.0%┣█████████████████████████████████┫ 20000/20000 [00:02<00:00, 9357.2 it/s]3624/20000 [00:00<00:02, 8080.2 it/s]4661/20000 [00:01<00:02, 8463.7 it/s]5750/20000 [00:01<00:02, 8777.3 it/s]10768/20000 [00:01<00:01, 8470.2 it/s]17606/20000 [00:02<00:00, 8974.4 it/s]\n",
      "  2.385756 seconds (1.57 M allocations: 1.258 GiB, 15.33% gc time)\n",
      "  0.553908 seconds (170.45 k allocations: 450.826 MiB, 20.03% gc time)\n",
      "w_hat is Any[[10.004618830221458; 19.994929553248205; 40.0006750810889]], b_hat is Any[[10.011254438889358]]\n",
      "\n",
      "in linear regression have [10.00461883019421; 19.994929553269152; 40.000675081155244], 10.011254398521011 with loss: 11.703299944444208 after 18938 iterations\n",
      "\n",
      "Optimized w and b in theory is [10.00461883023592; 19.994929553237085; 40.000675081053714; 10.011254460306148]\n"
     ]
    }
   ],
   "source": [
    "Random.seed!(0x3133534152)\n",
    "with_logger(SimpleLogger(stdout, Logging.Debug)) do\n",
    "    size = 1000\n",
    "    w = [10;20;40]\n",
    "    b = 10\n",
    "    X = (rand(3, size) .- 0.5) * 100  # rand from -50 to 50\n",
    "    Y = w'*X .+ b + randn(1, size)*5\n",
    "\n",
    "    local loss(y_hat, y) = (y_hat - y).^2 / 2  # scalar\n",
    "    local deri_loss(Y_hat, Y) = Y_hat - Y\n",
    "\n",
    "    nodes = [1]\n",
    "    activations = [NeuralNetwork.Linear]\n",
    "\n",
    "    @time local w_hat, b_hat, = NeuralNetwork.backpropagation(\n",
    "        X, Y, \n",
    "        nodes=nodes, \n",
    "        activations=activations, \n",
    "        loss=loss, \n",
    "        deri_loss=deri_loss, \n",
    "        max_iter=20000, \n",
    "        α=0.001,\n",
    "        mode=\"CPU\"\n",
    "    )\n",
    "    @time local θ, b, J, iter = LinearRegression.batch_gradient_descent(X', Y', α=0.001)\n",
    "    \n",
    "    # calculate loss\n",
    "    println(\"w_hat is $w_hat, b_hat is $b_hat\")\n",
    "    println(\"\\nin linear regression have $θ, $b with loss: $J after $iter iterations\\n\")\n",
    "    print(\"Optimized w and b in theory is \")\n",
    "    [X;ones(1, size)] |> X_ex -> inv(X_ex*X_ex')*X_ex*Y' |> println\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could find that NN costs a little bit more time that the normal linear regression algorithm. That is because we have to store $z$ and $a$, which means we need more space and time to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mnist \n",
    "\n",
    "First we download and decompress the [mnist](http://yann.lecun.com/exdb/mnist/) data, then load these file with the format mentioned in mnist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start reading images. Read 28×28 data 60000 times.\n",
      "Start reading labels. Read 60000 data.\n"
     ]
    }
   ],
   "source": [
    "images = open(\"data/mnist/train-images-idx3-ubyte\", \"r\") do f\n",
    "    if hton(read(f, UInt32)) != 2051  # magic number\n",
    "        println(\"image file format is not legal\")\n",
    "        return\n",
    "    end\n",
    "    training_num = read(f, UInt32) |> hton  # big endian\n",
    "    row_size = read(f, UInt32) |> hton\n",
    "    column_size = read(f, UInt32) |> hton\n",
    "    sample_size = row_size * column_size  # N features\n",
    "    println(\"Start reading images. Read $(row_size)×$(column_size) data $(training_num) times.\")\n",
    "    # fill data\n",
    "    images = fill(Float64(0.0), (sample_size, training_num))  # [x1, x2, ..., xM]\n",
    "    for idx = 1:training_num\n",
    "        images[:, idx] = read(f, sample_size) |> Array{Float64}\n",
    "    end\n",
    "    return images\n",
    "end\n",
    "\n",
    "labels = open(\"data/mnist/train-labels-idx1-ubyte\", \"r\") do f\n",
    "    if hton(read(f, UInt32)) != 2049  # magic number\n",
    "        println(\"label file format is not legal\")\n",
    "        return\n",
    "    end\n",
    "    labels_num = read(f, UInt32) |> hton  # big endian\n",
    "    println(\"Start reading labels. Read $labels_num data.\")\n",
    "    # fill labels\n",
    "    labels = fill(Float64(0.0), (10, labels_num))\n",
    "    idx = 1\n",
    "    for label in read(f, labels_num)\n",
    "        labels[label+1, idx] = 1\n",
    "        idx += 1\n",
    "    end\n",
    "    return labels\n",
    "end\n",
    "\n",
    "# to [0, 1]\n",
    "images = images / 255;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could use what we have done before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this module, we have to provide activation functions and loss function, which we found from the documentation. Here we think aboud constructing a $784\\times100\\times50\\times10$ NN, and each neuron uses logistic activation function. And softmax is choosen as the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of layers is 3\n",
      "number of training data is 60000\n",
      "scale of NN is 784 × [100 50 10]\n",
      "100.0%┣████████████████████████████████████████┫ 100/100 [02:26<00:00, 0.7 it/s]1/100 [00:01<Inf:Inf, 0.0 it/s]2/100 [00:03<04:40, 0.4 it/s]3/100 [00:04<03:32, 0.5 it/s]4/100 [00:06<03:10, 0.5 it/s]5/100 [00:08<02:59, 0.5 it/s]7/100 [00:11<02:45, 0.6 it/s]10/100 [00:15<02:34, 0.6 it/s]12/100 [00:18<02:27, 0.6 it/s]13/100 [00:20<02:23, 0.6 it/s]15/100 [00:22<02:15, 0.6 it/s]16/100 [00:24<02:12, 0.6 it/s]17/100 [00:25<02:09, 0.6 it/s]18/100 [00:26<02:06, 0.6 it/s]19/100 [00:28<02:04, 0.7 it/s]21/100 [00:30<01:59, 0.7 it/s]23/100 [00:33<01:55, 0.7 it/s]24/100 [00:34<01:53, 0.7 it/s]28/100 [00:39<01:45, 0.7 it/s]32/100 [00:45<01:38, 0.7 it/s]33/100 [00:46<01:37, 0.7 it/s]34/100 [00:48<01:36, 0.7 it/s]35/100 [00:50<01:35, 0.7 it/s]39/100 [00:56<01:30, 0.7 it/s]40/100 [00:57<01:28, 0.7 it/s]45/100 [01:04<01:20, 0.7 it/s]46/100 [01:05<01:18, 0.7 it/s]47/100 [01:07<01:17, 0.7 it/s]49/100 [01:10<01:14, 0.7 it/s]52/100 [01:15<01:11, 0.7 it/s]53/100 [01:16<01:09, 0.7 it/s]55/100 [01:19<01:06, 0.7 it/s]62/100 [01:29<00:56, 0.7 it/s]67/100 [01:39<00:49, 0.7 it/s]71/100 [01:46<00:44, 0.7 it/s]73/100 [01:49<00:41, 0.7 it/s]75/100 [01:52<00:38, 0.7 it/s]77/100 [01:55<00:35, 0.7 it/s]79/100 [01:58<00:32, 0.7 it/s]80/100 [01:59<00:30, 0.7 it/s]82/100 [02:03<00:27, 0.7 it/s]84/100 [02:06<00:24, 0.7 it/s]85/100 [02:07<00:23, 0.7 it/s]87/100 [02:10<00:20, 0.7 it/s]88/100 [02:11<00:18, 0.7 it/s]90/100 [02:13<00:15, 0.7 it/s]91/100 [02:15<00:13, 0.7 it/s]92/100 [02:16<00:12, 0.7 it/s]97/100 [02:22<00:04, 0.7 it/s]99/100 [02:25<00:01, 0.7 it/s]\n",
      "146.343924 seconds (1.34 M allocations: 83.844 GiB, 6.29% gc time)\n"
     ]
    }
   ],
   "source": [
    "Random.seed!(0x466f756e64205521)\n",
    "with_logger(SimpleLogger(stdout, Logging.Info)) do\n",
    "    nodes = [100 50 10]\n",
    "    activations = fill(NeuralNetwork.Logistic, (1, 3))\n",
    "\n",
    "    # local softmax(x) = exp.(x) |> x -> x / sum(x)\n",
    "    # local loss(y_hat, y) = begin\n",
    "    #     y_hat = softmax(y_hat)\n",
    "    #     -dot(y_hat, y)\n",
    "    # end\n",
    "    # local deri_loss(y_hat, y) = begin\n",
    "    #     y_hat = softmax(y_hat)\n",
    "    #     gram = - y_hat' .* y_hat\n",
    "    #     for idx = 1:length(y_hat)\n",
    "    #         gram[idx, idx] += y_hat[idx]\n",
    "    #     end\n",
    "    #     -gram * y ./ y_hat\n",
    "    # end\n",
    "    local loss(y_hat, y) = y_hat - y |> v -> dot(v, v)/2  # scalar\n",
    "    local deri_loss(y_hat, y) = y_hat - y\n",
    "    \n",
    "    local kwargs = [\n",
    "        :Nodes => nodes,\n",
    "        :Activations => activations,\n",
    "        :α => 1,\n",
    "        :max_iter => 100,\n",
    "        :loss => loss,\n",
    "        :deri_loss => deri_loss,\n",
    "    ]\n",
    "    global net = NeuralNetwork.initialize_net(images, labels; kwargs...)\n",
    "    \n",
    "    @time NeuralNetwork.train(net)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check the accurancy of the NN model we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_num(v) = findmax(v) |> v -> (v[1], v[2]-1)\n",
    "validate(X, Y, model) = [\n",
    "    reshape(model(X[:, idx]), (10)) |> findmax |> v -> v[2] == findmax(Y[:, idx])[2] for idx = 1:size(Y, 2)\n",
    "] |> sum |> s -> s/size(Y, 2);\n",
    "predict(x) = reshape(model(x), (10)) |> to_num;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurancy is 0.9715333333333334"
     ]
    }
   ],
   "source": [
    "print(\"Accurancy is \", validate(images, labels, net.model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And using the rest part to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start reading images. Read 28×28 data 10000 times.\n",
      "Start reading labels. Read 10000 data.\n",
      "Accurancy is 0.8902"
     ]
    }
   ],
   "source": [
    "images_test = open(\"data/mnist/t10k-images-idx3-ubyte\", \"r\") do f\n",
    "    if hton(read(f, UInt32)) != 2051  # magic number\n",
    "        println(\"image file format is not legal\")\n",
    "        return\n",
    "    end\n",
    "    training_num = read(f, UInt32) |> hton  # big endian\n",
    "    row_size = read(f, UInt32) |> hton\n",
    "    column_size = read(f, UInt32) |> hton\n",
    "    sample_size = row_size * column_size  # N features\n",
    "    println(\"Start reading images. Read $(row_size)×$(column_size) data $(training_num) times.\")\n",
    "    # fill data\n",
    "    images = fill(Float64(0.0), (sample_size, training_num))  # [x1, x2, ..., xM]\n",
    "    for idx = 1:training_num\n",
    "        images[:, idx] = read(f, sample_size) |> Array{Float64}\n",
    "    end\n",
    "    return images\n",
    "end\n",
    "\n",
    "labels_test = open(\"data/mnist/t10k-labels-idx1-ubyte\", \"r\") do f\n",
    "    if hton(read(f, UInt32)) != 2049  # magic number\n",
    "        println(\"label file format is not legal\")\n",
    "        return\n",
    "    end\n",
    "    labels_num = read(f, UInt32) |> hton  # big endian\n",
    "    println(\"Start reading labels. Read $labels_num data.\")\n",
    "    # fill labels\n",
    "    labels = fill(Float64(0.0), (10, labels_num))\n",
    "    idx = 1\n",
    "    for label in read(f, labels_num)\n",
    "        labels[label+1, idx] = 1\n",
    "        idx += 1\n",
    "    end\n",
    "    return labels\n",
    "end\n",
    "\n",
    "print(\"Accurancy is \", validate(images_test / 255, labels_test, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. Now we could draw some numbers and send it to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tbody><tr><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAANtSURBVGje7drLa5xlFAbw30yaGpM2tgiWqqiV1hqol0VVXIgK4gW8FrsQt0IXRRfu/APciTt14UYsgrhREEFBFMGFiBWKqC3e4gVsGhPFW2qSTlw88zEzadIkRfuFlznwwcx3ec97zsO5vw1d1Oj6vdB1bzO2ooU/2/ebmMdJ/GP11FzDu/8Jlc+wcaaHQ9iLfbgZ52NccBzAH/gWh/EZJnSwXzcSls9wWQyHcS+exB7Bs7K92faHCziFabyHQ/hIbHPdSFg+wyUxHMCdeBpXYYNgNyeYLWCwfVU7nscXeAZv4e/1ImH5DDcsdfNKHBD8mjiGF3G0/cEmbMMYbsJuwXMPnhL83hFca5ewfIanYXge7sYtYo+/4nm8jJlFO92EXXgEj2KL4H4Q34ld1i5h+QxPw/BS3Cfx7xTexZt68SM56u+Sz/wk+c3jGJH8Zx9+0Mlja5OwfIY9GDZwPa5t72QKb+DnFRaZwEvYiYcE/wfxNj6pW8LyGfZgOIwbxZaIL/zUyvUCfI9XxAYvETxvl5qju34sX6X1YrhF8pIB8aNHcGINix3Gh9iPjbgVr+LHOiUsn2EPhhdKPGxITvm1M9d6i2kKH+B+6QeMYYc+hv8z9WB4AUYFwzlMSu6yWmrhc4mPV0h/bqfYZrVO+SqtF8ONUucRnc+dxYITOC4YDuKy9rqVPZev0noxrPowRP+jZ7HgjNQcxJ63LmJSvkrrxfC39nWR2M6YxLWZNSw4YJnmT10Sls+wR90n8JX0XgakNnhd8s3V0jZcLDbYwi96e27lq7ReDKfxvmA3IrX+QTyLL3X87HI0irskFpIYeEzmHLVJWD7DHgxnpbd2D26TmPgwtuM1mSsdl572vNhZU/zudunRPSb+tyX1/RG9uW35Kq2/X/oNntOZS1S1+t72s6MyB54SzIekprxO+nSbpa8zLX2b8bolLJ/hkvPDIdyBJ3BD+381863mvtXvhsTOpk4MnMILMq+arFvC8hkuO8cfxNV4QHzrLvGRzSV2uSDYnRT/eUhmHZNLrFu+StcPhhWN4HJcI73UHVJ7jIj9zcqMcVz64x/LGZvlzkmVr9L1h2H3i0Nii8MyK67O18zgL8l1Vspdy1fpOWfYpz71qU996lOfVpnTNHQidUNnltRa9M5qzmyUn2Kcc4b/AlrHrs4LnIQUAAAAAElFTkSuQmCC\"></td><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAIhSURBVGje7dpJa1NRGIfxX5raSkVqwYKCUgQFRRxABedh6daVn8Jv1O8gbgVRu2lBxOJUxWGhIjghiEapti7egMVFc3NLybmv3tUNyblP/nnIGd5zWlZcrRX3y9bnGlqn5/7DwNbaH0EbS6p5z/+TNs/hThzFczwQLotKmB84vJbGW3EFlzGNp/heWsL8wNoOWziOS9iIN1gsMWF+YG2H47iISbzFI/wqMWF+YG2Hu3Gy+41f4XWpCfMDazncgBOYEnPRBXwqNWF+YC2Hk7iAUfzAPL6VmjA/sJbD/Tgs5jXv8FC1sXAgCfMD+3Y4hrNiXbGMx3hZcsL8wL4d7sCZbsNFzKo+Fg4kYX5gXw6HcAT7RD/6HnNiTCw2YX5gXw63iLnMJlFTu4cnpSfMD+zL4R6xpmiLmtptfCg9YX5gZYcjOCVq3MSafla1+tpAE+YHVna4TfSjI6IfncOzJiTMD6zs8CAOibnMV9zE5yYkzA+s5HAzzmOi+3oBd/TeKywiYX5gJYdTON398E/MiDp3IxLmB/Z02MYxMSeFj7gl+tNGJMwP7OlwAufEPu8S7oozF41JmB/Y0+EuUZtpoYPros7dmIT5gas6bOMAtvtT357R/5pwoAnzA1d1OIq9Ys+3g6t40bSE+YHDvd4cF/+7G7im+l5vMQnzA1d12MH97v20enWZgSfMD+x5RnhM1Ne+rGgwImreS3896P8574EAfwPD81PkTMD6LwAAAABJRU5ErkJg\"></td><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAMiSURBVGje7drPi5VVGAfwz73NjGMzY1hD6mQDuqi01HIn0kJw40KMNKIWLhKNCoJA/4p2LXQdBS0SSsiVFCKWiD9gGBmLFFF0qCaxkRln6s44Lp53nHvn570T9L4d5sCBc997zv3e7/nynPP8eEuqWqlqPKG2lbAs6zCKvzXeyotY869a+oClhSY043m8hlfxnND3Gn5Aj9CzsAzTB5xXw1XYhbewBW3ZgjLG8TM+xXf4p6gM0wecU8MX8AH2ogN3he39hhXCLp/BJXyM3qIyTB+wabaH63EEb+ABvsa3uIJBYY/v4jA24XVcxVgRGaYPOEPDFdiPPfgDR3Ecv1fNGcRJvIlXhE22Zc8LxzB9wBkabhbn54jQ73MMzbJwAHeEHa61pGGOgE3TP2zFGpzBCbPrB5Wq79rRWlSG6QPWaFjGkyJ2GLCwXU06RE808M/T39J8NawIP/OWmTH+9NYs9CbiikpRGaYPWKPhBH4SscIohudZ2I7ObHxP/XF++luav08zjB/rWNhtKmdzy/x658owfcCmxSxqxXY8K87RHuHHFpJh+oCL0nAzdmeLfxF36HhRGaYP2LCG3TiAl4Ufc1Lk4ArLMH3AhjRcg0MiP1PGOXyj9i4sZ/1h1nNnmD5g3Rp24328h+W4gWO4KbRdhS6sFjnyB+jDebV51PS3tDh1i+oJG/Ah3hb6DeALXMeL4lxdJ2KNyXzNGC7gIPrzZJg+4LwaLsc2ES/uQIs4N28IrVZnc8pVPzQuYsXz+Ayn1J6p6W9pcTTswjvCjtaJfNq4Kf+zlPUJoVFF1Bh7hW7fC19nej0q/S3N/z5sw058JGLAVlNaVetWETZ5W9QOL4s48VdRs5qrrp/+luZrhy/hE+zDSrXnY0X4Kf3iHYweYXOTtf376suZpr+l+dlhp6jL7xf5bMLeRkQcf1b4KH3C9v6y9F5bQQAfa/gUNmbjCaHPRXyJ08L+hs0e8xWaYfqAjzX8E18J/YZE/feE0G6hOlShGaYPWHMfduBpce8NpMIwfcCl9v9vC+ZpJieVq8YtIo5/OG1OPWdu+mbxnwM+AnLupBTDqzFCAAAAAElFTkSuQmCC\"></td><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAM/SURBVGje7dpdqKVTHAbw33v2PuaMYwyJMdMMhRkpV5SPwsWEGyQlkq8kTIlSXEluXJHccKG4oJSQJCUajTsljY98ZeQzCXMxo5nz4Thnu3jek132x5kZdZaVVbve1lr7ffazn9Za/+f/X42+1vQ99/yzrcGJ2IyNmMYB7ME3mDW+Taxgzr/a6gdsVjJpGmdhOy7GVhyPDv7E13gKrxmvY/1/aVkaNjgN17afM9BtxxZlrU628z7EHfiiNIb1A3aHDXRwHu7FZbKP/oEv8Qm+wkFciUtkfz2hRIb1Aw7UsBH9HsYFbd9uvIp38b2cg2fiGizhN+wrkWH9gAM13Igdot88XsGz+FTWImzCbThHzsRd+LFEhvUDDtTwdFzUPr+MR/FD3/gm3IUb2xfsbuftL5Fh/YADNZyTfXFWtFnWr8E23Cn6Tcu++iQ+LpVh/YAD49J14iGmsBO/42jZW3fgUhwl+j0mOh8slWH9gEO9RadvcJvELteL1yC+/gmJcw6UzLB+wKHeYlFim1txs5yRXfGEeyV+OQnn4rO2r0iG9QMOXYdT4tkfkb21Ef0WJQ5dxIJ4ip14TmKbhdIY1g84dB1Oim+fEO164ivm2+eOnImbJb7ZKn7yPYNzravGsH7AoRrO4g2cIjp9i49kz+xI7vtsXChe43zxi3tkbRbDsH7AkfnSSX/ntveL51hqv9TBcbgcD+FUfIdb8EFJDOsH7I4aXMCvA/p7cibulfzMDaLhetG8KIb1A3aP9AUbcHL7PIeZ0hjWD3hEGq7H1RLP9KQO/FNpDOsHPGwN1+Iq8Y5rJF7dhV9KY1g/4GFpuBZX4H6JSXvi89/0fx3/v6DhsbL+HpAcXCMx6/PG1/BXhWH9gIek4QZch3uwRfSbx0t4XWKa4hjWD7giDSckbrkdN4l/aMR7vIWn8XOpDOsHHKvhlNSc7pZ7NVNt/wLekdrT5yUzrB9wpIbHSOxyn+TVlifPSfzyuOS5R+VHV51h/YAj77Vtx4NSM+xIrWIfXsQzcsftUPRbFYb1Aw7VcJ3UfLeIdjNyh/QFOQPH+cBiGNYPOLJu8bbcw5iRO6Xvy/2L5V+61Dd/ub5YHMP6Af8CPoyf5PRLDe4AAAAASUVORK5C\"></td><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAALdSURBVGje7drJalRBFMbxX3c6iUmIBAPOigpO4Bx16cKVG7e+gvoALnwBn8CFTyFuRFy5VMSFKKKIIqiJEo1mMAMm6XZxbkgkuenQovdSpqCg6K5bX777Tw3nVFcsKZUl7Ya/U6p/adz/WLCy1g7NmFbQjp9lc5i+4KoMO3AUNTzDdE6/NpzHGdzCtzI5TF+wttqX+3FDsLyM1zn9tuMK9uGudYb/uOQyrOAUjuODWCfzysGs37SYk6VymL5gLsNenMUGfMTIKgMcwya8xVTZHKYvmMtwN06Ls8xzjOb02ygY1jAhf88szGH6gisyrGJA7G8/8AQzOQNsxiGx9o5aZ1gWhn04J9bRF1nNK3uwQzD8Kp91YQ7TF1yR4V4xDxt4hE+rPHxA7J11DFuPD8vAsIqT2IlxPJR/TukSZ9Ia5gTr2bI5TF9wGcMenBCxxDu8zPkr27BVMKxiHp3iLNslYsq6+D94j6GiHKYvuIxhN3YJRtMizt8imHSLdbMP/eIsM5B9V8FVEet3ZE7q4kz0EtfxpgiH6QsuYzgn5g6RO7sp9sWaYNOetatZe4PgV8G27PlZjGFQ5OjuiTxBIQ7TF1zGcAy3cVjEFv1+z3s3xLo5Y5FrBV9wX+Tj3md1UJxzxpeMn/4rLUfOuxtHROy+WXCaE9wmRSw/hYu4lA1yB9cEszn59xzpv9Li5yHB57GI7dssgq5ntSH2xQtZuy7ybCPWz6VlYbhQFpitVLrEHK2KmHBIc36FOExfsNbqg90iz00w/Cz2ydI5TF+wZYY9It9NxA9fy+owfcE/moddWXsc38vqMH3Blhl2iviwIe7tJ8rqMH3BlhlWs9oQseFkWR2mL9gyw9mszot7/mb3hoU5TF+wZYZj4s63D680vzcszGH6gi0zHBZxfRueWltcUYjD9AVbZjiCB/Lvp0rjMH3Bpr/zXuhUXdLuEGtnb9b+kn2+lt/3p/9K/7ngLy/ajAWRrgJKAAAAAElFTkSuQmCC\"></td><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAALiSURBVGje5drLi41xGAfwzzHHZdxGo9zJPQvJpVhJFtYUsnApKylSFhR/goWdNbIRssGGFMmChES5NmXILcltGOaMxfNOjuYMZ0ya3/z86u28533Peb/v9/n2PL/f8zy/kqpRqjrv9G/GoH/03P8YsNT3R8QoYw6m4Cke41sKDPMH/K2GIzCxeKuPaEdHca+MYRiNSViGjRiHK9gtdOx3hvkDlnu60YCl2I9GtAkN34k426VfM8ZiDIYX/51WnJd0j8n5mzQdDTuFbt+wAKOKH3/1U5cujTrxHc/wAMfQovacmr9J04qlQzAba7Cz+P5QxNMKPuOt0O4h7ov4+VL4bBIM8wcs/+5mO+5hqvDHFuzBc+Fj7cJX24R/dvjzyN+kaWnYNYZhcPH5Qe21SrIM8wesS8MPws/GYz5u+/v8MX+TpqnhUxFHF2Ed7hRHZSAwzB+wrhy/ETvEXFgW+d9JXBPrmY/q1zN/k6apIczAXqzFUHzCE9zCVaFni9q1mX5lmD9gr2pt07EeqzFXaEmsS2/hMM4Iv0yGYf6Ava6XDhc+uUTU1xZiVnG9VfjqWT3H1vxNmr6G1W86UtS4V2AbZuII9om1bBIM8wesa11aa1TwXtQAXmG58M+mPzw0f5MOHA2rxwxRk6vgkZgfk2GYP2CfNZyATULDVlzCl5QY5g/YJw2bsVnkGxWcws3UGOYP+NcajscWbBe94nOi5/Q+NYb5A/ZawwbMw1ZsEGvTyzgo5sLkGOYPWLeGDWLPxQqx52JZcf0CDuBGqgzzB+wxPxxaHKMwGYuxSmg3WtTaTuMQ7qq/j5G/Sftfw7LoT6wU9ZcposY2QfTxK6J/eBQnRM07aYb5A3aLpU3YJXyuseqNOvAC53Ec18VejOQZ5g/YTcM2XBR5+yTR+20Vml0UucObgcQwf8Ca8+FIET+bRb7+Cq/FHDjgGOYPWFfNu1T1ZiUxL37xa2+i1l7SJBjmD/gDW8eMJrKWQJUAAAAASUVORK5C\"></td><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAALrSURBVGje7drJj0xRFAbwX3W3pg2FkCBpQ2KKIYYEicSUEImVOTbCws7Gxr9iIYYgFhaCRIKFWFhYIcRCGxaChXQjZq3pbovzKopUd1cT6tXVN6nUe+/eut/76ss595xzb0FZK5Rd9/o7reEvzfsfAxb+fIofb95rYO3T/0vrT8MxWIiVQr9jeJcnhukDNv3uD1uwDDuwCu04gg95Y5g+4G/Z4WTswm6Mx3mcwH305I1h+oCD1nAW9mMnPuEwTuFlXhmmDzgoXzoXB7FFrHmHcBJv88wwfcCqNZyOA9iOLhzHaYPTryYM0wesSsPx2Cv8ZwMuCv/5uh4Ypg84oIZN2IA9Iha9J/KHZ/XCMH3AATWcjX0iFu3EGdypJ4bpA/ar4UhsxgoRwN7CZaFl3TBMH7BfDedhG0YI3S7iSdbXmH1KNbZufMsjw/QB+9SwBRsxJ7tvw03hW2eiVcQ6w/EVr/AUj8Ra+SkvDNMH7FPDqULDZmFjbSKvWC3yjJbsbUtFgm58Fhqew1k8zwPD9AEraljAEszPrj8K+9ss/GrBzwWeAoZlfctFLWCUqAG8qTXD9AErajhKxDEjs/sXwnd2ifrMG5Hbd2YTjMMUTBBaFoXeFwxp+A9aRQ3HCRtsFDXsW6Ku3Si0e4X3Yh0sYgG2YhPGCrsc0cfk6f+l+dCwKPLBgljnHuOKiDsbhX1OwmKswTrha5uzMV9wQ9hvzRmmD1hRw1LeUMi+Z2O98JNThY0uyp4Xy8b2inj0Ko6iIw8M0wesuPc0TewprRUid4p1sEnEo01+jkl7hO09FD73vLDd7jwwTB+woh124BKWivVtOCaW9ZfOzXSJuvddXMN1kVt8zBPD9AEravhZ5HijxV5FazawJ+trF1rdFvHOg+xZVx4Zpg/Y7z5+UdRkZoizT1/Efv0Lodlb1elWU4bpA1Z1FqOU01dz9jB3DNMHHGpDbajVvlXtSxvKrptFvtHzy5hq/Gz6ru2fA34Hti6GDloTdx0AAAAASUVORK5C\"></td><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAKDSURBVGje7drPb0xRGMbxz7SlreogFUGT2hEhDQmRWGAhVhIiLIl/QeIPsBdLayshLISEjUSUjRVhY0FiI36kFaSpakPH4r2TTqcV7cy09+aYNzm5N/eeOc8895vz473nltREqea8YnmiY5na/Y8F29GOduQfpaVWrv9Bpe74r0h/aFtxwdJCF8rYgLVYn5V+9GJ1zb+s4BemMYlxfMVoVsbNZ5v+I11xwa76C2VcxEHBrow1ZtnV98VKVmYEywl8wnPcwzP8yNNh+oLzGP7GWHajF51m+ZRqSkfNsSOrt1ow34o9OIRLeCD6ay4O0xdccD7swxYxnvahWzDqNJdZl+DWk9Vbh83YieHs/l1cEGNrLg7TF+xa6OIE3i6ygdq+2Sl478Zl7MWQ4NtmuGzR1WwD1fmQGIenMSW4VcQaZypPh+kLNs2wPrpxGNvEHPgC3/J0mL5gyxnuwEkxT37EU5F35OYwfcGWMuzHKbGeqQh+r/J2mL5gyxiWcACnRR8cFfnhWN4O0xdsGcNBnBPz4Awei/y+/Z5m2aMlDHtwHMdEfvEBt/G5CA7TF2wJw2HRB/vFWvS+hftgLg7TF2ya4SacFXl9CW9wC1+K4jB9waYYrsJRnMgamsRNkRMWxmH6gk0x3I7zGBDrmCe4Y+4+Re4O0xdsmGEZZ7BfjKHvcQ3viuYwfcGGGe4zmwv+xHWMiHemhXKYvmBDDAcEvyHB7CFu4HsRHaYv2BDDXTiSnb/EVYvfq0r/kRafYafYo9+I17ji77lgIRymL9hQPyzjkVjDjIg9w8I6TF9wSd8mVmNQvI8ZFTlFoR2mL7gohtVvn6rn1bXoTF2dxYyn6T/SFRf8A+LUaU8bNcMkAAAAAElFTkSuQmCC\"></td><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAANCSURBVGje7drLa11VFAbw3+21SWNDpbUBG+sTQSOFUnx2ICKiIihSdCKKgiCKk04KBTv3DxAHRaiooCCKKIiCz4oTtT4GPhHEgTaaVlu9NqVNm8TBd0pukmuSCs09bO+Cw3nte7679sd67LV2Q5s02q6nnRlZcYa++z8G7ElPetJ9aSxl0ADWYT3WVM8OYRR/Oj2/W75rW3bABTkcwmbcgKtwIQard4fwMV7EPpyoq4blA3bkcADX4j7cJFw2MVUd2u4/w+P4tK4alg941twHq3AnduByIfkvfIev8XM1bjNuw9W4C99gvI4alg84j8MrsR0j+Bvv4nV8gQM4JvFvBOfj+up6tR6HXQKcxWETW3AFJvAMdmO/2XnLaomPF1f3h/XiYRcBZ3HYQH91Po4/qvNKnJT4N4h7xNdukNzmHbTqqmH5gLM4PIlv8RsuwGO4Bj9gTHzpRXi4en8cL+A9TNZVw/IB5+Wla/AgHpV41xD7mxR/2i92OYFX8AR+rLOG5QPOy2laeE7yzBslNq7DubI+7Bd73YfXxJc2LH2dX/6Udt8O22WVxL8t2Ck5aEPi5Ljw97nkrZ/gSB01LB9w0VrbMHbhfvGh+8WPDqNPfOxPeBIvWzy3KX9Ku+9L2+Vs3F0dK4Wfp/GL1HC2VmMukzx1DG9ZOL8pf0rrw2ED1+EhyXNO4FXJQw+KD90mOerG6tgmNdTf66Rh+YD/yuEGPCA2No0vsUd6FWS9sVv43V59aARr9ThcZunIYVN6FbdW14fxLL6aM25cek9T1TgWz0/Ln9J6cLgWt+Mc4WQv3pZcpv2HW3GHxMpJfC+c1krD8gE7cjiETWJbR/E+fm37h0PSk3pE6qbTEiPfEJutlYblA3bksM9M3XQFLhWumrLmv1nW/+urMS2JlXstXnMrf0rrsT7ciKdwi/B2RGqlTanXDJipwY0Kf8+bsdVaaVg+YEc7PICXpJc4LL2mS6p302JrLXwkezE+lD5xLTUsH7AjhxN4U/i6V2LjgPQpRmWd8YHU10bN7M+opYblAy5Ya+vDedJnGpT8Zqw6Wv7bPuLyp7S3R7gnpy1L2l96Kj89dd0n+2qm5oxZil2WbxbLDvgPKremGL99AToAAAAASUVORK5C\"></td><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAMMSURBVGje7drNaxVnFAbw3yQxqV9NLaZQq1LF0oqoUQShreJOcKEoUrqxFQRxU+gfINJ9V+5cuRAEQRciuIpY1EUa0vrRYEQRP1HUGiklWjUx18UZyZcmcxd65772hWHmzpy5zzzzcM57zpk3M2JkI44r3s5oeEv/+x4D/j/qf2RFjKbk+4EJbFrQjKeT2KXvFu8ccEINP0Q7VqIRF/AH/hlj14Yt+BIncFJoWQqG6QM2venCJ/gB2zFHiP0Ah7AfN3O7ltxud368HL24URaG6QO+VsOZ2Iaf0YqHeIJPsQvT8Svu4wvsyO1g1iQs0n+ltdcww7eGdenFXtzGBmzKt+Pow/dYlN83iE7hr6VhmD7gOA3b8B3minlvH47gOXrwm5j3bmOd8NcmUYvcwUH0l4lh+oDjNPwK3wi/6kaH0I/QtAO/YzV+EVpnIhc9iD/LxjB9wKaxP5ZiNl6IuHh/xPVGfI6N+BGLhX4VXMIBPCsbw/QBR2nYjPn5yf9wXfhXMz7DGmzF15iWP21F6HYY18rIMH3AURpmop7P8idZiPWiRlyHFSJnJfKXodz+Lk6buLavGcP0AUdpOCBylUHMwE/Cz1pHGA7gKu5hbW7bk58rJcP0AUdp+Bxn8JfwuY+FhkMi17wkcppu7BTz4yC6RJ1RSobpA47LS3uwB5tFLH0q5rmzotd2C8sM5zN9QtPJcpmaMUwfcJyGz4QvnhN5yws8FjlORfheu+jZVHARl8vMMH3A1/bahvBvvo0draI2bDbcl/m7zAzTB2yq9oa5oq/dIHpqXYrH0ZowTB+wKg0zLME84as9qoujNWGYPmBVGk7HKkwVcfSMifvbpWCYPmBVGraJOJqJur7TcC+1tAzTB6xKwwWi3qiI7/lX6oFh+oCFNWwUNeFHomY8hUf1wDB9wMIafiC+2TeIfk2XqB1LzzB9wMIatog1NgNi3dP1emGYPmBhDV+tGe7FMdG7qQuG6QMW1rAf53FU9MTrhmH6gIXWCL/6njhN5DNT8v3QGJsi6/vTf6XvHPAlyZmXHdahEqQAAAAASUVORK5C\"></td></tr></tbody></table><div><small>(a vector displayed as a row to save space)</small></div>"
      ],
      "text/plain": [
       "10-element Array{Array{RGB{Normed{UInt8,8}},2},1}:\n",
       " [RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); … ; RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0)]\n",
       " [RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); … ; RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0)]\n",
       " [RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); … ; RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0)]\n",
       " [RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); … ; RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0)]\n",
       " [RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); … ; RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0)]\n",
       " [RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); … ; RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0)]\n",
       " [RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); … ; RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0)]\n",
       " [RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); … ; RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0)]\n",
       " [RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); … ; RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0)]\n",
       " [RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); … ; RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Images, Plots\n",
    "imgs = [Images.load(\"pic/mnist/$(pic_num).bmp\") for pic_num in 0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Tuple{Tuple{Float32,Int64},Int64},1}:\n",
       " ((0.6775051, 0), 0) \n",
       " ((0.87202424, 1), 1)\n",
       " ((0.7262942, 2), 2) \n",
       " ((0.41015354, 5), 3)\n",
       " ((0.5178301, 4), 4) \n",
       " ((0.2307581, 3), 5) \n",
       " ((0.9925464, 6), 6) \n",
       " ((0.70303345, 3), 7)\n",
       " ((0.46103132, 3), 8)\n",
       " ((0.37600863, 4), 9)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process the images\n",
    "img_formatted = map(imgs) do img\n",
    "    img = img .|> rgb -> dot([rgb.r, rgb.g, rgb.b], [1 1 1]) / 3\n",
    "    img = reshape(img', 28*28)\n",
    "end\n",
    "\n",
    "# start predicting\n",
    "[\n",
    "    (predict(img_formatted[idx]), idx-1) for idx in 1:length(img_formatted)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model just works well on some of these hand writting numbers, but provides completely wrong answers on others. The reason could be that samples in mnist are all well processed to be center and size adjusted, while these hand writting images are not. So our test accuracy could gets 94.8% but the model doesn't work well on these numbers above. This is one disadvantage of normal backpropagation NN, and CNN could get over it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work With GPU\n",
    "\n",
    "A lot of matrix calculations and space management are expensive. This heavy task needs some time with CPU. And with the scale of NN increased, CPU takes all day to train one NN, which may be a small part in our machine learning process(Because we need to tune the hyper parameters). _\"There's a limit to CPU ability... ~~I reject my CPUnity, ANN!~~\"_\n",
    "\n",
    "So to improve the performance on training huge scale NN, we are supposed to assign more tasks to GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use [CUDA](https://github.com/JuliaGPU/CUDA.jl) in juliaGPU. If we directly convert _Array_ to _CuArray_ in the implementation, we could hard found GPU performs better, because the assignment from CPU RAM to GPU RAM needs more time. So we need to think about the better algorithm for GPU.\n",
    "\n",
    "Recal the work flow of training a BP neural network, every hidden layer need the output activations from former layer and weight loss from the later one. The sequencial work flow could not be parallelized easily(Or we need some special theory to rebuild such time sequencial problem, e.g. the [Parareal](https://people.math.carleton.ca/~elorin/AMAI.pdf) algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
