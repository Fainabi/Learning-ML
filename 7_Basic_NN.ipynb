{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic NN\n",
    "\n",
    "This IJulia shows the basic concept and application with neural network, which has some matrix calculus.\n",
    "\n",
    "Firstly we set $$X = [x^{(1)}, x^{(2)}, ..., x^{(m)}]$$ as the inputted data, where $x^{i}$ is an $n$ by $1$ vector with $n$ features. Then we have an $N_{i-1}$ by $N_{i}$ vector $w^{[i]}$ to represent the _weight matrix_ at the $i$th layer and an $N_i$ by $1$ bias vector $b^{[i]}$.  \n",
    "\n",
    "Thus at each layer we could have the calculation:\n",
    "$$z^{[i]} = w^{[i]\\mathrm{T}}a^{[i-1]} + b^{[i]}\\\\a^{[i]} = \\sigma(z^{[i]})$$\n",
    "where $a^{[i]}$ is the activation ouput from layer $i$. \n",
    "\n",
    "It is all known that there are a lot of vector and matrix derivatives in NN. So we need to analyze them completely in order to get a deep comprehenson of NN.\n",
    "\n",
    "We construct a fully connected NN here: \n",
    "<center><img src=\"pic/342NN.png\" width=80%/></center>\n",
    "with 4 layers which have N, 3, 4 and 2 nodes in each layer. This may look like a binary classification applied in tasks such as picture classification(However CNN performs much better in these problems)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost/Loss Function\n",
    "\n",
    "To train a NN, we need some rules and directions to point out that how to adjust our $w$s and $b$s. And that is __Cost Funcion__ and __Loss Function__.<br/>\n",
    "Loss function is used to measure the performance of NN describing the \"distance\" bewteen the output of our NN and the true labels from dataset. Usually we would like to minimize the loss function to make NN work better. In the former NN we constructed, we have two output values. And we need a loss function $$L: \\mathcal{R}^2 \\rightarrow \\mathcal{R}$$ Here $L$ must be a functional, or it would generate a couple of conflicting NNs.\n",
    "\n",
    "Because in each layer, the weight edges are independent to each other, it's same to calculate $\\frac{\\partial L}{\\partial w^{[i]}}$, $\\frac{\\partial L}{\\partial w^{[i]}_j}$ and $\\frac{\\partial L}{\\partial w^{[i]}_{jk}}$ for updating the weights. For convience, we will using the matrix derivative to calculate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients\n",
    "\n",
    "With loss function, what we need to do in training NN is using the gradient to update weights and bias\n",
    "$$\n",
    "w^{[i]} := w^{[i]} - \\alpha\\nabla_{w^{[i]}}L \\\\\n",
    "b^{[i]} := b^{[i]} - \\alpha\\nabla_{b^{[i]}}L\n",
    "$$\n",
    "and with BGD, we could have\n",
    "$$\n",
    "w^{[i]} := w^{[i]} - \\alpha\\frac{1}{m}\\sum_{j=1}^m\\nabla_{w^{[i]}}L^{(j)} \\\\\n",
    "b^{[i]} := b^{[i]} - \\alpha\\frac{1}{m}\\sum_{j=1}^m\\nabla_{b^{[i]}}L^{(j)}\n",
    "$$\n",
    "So all we need to do is to calculate $\\nabla_{w^{[i]}}L$ and $\\nabla_{b^{[i]}}L$. With [chain rule](https://en.wikipedia.org/wiki/Chain_rule) applied, we have\n",
    "$$\\begin{align*}\n",
    "\\left(\\nabla_{w^{[3]}_j}L\\right)^\\mathrm{T}\n",
    "    &= \\frac{\\partial L}{\\partial w^{[3]}_j} \\\\\n",
    "    &= \\left(\\frac{\\partial L}{\\partial a^{[3]}}\\right)_{1\\times2}\n",
    "       \\left(\\frac{\\partial a^{[3]}}{\\partial z^{[3]}}\\right)_{2\\times2}\n",
    "       \\left(\\frac{\\partial z^{[3]}}{\\partial w^{[3]}_j}\\right)_{2\\times4} \\\\\n",
    "\\end{align*}$$\n",
    "where $w^{[3]}_j = w^{[3]}_{:,j}$. And because of independency, we could have\n",
    "$$\\begin{align*}\n",
    "\\left(\\nabla_{w^{[3]}}L\\right)^\\mathrm{T}\n",
    "    &= \\frac{\\partial L}{\\partial w^{[3]}} \\\\\n",
    "    &= \\sum_{j=1}^2(E_j)_{2\\times1}\\left(\\frac{\\partial L}{\\partial a^{[3]}}\\right)_{1\\times2}\n",
    "       \\left(\\frac{\\partial a^{[3]}}{\\partial z^{[3]}}\\right)_{2\\times2}\n",
    "       \\left(\\frac{\\partial z^{[3]}}{\\partial w^{[3]}_j}\\right)_{2\\times4} \\\\\n",
    "\\end{align*}$$\n",
    "where $E_1 = [1,0]^\\mathrm{T}, E_2 = [0,1]^\\mathrm{T}$. Samely, we could easily calculate $\\nabla_{b^{[3]}}L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a more suitable form, we need to inspect $\\nabla_{w^{[2]}}L$,\n",
    "$$\\begin{align*}\n",
    "\\left(\\nabla_{w^{[2]}_j}L\\right)^\\mathrm{T}\n",
    "    &= \\frac{\\partial L}{\\partial w^{[2]}_j} \\\\\n",
    "    &= \\left(\\frac{\\partial L}{\\partial a^{[3]}}\\right)_{1\\times2}\n",
    "       \\left(\\frac{\\partial a^{[3]}}{\\partial z^{[3]}}\\right)_{2\\times2}\n",
    "       \\left(\\frac{\\partial z^{[3]}}{\\partial a^{[2]}}\\right)_{2\\times4}\n",
    "       \\left(\\frac{\\partial a^{[2]}}{\\partial z^{[2]}}\\right)_{4\\times4}\n",
    "       \\left(\\frac{\\partial z^{[2]}}{\\partial w^{[2]}_j}\\right)_{4\\times3}\n",
    "\\end{align*}$$\n",
    "and \n",
    "$$\n",
    "\\left(\\nabla_{w^{[2]}}L\\right)^\\mathrm{T}\n",
    "    =  \\sum_{j=1}^4\\left(E_j\\right)_{4\\times1}\\left(\\frac{\\partial L}{\\partial a^{[3]}}\\right)_{1\\times2}\n",
    "       \\left(\\frac{\\partial a^{[3]}}{\\partial z^{[3]}}\\right)_{2\\times2}\n",
    "       \\left(\\frac{\\partial z^{[3]}}{\\partial a^{[2]}}\\right)_{2\\times4}\n",
    "       \\left(\\frac{\\partial a^{[2]}}{\\partial z^{[2]}}\\right)_{4\\times4}\n",
    "       \\left(\\frac{\\partial z^{[2]}}{\\partial w^{[2]}_j}\\right)_{4\\times3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization & Broadcasting\n",
    "\n",
    "We have known that the built in functions for matrix calculations perform much better than explicit loop operation in Julia, as well as Python. So we'd like to find a more matrix tasty calculation, where we need the [Vectorization](https://en.wikipedia.org/wiki/Vectorization_%28mathematics%29).\n",
    "\n",
    "Because of the independency between $w_j$ in the same layer, instead of calculating $\\nabla_{w}L$, we can calculate\n",
    "$$\\begin{align*}\n",
    "\\left(\\nabla_{\\mathrm{vec}(w^{[3]})}L\\right)^\\mathrm{T}\n",
    "    &= \\frac{\\partial L}{\\partial \\mathrm{vec}({w^{[3]}})} \\\\\n",
    "    &= \\left(\\frac{\\partial L}{\\partial a^{[3]}}\\right)_{1\\times2}\n",
    "       \\left(\\frac{\\partial a^{[3]}}{\\partial z^{[3]}}\\right)_{2\\times2}\n",
    "       \\left(\\frac{\\partial z^{[3]}}{\\partial \\mathrm{vec}(w^{[3]})}\\right)_{2\\times8} \\\\\n",
    "    &= \\left[\\frac{\\partial L}{\\partial {w^{[3]}_1}}, \\frac{\\partial L}{\\partial {w^{[3]}_2}}\\right]_{1\\times8} \\\\\n",
    "    &= \\left(\\mathrm{vec}(\\nabla_{w^{[3]}}L)\\right)^\\mathrm{T}\n",
    "\\end{align*}$$\n",
    "And with _reshape_ function, we could transform it easily.\n",
    "\n",
    "Now let's look at the most complicated gradient\n",
    "$$\\begin{align*}\n",
    "\\left(\\nabla_{\\mathrm{vec}(w^{[1]})}L\\right)^\\mathrm{T}\n",
    "    &= \\left(\\frac{\\partial L}{\\partial a^{[3]}}\\right)_{1\\times2}\n",
    "       \\left(\\frac{\\partial a^{[3]}}{\\partial z^{[3]}}\\right)_{2\\times2}\n",
    "       \\left(\\frac{\\partial z^{[3]}}{\\partial a^{[2]}}\\right)_{2\\times4}\n",
    "       \\left(\\frac{\\partial a^{[2]}}{\\partial z^{[2]}}\\right)_{4\\times4}\n",
    "       \\left(\\frac{\\partial z^{[2]}}{\\partial a^{[1]}}\\right)_{4\\times3}\n",
    "       \\left(\\frac{\\partial a^{[1]}}{\\partial z^{[1]}}\\right)_{3\\times3}\n",
    "       \\left(\\frac{\\partial z^{[1]}}{\\partial \\mathrm{vec}(w^{[1]})}\\right)_{3\\times3N} \\\\\n",
    "    &= \\left(\\nabla L(a^{[3]\\mathrm{T}})\\right)\n",
    "       \\left(diag\\left(\\nabla\\sigma(z^{[3]}_j)\\right)\\right)\n",
    "       w^{[3]\\mathrm{T}}\n",
    "       \\left(diag\\left(\\nabla\\sigma(z^{[2]}_j)\\right)\\right)\n",
    "       w^{[2]\\mathrm{T}}\n",
    "       \\left(diag\\left(\\nabla\\sigma(z^{[1]}_j)\\right)\\right)\n",
    "       \\left(diag\\left(x^{(i)\\mathrm{T}},x^{(i)\\mathrm{T}},x^{(i)\\mathrm{T}}\\right)\\right)\n",
    "\\end{align*}$$\n",
    "\n",
    "Here $\\nabla L,\\nabla\\sigma$ in the right are all map functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And because the diagonal matrix(and block matrix for $diag(x^{(i)\\mathrm{T}}, x^{(i)\\mathrm{T}}, x^{(i)\\mathrm{T}})$) has many zeros, we'd better use __broadcasting__ to save the space. So we have\n",
    "$$\\begin{align*}\n",
    "\\left(\\nabla_{w^{[1]}}L\\right)_{N\\times3}\n",
    "    &=  \\left(\\nabla L(a^{[3]\\mathrm{T}})\\right)_{1\\times2} .*\n",
    "       \\left(\\nabla\\sigma(z^{[3]\\mathrm{T}}_j)\\right)_{1\\times2} *\n",
    "       \\left(w^{[3]\\mathrm{T}}\\right)_{2\\times4} .*\n",
    "       \\left(\\nabla\\sigma(z^{[2]\\mathrm{T}}_j)\\right)_{1\\times4} *\n",
    "       \\left(w^{[2]\\mathrm{T}}\\right)_{4\\times3} .*\n",
    "       \\left(\\nabla\\sigma(z^{[1]\\mathrm{T}}_j)\\right)_{1\\times3} .*\n",
    "       \\left[x^{(i)},x^{(i)},x^{(i)}\\right]_{N\\times 3} \\\\\n",
    "    &=  \\left(\\nabla L(a^{[3]\\mathrm{T}})\\right)_{1\\times2} .*\n",
    "       \\left(\\nabla\\sigma(z^{[3]\\mathrm{T}}_j)\\right)_{1\\times2} *\n",
    "       \\left(w^{[3]\\mathrm{T}}\\right)_{2\\times4} .*\n",
    "       \\left(\\nabla\\sigma(z^{[2]\\mathrm{T}}_j)\\right)_{1\\times4} *\n",
    "       \\left(w^{[2]\\mathrm{T}}\\right)_{4\\times3} .*\n",
    "       \\left(\\nabla\\sigma(z^{[1]\\mathrm{T}}_j)\\right)_{1\\times3} .*\n",
    "       \\left[x^{(i)}\\right]_{N\\times 1}\n",
    "\\end{align*}$$\n",
    "where $.*$ is broadcasting in Julia and $*$ is the normal matrix multiplication. Thanks to broadcasting, we don't need to reshape the matrix. One example for broadcasting is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3; 2 4 6; 3 6 9; 4 8 12; 5 10 15; 6 12 18; 7 14 21; 8 16 24; 9 18 27; 10 20 30](10, 3)\n",
      "[1 4 9](1, 3)\n"
     ]
    }
   ],
   "source": [
    "a = [1 2 3]  # 1 by 3\n",
    "b = [1:10;]  # 10 by 1\n",
    "println(a .* b, size(a .* b))  # 10 by 3\n",
    "println(a .* a, size(a .* a))  # 1 by 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is same for $b^{[1]}$:\n",
    "$$\\begin{align*}\n",
    "\\left(\\nabla_{b^{[1]}}L\\right)^\\mathrm{T}\n",
    "    &=  \\left(\\nabla L(a^{[3]\\mathrm{T}})\\right)_{1\\times2} .*\n",
    "       \\left(\\nabla\\sigma(z^{[3]\\mathrm{T}}_j)\\right)_{1\\times2} *\n",
    "       \\left(w^{[3]\\mathrm{T}}\\right)_{2\\times4} .*\n",
    "       \\left(\\nabla\\sigma(z^{[2]\\mathrm{T}}_j)\\right)_{1\\times4} *\n",
    "       \\left(w^{[2]\\mathrm{T}}\\right)_{4\\times3} .*\n",
    "       \\left(\\nabla\\sigma(z^{[1]\\mathrm{T}}_j)\\right)_{1\\times3} * E_{3\\times3} \\\\\n",
    "    &= \\left(\\nabla L(a^{[3]\\mathrm{T}})\\right)_{1\\times2} .*\n",
    "       \\left(\\nabla\\sigma(z^{[3]\\mathrm{T}}_j)\\right)_{1\\times2} *\n",
    "       \\left(w^{[3]\\mathrm{T}}\\right)_{2\\times4} .*\n",
    "       \\left(\\nabla\\sigma(z^{[2]\\mathrm{T}}_j)\\right)_{1\\times4} *\n",
    "       \\left(w^{[2]\\mathrm{T}}\\right)_{4\\times3} .*\n",
    "       \\left(\\nabla\\sigma(z^{[1]\\mathrm{T}}_j)\\right)_{1\\times3}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we could have:\n",
    "$$\\begin{align*}\n",
    "\\delta^{[i]} \n",
    "    &=  \\left(\\nabla L(a^{[D]\\mathrm{T}})\\right)_{1\\times N_D} \\\\\n",
    "    &.* \\left(\\nabla\\sigma(z^{[D]\\mathrm{T}}_j)\\right)_{1\\times N_D} * \n",
    "        \\left(w^{[D]\\mathrm{T}}\\right)_{N_D\\times N_{D-1}} \\\\\n",
    "    &.*  \\left(\\nabla\\sigma(z^{[D-1]\\mathrm{T}}_j)\\right)_{1\\times N_{D-1}} *\n",
    "       \\left(w^{[D-1]\\mathrm{T}}\\right)_{N_{D-1}\\times N_{D-2}} \\\\\n",
    "    &.* \\cdots \\\\\n",
    "    &.*  \\left(\\nabla\\sigma(z^{[i+1]\\mathrm{T}}_j)\\right)_{1\\times N_{i+1}} *\n",
    "       \\left(w^{[i+1]\\mathrm{T}}\\right)_{N_{i+1}\\times N_{i}} \\\\\n",
    "    &.* \\left(\\nabla\\sigma(z^{[i]\\mathrm{T}}_j)\\right)_{1\\times N_{i}}\n",
    "\\end{align*}$$\n",
    "and\n",
    "$$\n",
    "\\nabla_{w^{[i]}}L = \\delta^{[i]} .* a^{[i-1]} \\\\\n",
    "\\nabla_{b^{[i]}}L = \\delta^{[i]\\mathrm{T}}\n",
    "$$\n",
    "also to save time and space, we would have\n",
    "$$\n",
    "\\delta^{[i]} = \\delta^{[i+1]} \n",
    "    * \\left(w^{[i+1]\\mathrm{T}}\\right)_{N_{i+1}\\times N_{i}}\n",
    "    .* \\left(\\nabla\\sigma(z^{[i]\\mathrm{T}}_j)\\right)_{1\\times N_{i}}\n",
    "$$\n",
    "So every time we get $z^{[i]}$, we could send it to calculate $w^{[i+1]\\mathrm{T}}.* \\nabla\\sigma(z^{[i]\\mathrm{T}}_j)$ and store it for the backpropagation. Note that \n",
    "$$A_{1\\times N}*B_{N\\times M}.*C_{1\\times M} = A*(B.*C)$$\n",
    "According to BGD, we need to calculate different $z^{[i](j)}$ with input $x^{(j)}$, and could we simplify it in matrix form rather than put then in a for-loop? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's think about $\\nabla_{w^{[1]}}L$ with $M$ samples.\n",
    "$$\\begin{align*}\n",
    "\\sum_{i=1}^M\\left(\\nabla_{w^{[1]}}L^{(i)}\\right)_{N\\times3}\n",
    "    &=  \\sum_{i=1}^M\\left\\{\\left(\\nabla L(a^{[3]\\mathrm{T}}_M)\\right)_{M\\times2} .*\n",
    "       \\left(\\nabla\\sigma(z^{[3]\\mathrm{T}}_M)\\right)_{M\\times2} *\n",
    "       \\left(w^{[3]\\mathrm{T}}\\right)_{2\\times4} .*\n",
    "       \\left(\\nabla\\sigma(z^{[2]\\mathrm{T}}_M)\\right)_{M\\times4} *\n",
    "       \\left(w^{[2]\\mathrm{T}}\\right)_{4\\times3} .*\n",
    "       \\left(\\nabla\\sigma(z^{[1]\\mathrm{T}}_M)\\right)_{M\\times3}\\right\\}_i .*\n",
    "       \\left[x^{(i)}\\right]_{N\\times 1} \\\\\n",
    "    &= X_{N\\times M}\n",
    "       \\left[\\left(\\nabla L(a^{[3]\\mathrm{T}}_M\\mathrm{T})\\right)_{M\\times2} .*\n",
    "       \\left(\\nabla\\sigma(z^{[3]\\mathrm{T}}_M)\\right)_{M\\times2} *\n",
    "       \\left(w^{[3]\\mathrm{T}}\\right)_{2\\times4} .*\n",
    "       \\left(\\nabla\\sigma(z^{[2]\\mathrm{T}}_M)\\right)_{M\\times4} *\n",
    "       \\left(w^{[2]\\mathrm{T}}\\right)_{4\\times3} .*\n",
    "       \\left(\\nabla\\sigma(z^{[1]\\mathrm{T}}_M)\\right)_{M\\times3}\\right]\\\\\n",
    "    &\\triangleq X\\delta^{[1]}_M\n",
    "\\end{align*}$$\n",
    "in the same way\n",
    "$$\n",
    "\\sum_{i=1}^M\\nabla_b^{[1]}L^{(i)} = \\delta^{[1]\\mathrm{T}}_M\\mathbf{1}_v\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\delta^{[i]}_M = \\delta^{[i+1]}_M\n",
    "    * \\left(w^{[i+1]\\mathrm{T}}\\right)_{N_{i+1}\\times N_{i}}\n",
    "    .* \\left(\\nabla\\sigma(z^{[i]})\\right)_{M\\times N_{i}}\n",
    "$$\n",
    "Thanks to broadcasting, it only needs a little difference to apply on matrix in BGD.\n",
    "However, in the matrix form, we could not have $A*B.*C = A*(B.*C)$, for $B_{P\\times Q}.*C_{M\\times Q}$ is not defined in broadcasting. So we could only calculate it in its origin order.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "Before we practice with some examples, we first have some preparations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Main.NeuralNetwork"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Logging, LinearAlgebra\n",
    "include(\"7_neural_network.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "backpropagation(X, Y; <keyword arguments>, max_iter=1e3, α=0.1)\n",
       "\\end{verbatim}\n",
       "Backpropagation NN. The accessible activation functions are \\texttt{logistic}, \\texttt{ReLU} and \\texttt{tanh} which could\n",
       "\n",
       "be inspected by NeuralNetwork.ActivationFunction.\n",
       "\n",
       "...\n",
       "\n",
       "\\section{Arguments}\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{X::Array}: samples\n",
       "\n",
       "\n",
       "\\item \\texttt{Y::Array}: labels\n",
       "\n",
       "\n",
       "\\item \\texttt{loss::Function}: loss function\n",
       "\n",
       "\n",
       "\\item \\texttt{deri\\_loss::Function}: derivative of loss function\n",
       "\n",
       "\n",
       "\\item \\texttt{activations::Array\\{ActivationFunction\\}}: arrays that contains activation function types for every layer. \n",
       "\n",
       "\n",
       "\\item \\texttt{nodes::Array}: number of nodes in every layer\n",
       "\n",
       "\n",
       "\\item \\texttt{max\\_iter::Number}: maximum time to iterate, default is 1e3\n",
       "\n",
       "\n",
       "\\item \\texttt{α::AbstractFloat}: learning rate, default is 0.001\n",
       "\n",
       "\\end{itemize}\n",
       "...\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "backpropagation(X, Y; <keyword arguments>, max_iter=1e3, α=0.1)\n",
       "```\n",
       "\n",
       "Backpropagation NN. The accessible activation functions are `logistic`, `ReLU` and `tanh` which could\n",
       "\n",
       "be inspected by NeuralNetwork.ActivationFunction.\n",
       "\n",
       "...\n",
       "\n",
       "# Arguments\n",
       "\n",
       "  * `X::Array`: samples\n",
       "  * `Y::Array`: labels\n",
       "  * `loss::Function`: loss function\n",
       "  * `deri_loss::Function`: derivative of loss function\n",
       "  * `activations::Array{ActivationFunction}`: arrays that contains activation function types for every layer.\n",
       "  * `nodes::Array`: number of nodes in every layer\n",
       "  * `max_iter::Number`: maximum time to iterate, default is 1e3\n",
       "  * `α::AbstractFloat`: learning rate, default is 0.001\n",
       "\n",
       "...\n"
      ],
      "text/plain": [
       "\u001b[36m  backpropagation(X, Y; <keyword arguments>, max_iter=1e3, α=0.1)\u001b[39m\n",
       "\n",
       "  Backpropagation NN. The accessible activation functions are \u001b[36mlogistic\u001b[39m, \u001b[36mReLU\u001b[39m\n",
       "  and \u001b[36mtanh\u001b[39m which could\n",
       "\n",
       "  be inspected by NeuralNetwork.ActivationFunction.\n",
       "\n",
       "  ...\n",
       "\n",
       "\u001b[1m  Arguments\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "    •    \u001b[36mX::Array\u001b[39m: samples\n",
       "\n",
       "    •    \u001b[36mY::Array\u001b[39m: labels\n",
       "\n",
       "    •    \u001b[36mloss::Function\u001b[39m: loss function\n",
       "\n",
       "    •    \u001b[36mderi_loss::Function\u001b[39m: derivative of loss function\n",
       "\n",
       "    •    \u001b[36mactivations::Array{ActivationFunction}\u001b[39m: arrays that contains\n",
       "        activation function types for every layer. \n",
       "\n",
       "    •    \u001b[36mnodes::Array\u001b[39m: number of nodes in every layer\n",
       "\n",
       "    •    \u001b[36mmax_iter::Number\u001b[39m: maximum time to iterate, default is 1e3\n",
       "\n",
       "    •    \u001b[36mα::AbstractFloat\u001b[39m: learning rate, default is 0.001\n",
       "\n",
       "  ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?NeuralNetwork.backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in every example, we need to define specific loss function and its derivative "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "Linear regression model is a $N\\times1$ NN whose activation functions are all identity function. Let us take a 3 dimension vector as the input\n",
    "$$\n",
    "y = w_1x_1 + w_2x_2 + w_3x_3 + b\n",
    "$$\n",
    "Then generate 1000 data with a random error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of layers is 1\n",
      "number of traning data is 1000\n",
      "scale of NN is 3 × [1]\n",
      "Resnorm at 1: 9560.50545080532\n",
      "Resnorm at 2: 64.60764673927905\n",
      "Resnorm at 3: 2.8060144731075893\n",
      "Resnorm at 4: 1.5958101620107012\n",
      "Resnorm at 5: 1.5574988576841315\n",
      "Resnorm at 6: 1.5552983191117025\n",
      "Resnorm at 7: 1.5543029380439062\n",
      "Resnorm at 8: 1.5533505574829867\n",
      "Resnorm at 9: 1.5524015031283307\n",
      "epochs: 10/10\t\n",
      "Resnorm at 10: 1.5514543841805197\n",
      "w_hat is Any[[0.9957731767289185; 1.9995343415420945; 3.9997233386018602]], b_hat is Any[[-0.00252603595689186]]\n",
      "The mean error of model is 1.5505091469849557\n"
     ]
    }
   ],
   "source": [
    "with_logger(SimpleLogger(stdout, Logging.Info)) do\n",
    "    w = [1;2;4]\n",
    "    b = -1\n",
    "    X = (rand(3, 1000) .- 0.5) * 100  # rand from -50 to 50\n",
    "    Y = w'*X .+ b + (rand(1, 1000).-0.5)*5  # with error from -2.5 to 2.5\n",
    "\n",
    "    local loss(y_hat, y) = (y_hat - y).^2 / 2  # scalar\n",
    "    local deri_loss(y_hat, y) = y_hat - y\n",
    "\n",
    "    nodes = [1]\n",
    "    activations = [NeuralNetwork.Linear]\n",
    "\n",
    "    local model, Y_hat, w_hat, b_hat, = NeuralNetwork.backpropagation(\n",
    "        X, Y, \n",
    "        nodes=nodes, \n",
    "        activations=activations, \n",
    "        loss=loss, \n",
    "        deri_loss=deri_loss, \n",
    "        max_iter=10, \n",
    "        α=0.001\n",
    "    )\n",
    "    \n",
    "    # calculate loss\n",
    "    println(\"w_hat is $w_hat, b_hat is $b_hat\")\n",
    "    print(\"The mean error of model is \")\n",
    "    [loss(model(X[:,idx])[1], Y[idx]) for idx in 1:1000] |> l -> sum(l)/1000 |> println  # final loss\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mnist \n",
    "\n",
    "First we download and decompress the [mnist](http://yann.lecun.com/exdb/mnist/) data, then load these file with the format mentioned in mnist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start reading images. Read 28×28 data 60000 times.\n",
      "Start reading labels. Read 60000 data.\n"
     ]
    }
   ],
   "source": [
    "images = open(\"data/mnist/train-images-idx3-ubyte\", \"r\") do f\n",
    "    if hton(read(f, UInt32)) != 2051  # magic number\n",
    "        println(\"image file format is not legal\")\n",
    "        return\n",
    "    end\n",
    "    training_num = read(f, UInt32) |> hton  # big endian\n",
    "    row_size = read(f, UInt32) |> hton\n",
    "    column_size = read(f, UInt32) |> hton\n",
    "    sample_size = row_size * column_size  # N features\n",
    "    println(\"Start reading images. Read $(row_size)×$(column_size) data $(training_num) times.\")\n",
    "    # fill data\n",
    "    images = fill(Float64(0.0), (sample_size, training_num))  # [x1, x2, ..., xM]\n",
    "    for idx = 1:training_num\n",
    "        images[:, idx] = read(f, sample_size) |> Array{Float64}\n",
    "    end\n",
    "    return images\n",
    "end\n",
    "\n",
    "labels = open(\"data/mnist/train-labels-idx1-ubyte\", \"r\") do f\n",
    "    if hton(read(f, UInt32)) != 2049  # magic number\n",
    "        println(\"label file format is not legal\")\n",
    "        return\n",
    "    end\n",
    "    labels_num = read(f, UInt32) |> hton  # big endian\n",
    "    println(\"Start reading labels. Read $labels_num data.\")\n",
    "    # fill labels\n",
    "    labels = fill(Float64(0.0), (10, labels_num))\n",
    "    idx = 1\n",
    "    for label in read(f, labels_num)\n",
    "        labels[label+1, idx] = 1\n",
    "        idx += 1\n",
    "    end\n",
    "    return labels\n",
    "end\n",
    "\n",
    "# to [0, 1]\n",
    "images = images / 255;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could use what we have done before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this module, we have to provide activation functions and loss function, which we found from the documentation. Here we think aboud constructing a $784\\times100\\times50\\times10$ NN, and each neuron uses logistic activation function. And softmax is choosen as the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of layers is 3\n",
      "number of traning data is 60000\n",
      "scale of NN is 784 × [100 50 10]\n",
      "Resnorm at 1: 0.8676755449787792\n",
      "Resnorm at 2: 0.4411440794673859\n",
      "Resnorm at 3: 0.41090724636433346\n",
      "Resnorm at 4: 0.3807820987581197\n",
      "Resnorm at 5: 0.35742137879248437\n",
      "Resnorm at 6: 0.3375827938691628\n",
      "Resnorm at 7: 0.31950003399724014\n",
      "Resnorm at 8: 0.3018175490923727\n",
      "Resnorm at 9: 0.2826567865212287\n",
      "epochs: 10/1000\t\n",
      "Resnorm at 10: 0.2651370656734727\n",
      "Resnorm at 11: 0.2504153258846294\n",
      "Resnorm at 12: 0.23740317585177043\n",
      "Resnorm at 13: 0.22523841242568735\n",
      "Resnorm at 14: 0.21376324968790156\n",
      "Resnorm at 15: 0.20253744226588333\n",
      "Resnorm at 16: 0.19192839577101933\n",
      "Resnorm at 17: 0.18256343867183195\n",
      "Resnorm at 18: 0.17416594616411396\n",
      "Resnorm at 19: 0.1663185324063261\n",
      "epochs: 20/1000\t\n",
      "Resnorm at 20: 0.15892204904786147\n",
      "Resnorm at 21: 0.1521140152051816\n",
      "Resnorm at 22: 0.14595369547498654\n",
      "Resnorm at 23: 0.1404041963601209\n",
      "Resnorm at 24: 0.13539758356331338\n",
      "Resnorm at 25: 0.13085920694192477\n",
      "Resnorm at 26: 0.12672184748251888\n",
      "Resnorm at 27: 0.12292655959660119\n",
      "Resnorm at 28: 0.11942341954996963\n",
      "Resnorm at 29: 0.11617423701982484\n",
      "epochs: 30/1000\t\n",
      "Resnorm at 30: 0.11315159679418575\n",
      "Resnorm at 31: 0.11033524628506772\n",
      "Resnorm at 32: 0.10770860180154049\n",
      "Resnorm at 33: 0.10525648156702579\n",
      "Resnorm at 34: 0.10296408391004482\n",
      "Resnorm at 35: 0.10081678670092754\n",
      "Resnorm at 36: 0.0988003008189856\n",
      "Resnorm at 37: 0.096900931090126\n",
      "Resnorm at 38: 0.0951058604798708\n",
      "Resnorm at 39: 0.09340338859661902\n",
      "epochs: 40/1000\t\n",
      "Resnorm at 40: 0.09178306959512626\n",
      "Resnorm at 41: 0.09023574718953682\n",
      "Resnorm at 42: 0.0887535195698037\n",
      "Resnorm at 43: 0.08732966689650797\n",
      "Resnorm at 44: 0.0859585589732004\n",
      "Resnorm at 45: 0.08463554823720418\n",
      "Resnorm at 46: 0.0833568491509346\n",
      "Resnorm at 47: 0.08211940824426146\n",
      "Resnorm at 48: 0.08092077440559656\n",
      "Resnorm at 49: 0.0797589806761436\n",
      "epochs: 50/1000\t\n",
      "Resnorm at 50: 0.07863244469596026\n",
      "Resnorm at 51: 0.07753988803223312\n",
      "Resnorm at 52: 0.07648026907304002\n",
      "Resnorm at 53: 0.07545272214777461\n",
      "Resnorm at 54: 0.07445649746531798\n",
      "Resnorm at 55: 0.07349090170112403\n",
      "Resnorm at 56: 0.07255524486304475\n",
      "Resnorm at 57: 0.07164880120374333\n",
      "Resnorm at 58: 0.07077078880420283\n",
      "Resnorm at 59: 0.06992036695862575\n",
      "epochs: 60/1000\t\n",
      "Resnorm at 60: 0.06909664660751436\n",
      "Resnorm at 61: 0.06829870805423596\n",
      "Resnorm at 62: 0.06752562093943765\n",
      "Resnorm at 63: 0.06677646262515542\n",
      "Resnorm at 64: 0.066050332328486\n",
      "Resnorm at 65: 0.06534635966530708\n",
      "Resnorm at 66: 0.06466370770927507\n",
      "Resnorm at 67: 0.06400157188595387\n",
      "Resnorm at 68: 0.06335917660710393\n",
      "Resnorm at 69: 0.06273577141580233\n",
      "epochs: 70/1000\t\n",
      "Resnorm at 70: 0.06213062781606977\n",
      "Resnorm at 71: 0.06154303727475437\n",
      "Resnorm at 72: 0.060972310369336674\n",
      "Resnorm at 73: 0.060417776790602784\n",
      "Resnorm at 74: 0.059878785846169506\n",
      "Resnorm at 75: 0.05935470716278475\n",
      "Resnorm at 76: 0.05884493137873906\n",
      "Resnorm at 77: 0.05834887070686625\n",
      "Resnorm at 78: 0.05786595931093077\n",
      "Resnorm at 79: 0.05739565346788292\n",
      "epochs: 80/1000\t\n",
      "Resnorm at 80: 0.05693743149342498\n",
      "Resnorm at 81: 0.05649079340710514\n",
      "Resnorm at 82: 0.05605526032667022\n",
      "Resnorm at 83: 0.055630373619991354\n",
      "Resnorm at 84: 0.055215693896532594\n",
      "Resnorm at 85: 0.054810799962248864\n",
      "Resnorm at 86: 0.05441528786606413\n",
      "Resnorm at 87: 0.05402877012709365\n",
      "Resnorm at 88: 0.053650875169018494\n",
      "Resnorm at 89: 0.053281246932252974\n",
      "epochs: 90/1000\t\n",
      "Resnorm at 90: 0.052919544607552056\n",
      "Resnorm at 91: 0.05256544243862616\n",
      "Resnorm at 92: 0.05221862956275245\n",
      "Resnorm at 93: 0.05187880988040713\n",
      "Resnorm at 94: 0.05154570195715102\n",
      "Resnorm at 95: 0.051219038962111406\n",
      "Resnorm at 96: 0.050898568642067564\n",
      "Resnorm at 97: 0.05058405332388807\n",
      "Resnorm at 98: 0.05027526993379192\n",
      "Resnorm at 99: 0.04997201001920408\n",
      "epochs: 100/1000\t\n",
      "Resnorm at 100: 0.049674079755540154\n",
      "Resnorm at 101: 0.04938129991410922\n",
      "Resnorm at 102: 0.04909350575853931\n",
      "Resnorm at 103: 0.048810546828245864\n",
      "Resnorm at 104: 0.04853228656253097\n",
      "Resnorm at 105: 0.04825860172169853\n",
      "Resnorm at 106: 0.047989381573925495\n",
      "Resnorm at 107: 0.04772452683758834\n",
      "Resnorm at 108: 0.047463948394855494\n",
      "Resnorm at 109: 0.047207565818750886\n",
      "epochs: 110/1000\t\n",
      "Resnorm at 110: 0.04695530577769575\n",
      "Resnorm at 111: 0.0467071003950701\n",
      "Resnorm at 112: 0.04646288564490488\n",
      "Resnorm at 113: 0.04622259985885419\n",
      "Resnorm at 114: 0.04598618240625137\n",
      "Resnorm at 115: 0.045753572591349545\n",
      "Resnorm at 116: 0.04552470879279114\n",
      "Resnorm at 117: 0.04529952785228218\n",
      "Resnorm at 118: 0.04507796470382906\n",
      "Resnorm at 119: 0.04485995222244912\n",
      "epochs: 120/1000\t\n",
      "Resnorm at 120: 0.044645421262239623\n",
      "Resnorm at 121: 0.04443430084808151\n",
      "Resnorm at 122: 0.04422651848289614\n",
      "Resnorm at 123: 0.04402200053293738\n",
      "Resnorm at 124: 0.04382067265658132\n",
      "Resnorm at 125: 0.0436224602468186\n",
      "Resnorm at 126: 0.04342728886342833\n",
      "Resnorm at 127: 0.04323508463691023\n",
      "Resnorm at 128: 0.0430457746320793\n",
      "Resnorm at 129: 0.042859287164357524\n",
      "epochs: 130/1000\t\n",
      "Resnorm at 130: 0.04267555206599074\n",
      "Resnorm at 131: 0.04249450090260278\n",
      "Resnorm at 132: 0.04231606714272676\n",
      "Resnorm at 133: 0.042140186284357964\n",
      "Resnorm at 134: 0.04196679594332204\n",
      "Resnorm at 135: 0.041795835908515294\n",
      "Resnorm at 136: 0.04162724816900048\n",
      "Resnorm at 137: 0.041460976917649765\n",
      "Resnorm at 138: 0.04129696853560317\n",
      "Resnorm at 139: 0.04113517156131027\n",
      "epochs: 140/1000\t\n",
      "Resnorm at 140: 0.04097553664737404\n",
      "Resnorm at 141: 0.04081801650783083\n",
      "Resnorm at 142: 0.040662565857881534\n",
      "Resnorm at 143: 0.04050914134744399\n",
      "Resnorm at 144: 0.040357701489245335\n",
      "Resnorm at 145: 0.04020820658155943\n",
      "Resnorm at 146: 0.04006061862518158\n",
      "Resnorm at 147: 0.039914901233903066\n",
      "Resnorm at 148: 0.039771019537678275\n",
      "Resnorm at 149: 0.03962894007792934\n",
      "epochs: 150/1000\t\n",
      "Resnorm at 150: 0.039488630695019906\n",
      "Resnorm at 151: 0.03935006040881244\n",
      "Resnorm at 152: 0.03921319929428527\n",
      "Resnorm at 153: 0.039078018355249676\n",
      "Resnorm at 154: 0.03894448940004426\n",
      "Resnorm at 155: 0.03881258492345978\n",
      "Resnorm at 156: 0.03868227799888154\n",
      "Resnorm at 157: 0.03855354218365775\n",
      "Resnorm at 158: 0.0384263514391097\n",
      "Resnorm at 159: 0.03830068006465447\n",
      "epochs: 160/1000\t\n",
      "Resnorm at 160: 0.03817650264360314\n",
      "Resnorm at 161: 0.03805379399673667\n",
      "Resnorm at 162: 0.0379325291390733\n",
      "Resnorm at 163: 0.03781268323546606\n",
      "Resnorm at 164: 0.037694231551730975\n",
      "Resnorm at 165: 0.03757714939964801\n",
      "Resnorm at 166: 0.03746141207603089\n",
      "Resnorm at 167: 0.03734699479777266\n",
      "Resnorm at 168: 0.03723387263605582\n",
      "Resnorm at 169: 0.037122020453623096\n",
      "epochs: 170/1000\t\n",
      "Resnorm at 170: 0.0370114128491126\n",
      "Resnorm at 171: 0.036902024112056904\n",
      "Resnorm at 172: 0.036793828191370476\n",
      "Resnorm at 173: 0.036686798679173134\n",
      "Resnorm at 174: 0.036580908810769926\n",
      "Resnorm at 175: 0.036476131480656135\n",
      "Resnorm at 176: 0.03637243927362393\n",
      "Resnorm at 177: 0.03626980450946135\n",
      "Resnorm at 178: 0.03616819929936955\n",
      "Resnorm at 179: 0.03606759561206688\n",
      "epochs: 180/1000\t\n",
      "Resnorm at 180: 0.035967965347569436\n",
      "Resnorm at 181: 0.03586928041679556\n",
      "Resnorm at 182: 0.0357715128253907\n",
      "Resnorm at 183: 0.035674634760465264\n",
      "Resnorm at 184: 0.03557861867923862\n",
      "Resnorm at 185: 0.03548343739885484\n",
      "Resnorm at 186: 0.035389064186854134\n",
      "Resnorm at 187: 0.03529547285192911\n",
      "Resnorm at 188: 0.03520263783466186\n",
      "Resnorm at 189: 0.03511053429792325\n",
      "epochs: 190/1000\t\n",
      "Resnorm at 190: 0.03501913821652836\n",
      "Resnorm at 191: 0.03492842646559504\n",
      "Resnorm at 192: 0.03483837690686349\n",
      "Resnorm at 193: 0.03474896847202647\n",
      "Resnorm at 194: 0.03466018124191355\n",
      "Resnorm at 195: 0.034571996520197254\n",
      "Resnorm at 196: 0.03448439690016159\n",
      "Resnorm at 197: 0.03439736632301662\n",
      "Resnorm at 198: 0.034310890126270116\n",
      "Resnorm at 199: 0.03422495508078351\n",
      "epochs: 200/1000\t\n",
      "Resnorm at 200: 0.03413954941534546\n",
      "Resnorm at 201: 0.03405466282788003\n",
      "Resnorm at 202: 0.0339702864827529\n",
      "Resnorm at 203: 0.033886412994020836\n",
      "Resnorm at 204: 0.033803036394863595\n",
      "Resnorm at 205: 0.03372015209381238\n",
      "Resnorm at 206: 0.03363775681872263\n",
      "Resnorm at 207: 0.03355584854970943\n",
      "Resnorm at 208: 0.03347442644246023\n",
      "Resnorm at 209: 0.033393490743455476\n",
      "epochs: 210/1000\t\n",
      "Resnorm at 210: 0.03331304269866546\n",
      "Resnorm at 211: 0.03323308445726036\n",
      "Resnorm at 212: 0.03315361897178211\n",
      "Resnorm at 213: 0.03307464989609673\n",
      "Resnorm at 214: 0.032996181482290395\n",
      "Resnorm at 215: 0.03291821847750892\n",
      "Resnorm at 216: 0.03284076602157994\n",
      "Resnorm at 217: 0.032763829546115626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resnorm at 218: 0.03268741467567663\n",
      "Resnorm at 219: 0.032611527131493505\n",
      "epochs: 220/1000\t\n",
      "Resnorm at 220: 0.032536172638192105\n",
      "Resnorm at 221: 0.032461356833952355\n",
      "Resnorm at 222: 0.03238708518454274\n",
      "Resnorm at 223: 0.03231336290170826\n",
      "Resnorm at 224: 0.03224019486643794\n",
      "Resnorm at 225: 0.03216758555768935\n",
      "Resnorm at 226: 0.032095538987191224\n",
      "Resnorm at 227: 0.03202405864096919\n",
      "Resnorm at 228: 0.03195314742823715\n",
      "Resnorm at 229: 0.03188280763826115\n",
      "epochs: 230/1000\t\n",
      "Resnorm at 230: 0.03181304090572792\n",
      "Resnorm at 231: 0.03174384818504009\n",
      "Resnorm at 232: 0.031675229733815806\n",
      "Resnorm at 233: 0.03160718510569557\n",
      "Resnorm at 234: 0.03153971315236787\n",
      "Resnorm at 235: 0.03147281203452047\n",
      "Resnorm at 236: 0.03140647924122232\n",
      "Resnorm at 237: 0.0313407116170491\n",
      "Resnorm at 238: 0.03127550539609484\n",
      "Resnorm at 239: 0.031210856241871837\n",
      "epochs: 240/1000\t\n",
      "Resnorm at 240: 0.03114675929199464\n",
      "Resnorm at 241: 0.031083209206481433\n",
      "Resnorm at 242: 0.031020200218482875\n",
      "Resnorm at 243: 0.030957726186268686\n",
      "Resnorm at 244: 0.030895780645362084\n",
      "Resnorm at 245: 0.030834356859804395\n",
      "Resnorm at 246: 0.030773447871654334\n",
      "Resnorm at 247: 0.030713046547967113\n",
      "Resnorm at 248: 0.030653145624651355\n",
      "Resnorm at 249: 0.03059373774675909\n",
      "epochs: 250/1000\t\n",
      "Resnorm at 250: 0.03053481550491651\n",
      "Resnorm at 251: 0.030476371467744935\n",
      "Resnorm at 252: 0.03041839821024868\n",
      "Resnorm at 253: 0.030360888338252873\n",
      "Resnorm at 254: 0.030303834509060636\n",
      "Resnorm at 255: 0.03024722944856316\n",
      "Resnorm at 256: 0.03019106596507999\n",
      "Resnorm at 257: 0.03013533696023207\n",
      "Resnorm at 258: 0.030080035437159227\n",
      "Resnorm at 259: 0.03002515450639034\n",
      "epochs: 260/1000\t\n",
      "Resnorm at 260: 0.02997068738966111\n",
      "Resnorm at 261: 0.029916627421954764\n",
      "Resnorm at 262: 0.029862968052016978\n",
      "Resnorm at 263: 0.029809702841570678\n",
      "Resnorm at 264: 0.029756825463430916\n",
      "Resnorm at 265: 0.029704329698695287\n",
      "Resnorm at 266: 0.02965220943316343\n",
      "Resnorm at 267: 0.029600458653118816\n",
      "Resnorm at 268: 0.029549071440589367\n",
      "Resnorm at 269: 0.02949804196818924\n",
      "epochs: 270/1000\t\n",
      "Resnorm at 270: 0.029447364493631726\n",
      "Resnorm at 271: 0.02939703335399492\n",
      "Resnorm at 272: 0.029347042959813132\n",
      "Resnorm at 273: 0.029297387789062383\n",
      "Resnorm at 274: 0.029248062381103525\n",
      "Resnorm at 275: 0.029199061330643216\n",
      "Resnorm at 276: 0.029150379281771048\n",
      "Resnorm at 277: 0.029102010922128663\n",
      "Resnorm at 278: 0.02905395097726582\n",
      "Resnorm at 279: 0.029006194205236632\n",
      "epochs: 280/1000\t\n",
      "Resnorm at 280: 0.028958735391488067\n",
      "Resnorm at 281: 0.028911569344091267\n",
      "Resnorm at 282: 0.02886469088936434\n",
      "Resnorm at 283: 0.028818094867933085\n",
      "Resnorm at 284: 0.028771776131273286\n",
      "Resnorm at 285: 0.02872572953877466\n",
      "Resnorm at 286: 0.028679949955362223\n",
      "Resnorm at 287: 0.028634432249706018\n",
      "Resnorm at 288: 0.02858917129304405\n",
      "Resnorm at 289: 0.02854416195863641\n",
      "epochs: 290/1000\t\n",
      "Resnorm at 290: 0.028499399121861318\n",
      "Resnorm at 291: 0.028454877660954933\n",
      "Resnorm at 292: 0.02841059245838826\n",
      "Resnorm at 293: 0.02836653840286422\n",
      "Resnorm at 294: 0.028322710391908687\n",
      "Resnorm at 295: 0.028279103335018645\n",
      "Resnorm at 296: 0.02823571215732083\n",
      "Resnorm at 297: 0.028192531803684934\n",
      "Resnorm at 298: 0.028149557243226247\n",
      "Resnorm at 299: 0.028106783474124965\n",
      "epochs: 300/1000\t\n",
      "Resnorm at 300: 0.02806420552868314\n",
      "Resnorm at 301: 0.028021818478534955\n",
      "Resnorm at 302: 0.027979617439923644\n",
      "Resnorm at 303: 0.027937597578956412\n",
      "Resnorm at 304: 0.027895754116750885\n",
      "Resnorm at 305: 0.02785408233438898\n",
      "Resnorm at 306: 0.02781257757759994\n",
      "Resnorm at 307: 0.02777123526110132\n",
      "Resnorm at 308: 0.02773005087253592\n",
      "Resnorm at 309: 0.02768901997595309\n",
      "epochs: 310/1000\t\n",
      "Resnorm at 310: 0.02764813821479449\n",
      "Resnorm at 311: 0.027607401314356888\n",
      "Resnorm at 312: 0.027566805083716957\n",
      "Resnorm at 313: 0.02752634541711644\n",
      "Resnorm at 314: 0.02748601829481728\n",
      "Resnorm at 315: 0.027445819783448663\n",
      "Resnorm at 316: 0.02740574603587709\n",
      "Resnorm at 317: 0.027365793290640723\n",
      "Resnorm at 318: 0.027325957870994885\n",
      "Resnorm at 319: 0.02728623618362236\n",
      "epochs: 320/1000\t\n",
      "Resnorm at 320: 0.027246624717065016\n",
      "Resnorm at 321: 0.02720712003993537\n",
      "Resnorm at 322: 0.027167718798966744\n",
      "Resnorm at 323: 0.027128417716959445\n",
      "Resnorm at 324: 0.02708921359067726\n",
      "Resnorm at 325: 0.0270501032887448\n",
      "Resnorm at 326: 0.02701108374959076\n",
      "Resnorm at 327: 0.02697215197947719\n",
      "Resnorm at 328: 0.026933305050647727\n",
      "Resnorm at 329: 0.026894540099621802\n",
      "epochs: 330/1000\t\n",
      "Resnorm at 330: 0.026855854325655366\n",
      "Resnorm at 331: 0.02681724498938163\n",
      "Resnorm at 332: 0.026778709411640046\n",
      "Resnorm at 333: 0.026740244972495067\n",
      "Resnorm at 334: 0.02670184911044195\n",
      "Resnorm at 335: 0.026663519321791423\n",
      "Resnorm at 336: 0.026625253160221855\n",
      "Resnorm at 337: 0.02658704823648347\n",
      "Resnorm at 338: 0.026548902218237597\n",
      "Resnorm at 339: 0.026510812830011114\n",
      "epochs: 340/1000\t\n",
      "Resnorm at 340: 0.02647277785324623\n",
      "Resnorm at 341: 0.02643479512642512\n",
      "Resnorm at 342: 0.02639686254524975\n",
      "Resnorm at 343: 0.02635897806285872\n",
      "Resnorm at 344: 0.02632113969006589\n",
      "Resnorm at 345: 0.026283345495608398\n",
      "Resnorm at 346: 0.02624559360639635\n",
      "Resnorm at 347: 0.026207882207761407\n",
      "Resnorm at 348: 0.026170209543707548\n",
      "Resnorm at 349: 0.026132573917173118\n",
      "epochs: 350/1000\t\n",
      "Resnorm at 350: 0.02609497369032102\n",
      "Resnorm at 351: 0.026057407284879718\n",
      "Resnorm at 352: 0.02601987318256528\n",
      "Resnorm at 353: 0.02598236992562039\n",
      "Resnorm at 354: 0.02594489611751149\n",
      "Resnorm at 355: 0.02590745042382907\n",
      "Resnorm at 356: 0.02587003157343769\n",
      "Resnorm at 357: 0.025832638359921816\n",
      "Resnorm at 358: 0.025795269643370682\n",
      "Resnorm at 359: 0.025757924352538195\n",
      "epochs: 360/1000\t\n",
      "Resnorm at 360: 0.025720601487404925\n",
      "Resnorm at 361: 0.02568330012215571\n",
      "Resnorm at 362: 0.025646019408570427\n",
      "Resnorm at 363: 0.025608758579806195\n",
      "Resnorm at 364: 0.025571516954528143\n",
      "Resnorm at 365: 0.025534293941323682\n",
      "Resnorm at 366: 0.025497089043311796\n",
      "Resnorm at 367: 0.025459901862838974\n",
      "Resnorm at 368: 0.025422732106132894\n",
      "Resnorm at 369: 0.025385579587772388\n",
      "epochs: 370/1000\t\n",
      "Resnorm at 370: 0.02534844423482148\n",
      "Resnorm at 371: 0.025311326090473995\n",
      "Resnorm at 372: 0.025274225317059695\n",
      "Resnorm at 373: 0.02523714219827527\n",
      "Resnorm at 374: 0.02520007714052354\n",
      "Resnorm at 375: 0.025163030673270196\n",
      "Resnorm at 376: 0.02512600344835893\n",
      "Resnorm at 377: 0.025088996238260348\n",
      "Resnorm at 378: 0.025052009933265857\n",
      "Resnorm at 379: 0.0250150455376729\n",
      "epochs: 380/1000\t\n",
      "Resnorm at 380: 0.024978104165039827\n",
      "Resnorm at 381: 0.024941187032615938\n",
      "Resnorm at 382: 0.024904295455073503\n",
      "Resnorm at 383: 0.024867430837682912\n",
      "Resnorm at 384: 0.024830594669078693\n",
      "Resnorm at 385: 0.024793788513764754\n",
      "Resnorm at 386: 0.0247570140045003\n",
      "Resnorm at 387: 0.024720272834696733\n",
      "Resnorm at 388: 0.02468356675094036\n",
      "Resnorm at 389: 0.024646897545737595\n",
      "epochs: 390/1000\t\n",
      "Resnorm at 390: 0.024610267050560235\n",
      "Resnorm at 391: 0.024573677129249166\n",
      "Resnorm at 392: 0.024537129671816718\n",
      "Resnorm at 393: 0.024500626588671075\n",
      "Resnorm at 394: 0.024464169805272728\n",
      "Resnorm at 395: 0.02442776125722022\n",
      "Resnorm at 396: 0.024391402885754393\n",
      "Resnorm at 397: 0.02435509663366285\n",
      "Resnorm at 398: 0.024318844441562532\n",
      "Resnorm at 399: 0.02428264824453507\n",
      "epochs: 400/1000\t\n",
      "Resnorm at 400: 0.024246509969089215\n",
      "Resnorm at 401: 0.024210431530423897\n",
      "Resnorm at 402: 0.024174414829967017\n",
      "Resnorm at 403: 0.024138461753165818\n",
      "Resnorm at 404: 0.024102574167506705\n",
      "Resnorm at 405: 0.024066753920743256\n",
      "Resnorm at 406: 0.024031002839313545\n",
      "Resnorm at 407: 0.02399532272692788\n",
      "Resnorm at 408: 0.023959715363310125\n",
      "Resnorm at 409: 0.023924182503075708\n",
      "epochs: 410/1000\t\n",
      "Resnorm at 410: 0.023888725874729972\n",
      "Resnorm at 411: 0.02385334717977085\n",
      "Resnorm at 412: 0.02381804809187968\n",
      "Resnorm at 413: 0.023782830256184016\n",
      "Resnorm at 414: 0.0237476952885762\n",
      "Resnorm at 415: 0.023712644775071304\n",
      "Resnorm at 416: 0.023677680271188255\n",
      "Resnorm at 417: 0.02364280330133765\n",
      "Resnorm at 418: 0.023608015358200743\n",
      "Resnorm at 419: 0.02357331790208381\n",
      "epochs: 420/1000\t\n",
      "Resnorm at 420: 0.02353871236023319\n",
      "Resnorm at 421: 0.023504200126096816\n",
      "Resnorm at 422: 0.023469782558519423\n",
      "Resnorm at 423: 0.023435460980859215\n",
      "Resnorm at 424: 0.02340123668001575\n",
      "Resnorm at 425: 0.023367110905359968\n",
      "Resnorm at 426: 0.023333084867559143\n",
      "Resnorm at 427: 0.02329915973729119\n",
      "Resnorm at 428: 0.02326533664384531\n",
      "Resnorm at 429: 0.023231616673607496\n",
      "epochs: 430/1000\t\n",
      "Resnorm at 430: 0.023198000868432225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resnorm at 431: 0.02316449022390346\n",
      "Resnorm at 432: 0.023131085687491327\n",
      "Resnorm at 433: 0.023097788156612287\n",
      "Resnorm at 434: 0.023064598476603707\n",
      "Resnorm at 435: 0.023031517438625623\n",
      "Resnorm at 436: 0.02299854577750484\n",
      "Resnorm at 437: 0.022965684169537978\n",
      "Resnorm at 438: 0.022932933230272224\n",
      "Resnorm at 439: 0.02290029351228322\n",
      "epochs: 440/1000\t\n",
      "Resnorm at 440: 0.02286776550297082\n",
      "Resnorm at 441: 0.022835349622393607\n",
      "Resnorm at 442: 0.022803046221163133\n",
      "Resnorm at 443: 0.02277085557841869\n",
      "Resnorm at 444: 0.022738777899901846\n",
      "Resnorm at 445: 0.022706813316149623\n",
      "Resnorm at 446: 0.02267496188082278\n",
      "Resnorm at 447: 0.02264322356918414\n",
      "Resnorm at 448: 0.022611598276739735\n",
      "Resnorm at 449: 0.022580085818053\n",
      "epochs: 450/1000\t\n",
      "Resnorm at 450: 0.02254868592574009\n",
      "Resnorm at 451: 0.02251739824965192\n",
      "Resnorm at 452: 0.022486222356246234\n",
      "Resnorm at 453: 0.022455157728151057\n",
      "Resnorm at 454: 0.02242420376391849\n",
      "Resnorm at 455: 0.022393359777966908\n",
      "Resnorm at 456: 0.022362625000707445\n",
      "Resnorm at 457: 0.02233199857885047\n",
      "Resnorm at 458: 0.022301479575886102\n",
      "Resnorm at 459: 0.02227106697273333\n",
      "epochs: 460/1000\t\n",
      "Resnorm at 460: 0.022240759668551355\n",
      "Resnorm at 461: 0.022210556481707593\n",
      "Resnorm at 462: 0.022180456150896616\n",
      "Resnorm at 463: 0.022150457336404996\n",
      "Resnorm at 464: 0.022120558621517645\n",
      "Resnorm at 465: 0.022090758514062208\n",
      "Resnorm at 466: 0.022061055448088215\n",
      "Resnorm at 467: 0.022031447785679316\n",
      "Resnorm at 468: 0.022001933818897056\n",
      "Resnorm at 469: 0.021972511771855788\n",
      "epochs: 470/1000\t\n",
      "Resnorm at 470: 0.021943179802929244\n",
      "Resnorm at 471: 0.021913936007089487\n",
      "Resnorm at 472: 0.021884778418380574\n",
      "Resnorm at 473: 0.021855705012529498\n",
      "Resnorm at 474: 0.021826713709698044\n",
      "Resnorm at 475: 0.021797802377380406\n",
      "Resnorm at 476: 0.021768968833451988\n",
      "Resnorm at 477: 0.02174021084937661\n",
      "Resnorm at 478: 0.02171152615358018\n",
      "Resnorm at 479: 0.021682912435000458\n",
      "epochs: 480/1000\t\n",
      "Resnorm at 480: 0.02165436734682426\n",
      "Resnorm at 481: 0.021625888510424546\n",
      "Resnorm at 482: 0.02159747351951204\n",
      "Resnorm at 483: 0.021569119944516907\n",
      "Resnorm at 484: 0.021540825337217843\n",
      "Resnorm at 485: 0.021512587235636763\n",
      "Resnorm at 486: 0.021484403169218267\n",
      "Resnorm at 487: 0.021456270664313104\n",
      "Resnorm at 488: 0.021428187249984996\n",
      "Resnorm at 489: 0.02140015046415886\n",
      "epochs: 490/1000\t\n",
      "Resnorm at 490: 0.021372157860126955\n",
      "Resnorm at 491: 0.0213442070134269\n",
      "Resnorm at 492: 0.02131629552910177\n",
      "Resnorm at 493: 0.02128842104934803\n",
      "Resnorm at 494: 0.02126058126155139\n",
      "Resnorm at 495: 0.021232773906703792\n",
      "Resnorm at 496: 0.021204996788187058\n",
      "Resnorm at 497: 0.021177247780899716\n",
      "Resnorm at 498: 0.021149524840694024\n",
      "Resnorm at 499: 0.021121826014079564\n",
      "epochs: 500/1000\t\n",
      "Resnorm at 500: 0.02109414944813814\n",
      "Resnorm at 501: 0.021066493400584082\n",
      "Resnorm at 502: 0.021038856249890936\n",
      "Resnorm at 503: 0.02101123650539499\n",
      "Resnorm at 504: 0.020983632817273733\n",
      "Resnorm at 505: 0.020956043986287577\n",
      "Resnorm at 506: 0.020928468973162886\n",
      "Resnorm at 507: 0.020900906907486968\n",
      "Resnorm at 508: 0.020873357095979102\n",
      "Resnorm at 509: 0.020845819029997863\n",
      "epochs: 510/1000\t\n",
      "Resnorm at 510: 0.02081829239214402\n",
      "Resnorm at 511: 0.020790777061819293\n",
      "Resnorm at 512: 0.020763273119606594\n",
      "Resnorm at 513: 0.02073578085034561\n",
      "Resnorm at 514: 0.02070830074478874\n",
      "Resnorm at 515: 0.020680833499738496\n",
      "Resnorm at 516: 0.020653380016585527\n",
      "Resnorm at 517: 0.02062594139818854\n",
      "Resnorm at 518: 0.020598518944062363\n",
      "Resnorm at 519: 0.020571114143866975\n",
      "epochs: 520/1000\t\n",
      "Resnorm at 520: 0.02054372866921968\n",
      "Resnorm at 521: 0.02051636436388226\n",
      "Resnorm at 522: 0.020489023232405108\n",
      "Resnorm at 523: 0.020461707427339902\n",
      "Resnorm at 524: 0.020434419235160657\n",
      "Resnorm at 525: 0.020407161061058002\n",
      "Resnorm at 526: 0.020379935412795203\n",
      "Resnorm at 527: 0.020352744883831656\n",
      "Resnorm at 528: 0.02032559213593483\n",
      "Resnorm at 529: 0.020298479881510127\n",
      "epochs: 530/1000\t\n",
      "Resnorm at 530: 0.020271410865881818\n",
      "Resnorm at 531: 0.020244387849757173\n",
      "Resnorm at 532: 0.020217413592098683\n",
      "Resnorm at 533: 0.020190490833617614\n",
      "Resnorm at 534: 0.02016362228108591\n",
      "Resnorm at 535: 0.020136810592643314\n",
      "Resnorm at 536: 0.02011005836425324\n",
      "Resnorm at 537: 0.02008336811743524\n",
      "Resnorm at 538: 0.02005674228837504\n",
      "Resnorm at 539: 0.020030183218484907\n",
      "epochs: 540/1000\t\n",
      "Resnorm at 540: 0.02000369314645991\n",
      "Resnorm at 541: 0.01997727420184872\n",
      "Resnorm at 542: 0.0199509284001326\n",
      "Resnorm at 543: 0.01992465763928292\n",
      "Resnorm at 544: 0.01989846369774761\n",
      "Resnorm at 545: 0.019872348233798492\n",
      "Resnorm at 546: 0.019846312786157123\n",
      "Resnorm at 547: 0.01982035877580523\n",
      "Resnorm at 548: 0.019794487508876128\n",
      "Resnorm at 549: 0.019768700180518493\n",
      "epochs: 550/1000\t\n",
      "Resnorm at 550: 0.019742997879619317\n",
      "Resnorm at 551: 0.019717381594272797\n",
      "Resnorm at 552: 0.019691852217881486\n",
      "Resnorm at 553: 0.019666410555779584\n",
      "Resnorm at 554: 0.01964105733227145\n",
      "Resnorm at 555: 0.019615793197983507\n",
      "Resnorm at 556: 0.019590618737433497\n",
      "Resnorm at 557: 0.019565534476727358\n",
      "Resnorm at 558: 0.019540540891300306\n",
      "Resnorm at 559: 0.019515638413625632\n",
      "epochs: 560/1000\t\n",
      "Resnorm at 560: 0.019490827440821427\n",
      "Resnorm at 561: 0.01946610834209165\n",
      "Resnorm at 562: 0.01944148146594463\n",
      "Resnorm at 563: 0.019416947147137757\n",
      "Resnorm at 564: 0.019392505713302862\n",
      "Resnorm at 565: 0.01936815749121187\n",
      "Resnorm at 566: 0.019343902812647352\n",
      "Resnorm at 567: 0.0193197420198467\n",
      "Resnorm at 568: 0.019295675470493278\n",
      "Resnorm at 569: 0.019271703542231036\n",
      "epochs: 570/1000\t\n",
      "Resnorm at 570: 0.01924782663668304\n",
      "Resnorm at 571: 0.01922404518295745\n",
      "Resnorm at 572: 0.01920035964062738\n",
      "Resnorm at 573: 0.019176770502174142\n",
      "Resnorm at 574: 0.019153278294886122\n",
      "Resnorm at 575: 0.019129883582208088\n",
      "Resnorm at 576: 0.019106586964538912\n",
      "Resnorm at 577: 0.01908338907947776\n",
      "Resnorm at 578: 0.019060290601522447\n",
      "Resnorm at 579: 0.019037292241226217\n",
      "epochs: 580/1000\t\n",
      "Resnorm at 580: 0.019014394743822265\n",
      "Resnorm at 581: 0.01899159888732899\n",
      "Resnorm at 582: 0.0189689054801524\n",
      "Resnorm at 583: 0.018946315358205422\n",
      "Resnorm at 584: 0.01892382938156828\n",
      "Resnorm at 585: 0.018901448430717935\n",
      "Resnorm at 586: 0.01887917340235853\n",
      "Resnorm at 587: 0.018857005204889407\n",
      "Resnorm at 588: 0.018834944753551252\n",
      "Resnorm at 589: 0.018812992965295153\n",
      "epochs: 590/1000\t\n",
      "Resnorm at 590: 0.01879115075342322\n",
      "Resnorm at 591: 0.01876941902205356\n",
      "Resnorm at 592: 0.01874779866046514\n",
      "Resnorm at 593: 0.018726290537381274\n",
      "Resnorm at 594: 0.018704895495252546\n",
      "Resnorm at 595: 0.018683614344601363\n",
      "Resnorm at 596: 0.018662447858490753\n",
      "Resnorm at 597: 0.01864139676717964\n",
      "Resnorm at 598: 0.018620461753025234\n",
      "Resnorm at 599: 0.018599643445690407\n",
      "epochs: 600/1000\t\n",
      "Resnorm at 600: 0.01857894241771018\n",
      "Resnorm at 601: 0.018558359180466433\n",
      "Resnorm at 602: 0.018537894180613874\n",
      "Resnorm at 603: 0.018517547796993477\n",
      "Resnorm at 604: 0.018497320338061715\n",
      "Resnorm at 605: 0.018477212039855598\n",
      "Resnorm at 606: 0.01845722306450493\n",
      "Resnorm at 607: 0.018437353499294085\n",
      "Resnorm at 608: 0.01841760335626715\n",
      "Resnorm at 609: 0.018397972572361566\n",
      "epochs: 610/1000\t\n",
      "Resnorm at 610: 0.01837846101004812\n",
      "Resnorm at 611: 0.018359068458447512\n",
      "Resnorm at 612: 0.01833979463488825\n",
      "Resnorm at 613: 0.018320639186865263\n",
      "Resnorm at 614: 0.018301601694355137\n",
      "Resnorm at 615: 0.018282681672440893\n",
      "Resnorm at 616: 0.01826387857419805\n",
      "Resnorm at 617: 0.018245191793793025\n",
      "Resnorm at 618: 0.01822662066974581\n",
      "Resnorm at 619: 0.018208164488309912\n",
      "epochs: 620/1000\t\n",
      "Resnorm at 620: 0.01818982248692506\n",
      "Resnorm at 621: 0.01817159385770055\n",
      "Resnorm at 622: 0.018153477750890816\n",
      "Resnorm at 623: 0.018135473278327454\n",
      "Resnorm at 624: 0.01811757951677632\n",
      "Resnorm at 625: 0.0180997955111914\n",
      "Resnorm at 626: 0.018082120277841215\n",
      "Resnorm at 627: 0.018064552807286596\n",
      "Resnorm at 628: 0.018047092067192395\n",
      "Resnorm at 629: 0.018029737004958652\n",
      "epochs: 630/1000\t\n",
      "Resnorm at 630: 0.01801248655015965\n",
      "Resnorm at 631: 0.01799533961678232\n",
      "Resnorm at 632: 0.017978295105257554\n",
      "Resnorm at 633: 0.017961351904280717\n",
      "Resnorm at 634: 0.017944508892419473\n",
      "Resnorm at 635: 0.01792776493950935\n",
      "Resnorm at 636: 0.017911118907838693\n",
      "Resnorm at 637: 0.017894569653127215\n",
      "Resnorm at 638: 0.01787811602530247\n",
      "Resnorm at 639: 0.017861756869081282\n",
      "epochs: 640/1000\t\n",
      "Resnorm at 640: 0.01784549102436301\n",
      "Resnorm at 641: 0.017829317326443137\n",
      "Resnorm at 642: 0.017813234606056043\n",
      "Resnorm at 643: 0.017797241689255866\n",
      "Resnorm at 644: 0.0177813373971452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resnorm at 645: 0.017765520545460396\n",
      "Resnorm at 646: 0.01774978994402221\n",
      "Resnorm at 647: 0.01773414439606031\n",
      "Resnorm at 648: 0.017718582697418397\n",
      "Resnorm at 649: 0.017703103635646845\n",
      "epochs: 650/1000\t\n",
      "Resnorm at 650: 0.01768770598898835\n",
      "Resnorm at 651: 0.017672388525261443\n",
      "Resnorm at 652: 0.017657150000646347\n",
      "Resnorm at 653: 0.017641989158377995\n",
      "Resnorm at 654: 0.0176269047273507\n",
      "Resnorm at 655: 0.017611895420641635\n",
      "Resnorm at 656: 0.017596959933961782\n",
      "Resnorm at 657: 0.017582096944046603\n",
      "Resnorm at 658: 0.01756730510700391\n",
      "Resnorm at 659: 0.01755258305664164\n",
      "epochs: 660/1000\t\n",
      "Resnorm at 660: 0.01753792940280651\n",
      "Resnorm at 661: 0.017523342729771928\n",
      "Resnorm at 662: 0.01750882159472419\n",
      "Resnorm at 663: 0.0174943645264051\n",
      "Resnorm at 664: 0.0174799700239803\n",
      "Resnorm at 665: 0.017465636556211903\n",
      "Resnorm at 666: 0.01745136256102258\n",
      "Resnorm at 667: 0.017437146445545195\n",
      "Resnorm at 668: 0.0174229865867541\n",
      "Resnorm at 669: 0.017408881332774157\n",
      "epochs: 670/1000\t\n",
      "Resnorm at 670: 0.01739482900495633\n",
      "Resnorm at 671: 0.01738082790079634\n",
      "Resnorm at 672: 0.01736687629775414\n",
      "Resnorm at 673: 0.01735297245800539\n",
      "Resnorm at 674: 0.017339114634124842\n",
      "Resnorm at 675: 0.0173253010756638\n",
      "Resnorm at 676: 0.01731153003654308\n",
      "Resnorm at 677: 0.01729779978314124\n",
      "Resnorm at 678: 0.01728410860291715\n",
      "Resnorm at 679: 0.017270454813369987\n",
      "epochs: 680/1000\t\n",
      "Resnorm at 680: 0.01725683677111147\n",
      "Resnorm at 681: 0.01724325288080616\n",
      "Resnorm at 682: 0.017229701603729636\n",
      "Resnorm at 683: 0.017216181465700288\n",
      "Resnorm at 684: 0.01720269106416021\n",
      "Resnorm at 685: 0.017189229074211998\n",
      "Resnorm at 686: 0.01717579425345929\n",
      "Resnorm at 687: 0.017162385445547358\n",
      "Resnorm at 688: 0.017149001582351643\n",
      "Resnorm at 689: 0.017135641684814334\n",
      "epochs: 690/1000\t\n",
      "Resnorm at 690: 0.017122304862477608\n",
      "Resnorm at 691: 0.01710899031180566\n",
      "Resnorm at 692: 0.017095697313422135\n",
      "Resnorm at 693: 0.01708242522841552\n",
      "Resnorm at 694: 0.01706917349388124\n",
      "Resnorm at 695: 0.017055941617875627\n",
      "Resnorm at 696: 0.017042729173955423\n",
      "Resnorm at 697: 0.017029535795467354\n",
      "Resnorm at 698: 0.017016361169738016\n",
      "Resnorm at 699: 0.017003205032296114\n",
      "epochs: 700/1000\t\n",
      "Resnorm at 700: 0.016990067161238357\n",
      "Resnorm at 701: 0.016976947371829153\n",
      "Resnorm at 702: 0.016963845511403265\n",
      "Resnorm at 703: 0.016950761454620822\n",
      "Resnorm at 704: 0.016937695099106316\n",
      "Resnorm at 705: 0.016924646361488136\n",
      "Resnorm at 706: 0.016911615173841637\n",
      "Resnorm at 707: 0.016898601480529272\n",
      "Resnorm at 708: 0.016885605235422662\n",
      "Resnorm at 709: 0.01687262639948595\n",
      "epochs: 710/1000\t\n",
      "Resnorm at 710: 0.016859664938696194\n",
      "Resnorm at 711: 0.016846720822273942\n",
      "Resnorm at 712: 0.016833794021196554\n",
      "Resnorm at 713: 0.016820884506966928\n",
      "Resnorm at 714: 0.01680799225061122\n",
      "Resnorm at 715: 0.01679511722188059\n",
      "Resnorm at 716: 0.01678225938863424\n",
      "Resnorm at 717: 0.016769418716382656\n",
      "Resnorm at 718: 0.016756595167972406\n",
      "Resnorm at 719: 0.016743788703396087\n",
      "epochs: 720/1000\t\n",
      "Resnorm at 720: 0.01673099927971287\n",
      "Resnorm at 721: 0.01671822685106717\n",
      "Resnorm at 722: 0.016705471368794675\n",
      "Resnorm at 723: 0.01669273278160647\n",
      "Resnorm at 724: 0.016680011035843382\n",
      "Resnorm at 725: 0.0166673060757939\n",
      "Resnorm at 726: 0.016654617844069596\n",
      "Resnorm at 727: 0.016641946282032992\n",
      "Resnorm at 728: 0.01662929133027341\n",
      "Resnorm at 729: 0.016616652929126327\n",
      "epochs: 730/1000\t\n",
      "Resnorm at 730: 0.0166040310192322\n",
      "Resnorm at 731: 0.016591425542131034\n",
      "Resnorm at 732: 0.016578836440888294\n",
      "Resnorm at 733: 0.01656626366074836\n",
      "Resnorm at 734: 0.016553707149810973\n",
      "Resnorm at 735: 0.01654116685972612\n",
      "Resnorm at 736: 0.01652864274640247\n",
      "Resnorm at 737: 0.016516134770724146\n",
      "Resnorm at 738: 0.0165036428992704\n",
      "Resnorm at 739: 0.016491167105032303\n",
      "epochs: 740/1000\t\n",
      "Resnorm at 740: 0.016478707368120547\n",
      "Resnorm at 741: 0.01646626367645825\n",
      "Resnorm at 742: 0.01645383602645223\n",
      "Resnorm at 743: 0.016441424423636632\n",
      "Resnorm at 744: 0.01642902888328265\n",
      "Resnorm at 745: 0.016416649430967923\n",
      "Resnorm at 746: 0.016404286103100053\n",
      "Resnorm at 747: 0.016391938947388364\n",
      "Resnorm at 748: 0.016379608023259065\n",
      "Resnorm at 749: 0.016367293402208848\n",
      "epochs: 750/1000\t\n",
      "Resnorm at 750: 0.01635499516809328\n",
      "Resnorm at 751: 0.01634271341734642\n",
      "Resnorm at 752: 0.016330448259129365\n",
      "Resnorm at 753: 0.01631819981540583\n",
      "Resnorm at 754: 0.01630596822094426\n",
      "Resnorm at 755: 0.01629375362324649\n",
      "Resnorm at 756: 0.016281556182404126\n",
      "Resnorm at 757: 0.016269376070884932\n",
      "Resnorm at 758: 0.01625721347325225\n",
      "Resnorm at 759: 0.016245068585821754\n",
      "epochs: 760/1000\t\n",
      "Resnorm at 760: 0.01623294161626049\n",
      "Resnorm at 761: 0.016220832783134187\n",
      "Resnorm at 762: 0.016208742315409794\n",
      "Resnorm at 763: 0.016196670451920518\n",
      "Resnorm at 764: 0.016184617440801595\n",
      "Resnorm at 765: 0.01617258353890525\n",
      "Resnorm at 766: 0.016160569011203687\n",
      "Resnorm at 767: 0.01614857413018905\n",
      "Resnorm at 768: 0.016136599175278953\n",
      "Resnorm at 769: 0.016124644432236376\n",
      "epochs: 770/1000\t\n",
      "Resnorm at 770: 0.016112710192611412\n",
      "Resnorm at 771: 0.01610079675321257\n",
      "Resnorm at 772: 0.016088904415613274\n",
      "Resnorm at 773: 0.016077033485699038\n",
      "Resnorm at 774: 0.01606518427325879\n",
      "Resnorm at 775: 0.01605335709162277\n",
      "Resnorm at 776: 0.01604155225734757\n",
      "Resnorm at 777: 0.016029770089947673\n",
      "Resnorm at 778: 0.01601801091167088\n",
      "Resnorm at 779: 0.016006275047313983\n",
      "epochs: 780/1000\t\n",
      "Resnorm at 780: 0.015994562824073237\n",
      "Resnorm at 781: 0.015982874571423392\n",
      "Resnorm at 782: 0.015971210621017834\n",
      "Resnorm at 783: 0.015959571306601954\n",
      "Resnorm at 784: 0.01594795696393105\n",
      "Resnorm at 785: 0.015936367930684637\n",
      "Resnorm at 786: 0.01592480454636847\n",
      "Resnorm at 787: 0.015913267152196663\n",
      "Resnorm at 788: 0.01590175609094669\n",
      "Resnorm at 789: 0.015890271706781027\n",
      "epochs: 790/1000\t\n",
      "Resnorm at 790: 0.015878814345030516\n",
      "Resnorm at 791: 0.015867384351935424\n",
      "Resnorm at 792: 0.015855982074341697\n",
      "Resnorm at 793: 0.015844607859351087\n",
      "Resnorm at 794: 0.015833262053925122\n",
      "Resnorm at 795: 0.015821945004444036\n",
      "Resnorm at 796: 0.01581065705622293\n",
      "Resnorm at 797: 0.015799398552988186\n",
      "Resnorm at 798: 0.015788169836318058\n",
      "Resnorm at 799: 0.015776971245052087\n",
      "epochs: 800/1000\t\n",
      "Resnorm at 800: 0.015765803114674112\n",
      "Resnorm at 801: 0.015754665776674462\n",
      "Resnorm at 802: 0.015743559557896785\n",
      "Resnorm at 803: 0.015732484779875117\n",
      "Resnorm at 804: 0.0157214417581674\n",
      "Resnorm at 805: 0.015710430801690677\n",
      "Resnorm at 806: 0.015699452212064476\n",
      "Resnorm at 807: 0.015688506282967963\n",
      "Resnorm at 808: 0.015677593299516977\n",
      "Resnorm at 809: 0.015666713537667284\n",
      "epochs: 810/1000\t\n",
      "Resnorm at 810: 0.015655867263650116\n",
      "Resnorm at 811: 0.01564505473344701\n",
      "Resnorm at 812: 0.01563427619231028\n",
      "Resnorm at 813: 0.015623531874336592\n",
      "Resnorm at 814: 0.015612822002100561\n",
      "Resnorm at 815: 0.015602146786355969\n",
      "Resnorm at 816: 0.015591506425811742\n",
      "Resnorm at 817: 0.015580901106989951\n",
      "Resnorm at 818: 0.0155703310041725\n",
      "Resnorm at 819: 0.015559796279442614\n",
      "epochs: 820/1000\t\n",
      "Resnorm at 820: 0.01554929708282623\n",
      "Resnorm at 821: 0.01553883355253726\n",
      "Resnorm at 822: 0.015528405815328983\n",
      "Resnorm at 823: 0.015518013986952229\n",
      "Resnorm at 824: 0.015507658172718753\n",
      "Resnorm at 825: 0.015497338468166096\n",
      "Resnorm at 826: 0.015487054959817685\n",
      "Resnorm at 827: 0.015476807726029806\n",
      "Resnorm at 828: 0.015466596837914277\n",
      "Resnorm at 829: 0.015456422360323876\n",
      "epochs: 830/1000\t\n",
      "Resnorm at 830: 0.015446284352885186\n",
      "Resnorm at 831: 0.01543618287106228\n",
      "Resnorm at 832: 0.015426117967232946\n",
      "Resnorm at 833: 0.015416089691758662\n",
      "Resnorm at 834: 0.015406098094029059\n",
      "Resnorm at 835: 0.015396143223461735\n",
      "Resnorm at 836: 0.015386225130439022\n",
      "Resnorm at 837: 0.015376343867164214\n",
      "Resnorm at 838: 0.015366499488421425\n",
      "Resnorm at 839: 0.015356692052225028\n",
      "epochs: 840/1000\t\n",
      "Resnorm at 840: 0.01534692162034661\n",
      "Resnorm at 841: 0.015337188258709793\n",
      "Resnorm at 842: 0.015327492037645508\n",
      "Resnorm at 843: 0.015317833032002872\n",
      "Resnorm at 844: 0.015308211321113131\n",
      "Resnorm at 845: 0.015298626988606212\n",
      "Resnorm at 846: 0.015289080122081824\n",
      "Resnorm at 847: 0.015279570812638784\n",
      "Resnorm at 848: 0.015270099154267802\n",
      "Resnorm at 849: 0.015260665243114598\n",
      "epochs: 850/1000\t\n",
      "Resnorm at 850: 0.015251269176621183\n",
      "Resnorm at 851: 0.015241911052553975\n",
      "Resnorm at 852: 0.015232590967928117\n",
      "Resnorm at 853: 0.015223309017837432\n",
      "Resnorm at 854: 0.015214065294199695\n",
      "Resnorm at 855: 0.015204859884426507\n",
      "Resnorm at 856: 0.015195692870026407\n",
      "Resnorm at 857: 0.015186564325149641\n",
      "Resnorm at 858: 0.015177474315081341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resnorm at 859: 0.015168422894689462\n",
      "epochs: 860/1000\t\n",
      "Resnorm at 860: 0.015159410106832095\n",
      "Resnorm at 861: 0.015150435980727503\n",
      "Resnorm at 862: 0.015141500530288954\n",
      "Resnorm at 863: 0.015132603752424685\n",
      "Resnorm at 864: 0.015123745625302045\n",
      "Resnorm at 865: 0.015114926106573424\n",
      "Resnorm at 866: 0.015106145131560333\n",
      "Resnorm at 867: 0.015097402611391185\n",
      "Resnorm at 868: 0.015088698431087661\n",
      "Resnorm at 869: 0.015080032447594427\n",
      "epochs: 870/1000\t\n",
      "Resnorm at 870: 0.0150714044877477\n",
      "Resnorm at 871: 0.015062814346179499\n",
      "Resnorm at 872: 0.015054261783156985\n",
      "Resnorm at 873: 0.01504574652236002\n",
      "Resnorm at 874: 0.015037268248605786\n",
      "Resnorm at 875: 0.015028826605536049\n",
      "Resnorm at 876: 0.015020421193292851\n",
      "Resnorm at 877: 0.015012051566219457\n",
      "Resnorm at 878: 0.015003717230638276\n",
      "Resnorm at 879: 0.014995417642774093\n",
      "epochs: 880/1000\t\n",
      "Resnorm at 880: 0.014987152206909918\n",
      "Resnorm at 881: 0.014978920273883948\n",
      "Resnorm at 882: 0.014970721140057073\n",
      "Resnorm at 883: 0.01496255404690125\n",
      "Resnorm at 884: 0.01495441818137661\n",
      "Resnorm at 885: 0.014946312677276521\n",
      "Resnorm at 886: 0.01493823661772287\n",
      "Resnorm at 887: 0.014930189038982961\n",
      "Resnorm at 888: 0.014922168935753025\n",
      "Resnorm at 889: 0.014914175268007132\n",
      "epochs: 890/1000\t\n",
      "Resnorm at 890: 0.014906206969441928\n",
      "Resnorm at 891: 0.014898262957460053\n",
      "Resnorm at 892: 0.014890342144528166\n",
      "Resnorm at 893: 0.014882443450628611\n",
      "Resnorm at 894: 0.014874565816405734\n",
      "Resnorm at 895: 0.014866708216502182\n",
      "Resnorm at 896: 0.014858869672501582\n",
      "Resnorm at 897: 0.01485104926485602\n",
      "Resnorm at 898: 0.014843246143190216\n",
      "Resnorm at 899: 0.014835459534445173\n",
      "epochs: 900/1000\t\n",
      "Resnorm at 900: 0.014827688748448364\n",
      "Resnorm at 901: 0.0148199331806674\n",
      "Resnorm at 902: 0.014812192312099449\n",
      "Resnorm at 903: 0.014804465706449207\n",
      "Resnorm at 904: 0.014796753004929766\n",
      "Resnorm at 905: 0.014789053919164026\n",
      "Resnorm at 906: 0.014781368222753898\n",
      "Resnorm at 907: 0.014773695742116734\n",
      "Resnorm at 908: 0.014766036347163078\n",
      "Resnorm at 909: 0.014758389942319353\n",
      "epochs: 910/1000\t\n",
      "Resnorm at 910: 0.014750756458295702\n",
      "Resnorm at 911: 0.014743135844879705\n",
      "Resnorm at 912: 0.014735528064916669\n",
      "Resnorm at 913: 0.014727933089527342\n",
      "Resnorm at 914: 0.014720350894524383\n",
      "Resnorm at 915: 0.014712781457921499\n",
      "Resnorm at 916: 0.014705224758386547\n",
      "Resnorm at 917: 0.014697680774468029\n",
      "Resnorm at 918: 0.01469014948442075\n",
      "Resnorm at 919: 0.014682630866465624\n",
      "epochs: 920/1000\t\n",
      "Resnorm at 920: 0.014675124899336435\n",
      "Resnorm at 921: 0.014667631562989602\n",
      "Resnorm at 922: 0.014660150839376584\n",
      "Resnorm at 923: 0.014652682713202997\n",
      "Resnorm at 924: 0.014645227172619502\n",
      "Resnorm at 925: 0.01463778420980818\n",
      "Resnorm at 926: 0.014630353821443443\n",
      "Resnorm at 927: 0.014622936009018329\n",
      "Resnorm at 928: 0.014615530779036068\n",
      "Resnorm at 929: 0.014608138143073266\n",
      "epochs: 930/1000\t\n",
      "Resnorm at 930: 0.014600758117725217\n",
      "Resnorm at 931: 0.014593390724446638\n",
      "Resnorm at 932: 0.014586035989302127\n",
      "Resnorm at 933: 0.014578693942641226\n",
      "Resnorm at 934: 0.014571364618712461\n",
      "Resnorm at 935: 0.014564048055229975\n",
      "Resnorm at 936: 0.014556744292905372\n",
      "Resnorm at 937: 0.014549453374956067\n",
      "Resnorm at 938: 0.014542175346600234\n",
      "Resnorm at 939: 0.014534910254547263\n",
      "epochs: 940/1000\t\n",
      "Resnorm at 940: 0.014527658146491219\n",
      "Resnorm at 941: 0.014520419070613978\n",
      "Resnorm at 942: 0.014513193075103502\n",
      "Resnorm at 943: 0.014505980207691676\n",
      "Resnorm at 944: 0.014498780515215626\n",
      "Resnorm at 945: 0.014491594043205328\n",
      "Resnorm at 946: 0.014484420835499891\n",
      "Resnorm at 947: 0.014477260933894166\n",
      "Resnorm at 948: 0.014470114377816881\n",
      "Resnorm at 949: 0.01446298120404088\n",
      "epochs: 950/1000\t\n",
      "Resnorm at 950: 0.014455861446426028\n",
      "Resnorm at 951: 0.014448755135694347\n",
      "Resnorm at 952: 0.014441662299237204\n",
      "Resnorm at 953: 0.01443458296095381\n",
      "Resnorm at 954: 0.014427517141120044\n",
      "Resnorm at 955: 0.014420464856286381\n",
      "Resnorm at 956: 0.014413426119203756\n",
      "Resnorm at 957: 0.01440640093877553\n",
      "Resnorm at 958: 0.014399389320034086\n",
      "Resnorm at 959: 0.014392391264140261\n",
      "epochs: 960/1000\t\n",
      "Resnorm at 960: 0.014385406768403487\n",
      "Resnorm at 961: 0.014378435826320873\n",
      "Resnorm at 962: 0.014371478427633054\n",
      "Resnorm at 963: 0.014364534558394728\n",
      "Resnorm at 964: 0.014357604201057729\n",
      "Resnorm at 965: 0.01435068733456454\n",
      "Resnorm at 966: 0.014343783934450153\n",
      "Resnorm at 967: 0.014336893972950238\n",
      "Resnorm at 968: 0.014330017419113578\n",
      "Resnorm at 969: 0.014323154238917158\n",
      "epochs: 970/1000\t\n",
      "Resnorm at 970: 0.014316304395382026\n",
      "Resnorm at 971: 0.014309467848688636\n",
      "Resnorm at 972: 0.014302644556290567\n",
      "Resnorm at 973: 0.014295834473025657\n",
      "Resnorm at 974: 0.01428903755122436\n",
      "Resnorm at 975: 0.014282253740815324\n",
      "Resnorm at 976: 0.014275482989428876\n",
      "Resnorm at 977: 0.014268725242499699\n",
      "Resnorm at 978: 0.014261980443370745\n",
      "Resnorm at 979: 0.014255248533401061\n",
      "epochs: 980/1000\t\n",
      "Resnorm at 980: 0.014248529452080994\n",
      "Resnorm at 981: 0.01424182313715901\n",
      "Resnorm at 982: 0.014235129524784909\n",
      "Resnorm at 983: 0.014228448549674835\n",
      "Resnorm at 984: 0.014221780145303642\n",
      "Resnorm at 985: 0.014215124244130182\n",
      "Resnorm at 986: 0.014208480777860681\n",
      "Resnorm at 987: 0.014201849677754362\n",
      "Resnorm at 988: 0.01419523087497395\n",
      "Resnorm at 989: 0.014188624300981702\n",
      "epochs: 990/1000\t\n",
      "Resnorm at 990: 0.014182029887978731\n",
      "Resnorm at 991: 0.014175447569382334\n",
      "Resnorm at 992: 0.014168877280332103\n",
      "Resnorm at 993: 0.01416231895821227\n",
      "Resnorm at 994: 0.014155772543173368\n",
      "Resnorm at 995: 0.014149237978633829\n",
      "Resnorm at 996: 0.014142715211739052\n",
      "Resnorm at 997: 0.014136204193754894\n",
      "Resnorm at 998: 0.014129704880372436\n",
      "Resnorm at 999: 0.014123217231903294\n",
      "epochs: 1000/1000\t\n",
      "Resnorm at 1000: 0.014116741213348057\n",
      "1741.085017 seconds (596.75 M allocations: 981.386 GiB, 5.29% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Main.NeuralNetwork.var\"#model#12\"{Array{Any,1},Array{Any,1},Int64}(Core.Box(typeof(Main.NeuralNetwork.logistic)[Main.NeuralNetwork.logistic Main.NeuralNetwork.logistic Main.NeuralNetwork.logistic]), Any[[0.28070726906508575 0.0488670072855031 … -0.8517185549579772 -0.34747137446812215; 0.3103777160577161 0.6718846683875316 … 0.4393508727318658 0.10645599565371075; … ; 1.4361543099709215 1.596722464913551 … 1.504116693578706 -0.7489687054716442; 0.19477835317636225 -1.127212255080213 … 1.980177895573288 -0.7652623157204634], [0.0943947674829529 -1.5962991337960168 … 1.7856072963807323 1.4693242950177163; 0.2522137117683144 -0.6499115653235935 … 1.4400804520667407 -0.16454649701263727; … ; -1.1377676555181975 -1.7342489336089744 … -0.04287362238519732 0.489862651065689; 2.847923123158606 1.0913298087269432 … -1.0348641344228466 0.2758789364934003], [-2.867940714310707 0.740229448013407 … -0.8991871769132023 -0.6622713297746362; -0.09398000196704173 0.4632390694887859 … -1.7264316475516632 0.8262058257581193; … ; -0.6747173780842299 0.47070576163906597 … 0.8152294003116853 -0.7067228854563911; -2.08112977248538 0.06698761055489298 … -1.2677291605647318 -1.0626246069769514]], Any[[-0.17516157588652476; -0.4458515861368174; … ; 0.19174476478744856; -0.5902416671153708], [0.5140427792335815; 0.14778863183666616; … ; 0.14707613608052003; 0.13482354985189987], [-0.5400110560941414; -0.7473023429007734; … ; -0.4752372206243165; -0.632440043315439]], 3), Any[[1.780058122630293e-5; 1.3127212689293462e-5; … ; 9.800907542882705e-6; 1.642742670918796e-5], [0.9996931692792438; 0.00011573415064992606; … ; 4.886957813672819e-6; 2.38376196176175e-6], [2.1044190484654346e-7; 0.0006188646575484881; … ; 2.3912713272990794e-6; 0.003506587098823429], [2.07719851825102e-7; 0.9994423318865996; … ; 0.00021211032364001667; 3.824485377237236e-5], [1.3520938194113557e-6; 0.00011379583641042069; … ; 0.003006545693827094; 0.9978619143605962], [0.00021493806899398934; 8.310995565795678e-6; … ; 0.0001874024015597308; 0.00030379649026298944], [8.627706495556933e-7; 0.9990026601014399; … ; 0.0005722221626586966; 0.00019135638414329305], [6.185501827958673e-6; 0.00013181917614558312; … ; 0.004401651203352814; 0.0010291004689041063], [6.292484676274479e-6; 0.99653987599691; … ; 0.00740508958288021; 0.0015691061399633083], [3.676601809989032e-6; 0.0002174419798904332; … ; 5.217275744640171e-6; 0.0001695810921149872]  …  [4.08365647943641e-6; 8.841595567746503e-5; … ; 3.446228329033138e-5; 0.9991404938914821], [2.739956741445789e-6; 1.994373882957711e-5; … ; 0.0002510544945019567; 0.0006678335777979655], [1.366920623113322e-6; 0.00031197441284279637; … ; 6.897771994091421e-6; 0.9996283781102406], [0.006496262942474714; 6.679677083818272e-5; … ; 7.130389708262575e-5; 1.4555460067020639e-5], [1.2802773197385574e-7; 0.9964520949279788; … ; 1.064554026771604e-5; 2.0804172406325526e-6], [0.0001556401475890098; 0.0004738348908063536; … ; 0.9993542776983181; 7.63336810468629e-5], [4.11721038805175e-5; 0.0008056679654805053; … ; 0.005616124757323474; 0.0002985577372308229], [0.0012160776746446011; 0.0009392923265254788; … ; 1.1258073898908602e-5; 0.0005878486984144071], [0.0012710570960596484; 1.035638294297531e-6; … ; 2.0614992070411953e-6; 2.468688133820753e-7], [0.001232728418997728; 8.381230109713766e-5; … ; 0.9999216364230425; 0.020847218580369916]], Any[[0.28070726906508575 0.0488670072855031 … -0.8517185549579772 -0.34747137446812215; 0.3103777160577161 0.6718846683875316 … 0.4393508727318658 0.10645599565371075; … ; 1.4361543099709215 1.596722464913551 … 1.504116693578706 -0.7489687054716442; 0.19477835317636225 -1.127212255080213 … 1.980177895573288 -0.7652623157204634], [0.0943947674829529 -1.5962991337960168 … 1.7856072963807323 1.4693242950177163; 0.2522137117683144 -0.6499115653235935 … 1.4400804520667407 -0.16454649701263727; … ; -1.1377676555181975 -1.7342489336089744 … -0.04287362238519732 0.489862651065689; 2.847923123158606 1.0913298087269432 … -1.0348641344228466 0.2758789364934003], [-2.867940714310707 0.740229448013407 … -0.8991871769132023 -0.6622713297746362; -0.09398000196704173 0.4632390694887859 … -1.7264316475516632 0.8262058257581193; … ; -0.6747173780842299 0.47070576163906597 … 0.8152294003116853 -0.7067228854563911; -2.08112977248538 0.06698761055489298 … -1.2677291605647318 -1.0626246069769514]], Any[[-0.17516157588652476; -0.4458515861368174; … ; 0.19174476478744856; -0.5902416671153708], [0.5140427792335815; 0.14778863183666616; … ; 0.14707613608052003; 0.13482354985189987], [-0.5400110560941414; -0.7473023429007734; … ; -0.4752372206243165; -0.632440043315439]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_logger(SimpleLogger(stdout, Logging.Info)) do\n",
    "    nodes = [100 50 10]\n",
    "    activations = fill(NeuralNetwork.Logistic, (1, 3))\n",
    "\n",
    "    # local softmax(x) = exp.(x) |> x -> x / sum(x)\n",
    "    # local loss(y_hat, y) = begin\n",
    "    #     y_hat = softmax(y_hat)\n",
    "    #     -dot(y_hat, y)\n",
    "    # end\n",
    "    # local deri_loss(y_hat, y) = begin\n",
    "    #     y_hat = softmax(y_hat)\n",
    "    #     gram = - y_hat' .* y_hat\n",
    "    #     for idx = 1:length(y_hat)\n",
    "    #         gram[idx, idx] += y_hat[idx]\n",
    "    #     end\n",
    "    #     -gram * y ./ y_hat\n",
    "    # end\n",
    "    local loss(y_hat, y) = y_hat - y |> v -> dot(v, v)/2  # scalar\n",
    "    local deri_loss(y_hat, y) = y_hat - y\n",
    "    \n",
    "    @time global model, labels_hat, w, b = NeuralNetwork.backpropagation(\n",
    "        images, labels, \n",
    "        loss=loss, \n",
    "        deri_loss=deri_loss, \n",
    "        activations=activations, \n",
    "        nodes=nodes,\n",
    "        α=1,\n",
    "        max_iter=1000\n",
    "    );\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check the accurancy of the NN model we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_num(v) = findmax(v) |> v -> (v[1], v[2]-1)\n",
    "validate(X, Y, model) = [\n",
    "    reshape(model(X[:, idx]), (10)) |> findmax |> v -> v[2] == findmax(Y[:, idx])[2] for idx = 1:size(Y, 2)\n",
    "] |> sum |> s -> s/size(Y, 2);\n",
    "predict(x) = reshape(model(x), (10)) |> to_num;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurancy is 0.97545"
     ]
    }
   ],
   "source": [
    "print(\"Accurancy is \", validate(images, labels, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And using the rest part to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start reading images. Read 28×28 data 10000 times.\n",
      "Start reading labels. Read 10000 data.\n",
      "Accurancy is 0.9481"
     ]
    }
   ],
   "source": [
    "images_test = open(\"data/mnist/t10k-images-idx3-ubyte\", \"r\") do f\n",
    "    if hton(read(f, UInt32)) != 2051  # magic number\n",
    "        println(\"image file format is not legal\")\n",
    "        return\n",
    "    end\n",
    "    training_num = read(f, UInt32) |> hton  # big endian\n",
    "    row_size = read(f, UInt32) |> hton\n",
    "    column_size = read(f, UInt32) |> hton\n",
    "    sample_size = row_size * column_size  # N features\n",
    "    println(\"Start reading images. Read $(row_size)×$(column_size) data $(training_num) times.\")\n",
    "    # fill data\n",
    "    images = fill(Float64(0.0), (sample_size, training_num))  # [x1, x2, ..., xM]\n",
    "    for idx = 1:training_num\n",
    "        images[:, idx] = read(f, sample_size) |> Array{Float64}\n",
    "    end\n",
    "    return images\n",
    "end\n",
    "\n",
    "labels_test = open(\"data/mnist/t10k-labels-idx1-ubyte\", \"r\") do f\n",
    "    if hton(read(f, UInt32)) != 2049  # magic number\n",
    "        println(\"label file format is not legal\")\n",
    "        return\n",
    "    end\n",
    "    labels_num = read(f, UInt32) |> hton  # big endian\n",
    "    println(\"Start reading labels. Read $labels_num data.\")\n",
    "    # fill labels\n",
    "    labels = fill(Float64(0.0), (10, labels_num))\n",
    "    idx = 1\n",
    "    for label in read(f, labels_num)\n",
    "        labels[label+1, idx] = 1\n",
    "        idx += 1\n",
    "    end\n",
    "    return labels\n",
    "end\n",
    "\n",
    "print(\"Accurancy is \", validate(images_test / 255, labels_test, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. Because our final cost of model is aroud 0.014, and if we train it to reach 0.001, it shall perform better. Now we could draw some numbers and send it to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tbody><tr><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAANtSURBVGje7drLa5xlFAbw30yaGpM2tgiWqqiV1hqol0VVXIgK4gW8FrsQt0IXRRfu/APciTt14UYsgrhREEFBFMGFiBWKqC3e4gVsGhPFW2qSTlw88zEzadIkRfuFlznwwcx3ec97zsO5vw1d1Oj6vdB1bzO2ooU/2/ebmMdJ/GP11FzDu/8Jlc+wcaaHQ9iLfbgZ52NccBzAH/gWh/EZJnSwXzcSls9wWQyHcS+exB7Bs7K92faHCziFabyHQ/hIbHPdSFg+wyUxHMCdeBpXYYNgNyeYLWCwfVU7nscXeAZv4e/1ImH5DDcsdfNKHBD8mjiGF3G0/cEmbMMYbsJuwXMPnhL83hFca5ewfIanYXge7sYtYo+/4nm8jJlFO92EXXgEj2KL4H4Q34ld1i5h+QxPw/BS3Cfx7xTexZt68SM56u+Sz/wk+c3jGJH8Zx9+0Mlja5OwfIY9GDZwPa5t72QKb+DnFRaZwEvYiYcE/wfxNj6pW8LyGfZgOIwbxZaIL/zUyvUCfI9XxAYvETxvl5qju34sX6X1YrhF8pIB8aNHcGINix3Gh9iPjbgVr+LHOiUsn2EPhhdKPGxITvm1M9d6i2kKH+B+6QeMYYc+hv8z9WB4AUYFwzlMSu6yWmrhc4mPV0h/bqfYZrVO+SqtF8ONUucRnc+dxYITOC4YDuKy9rqVPZev0noxrPowRP+jZ7HgjNQcxJ63LmJSvkrrxfC39nWR2M6YxLWZNSw4YJnmT10Sls+wR90n8JX0XgakNnhd8s3V0jZcLDbYwi96e27lq7ReDKfxvmA3IrX+QTyLL3X87HI0irskFpIYeEzmHLVJWD7DHgxnpbd2D26TmPgwtuM1mSsdl572vNhZU/zudunRPSb+tyX1/RG9uW35Kq2/X/oNntOZS1S1+t72s6MyB54SzIekprxO+nSbpa8zLX2b8bolLJ/hkvPDIdyBJ3BD+381863mvtXvhsTOpk4MnMILMq+arFvC8hkuO8cfxNV4QHzrLvGRzSV2uSDYnRT/eUhmHZNLrFu+StcPhhWN4HJcI73UHVJ7jIj9zcqMcVz64x/LGZvlzkmVr9L1h2H3i0Nii8MyK67O18zgL8l1Vspdy1fpOWfYpz71qU996lOfVpnTNHQidUNnltRa9M5qzmyUn2Kcc4b/AlrHrs4LnIQUAAAAAElFTkSuQmCC\"></td><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAIhSURBVGje7dpJa1NRGIfxX5raSkVqwYKCUgQFRRxABedh6daVn8Jv1O8gbgVRu2lBxOJUxWGhIjghiEapti7egMVFc3NLybmv3tUNyblP/nnIGd5zWlZcrRX3y9bnGlqn5/7DwNbaH0EbS6p5z/+TNs/hThzFczwQLotKmB84vJbGW3EFlzGNp/heWsL8wNoOWziOS9iIN1gsMWF+YG2H47iISbzFI/wqMWF+YG2Hu3Gy+41f4XWpCfMDazncgBOYEnPRBXwqNWF+YC2Hk7iAUfzAPL6VmjA/sJbD/Tgs5jXv8FC1sXAgCfMD+3Y4hrNiXbGMx3hZcsL8wL4d7sCZbsNFzKo+Fg4kYX5gXw6HcAT7RD/6HnNiTCw2YX5gXw63iLnMJlFTu4cnpSfMD+zL4R6xpmiLmtptfCg9YX5gZYcjOCVq3MSafla1+tpAE+YHVna4TfSjI6IfncOzJiTMD6zs8CAOibnMV9zE5yYkzA+s5HAzzmOi+3oBd/TeKywiYX5gJYdTON398E/MiDp3IxLmB/Z02MYxMSeFj7gl+tNGJMwP7OlwAufEPu8S7oozF41JmB/Y0+EuUZtpoYPros7dmIT5gas6bOMAtvtT357R/5pwoAnzA1d1OIq9Ys+3g6t40bSE+YHDvd4cF/+7G7im+l5vMQnzA1d12MH97v20enWZgSfMD+x5RnhM1Ne+rGgwImreS3896P8574EAfwPD81PkTMD6LwAAAABJRU5ErkJg\"></td><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAMiSURBVGje7drPi5VVGAfwz73NjGMzY1hD6mQDuqi01HIn0kJw40KMNKIWLhKNCoJA/4p2LXQdBS0SSsiVFCKWiD9gGBmLFFF0qCaxkRln6s44Lp53nHvn570T9L4d5sCBc997zv3e7/nynPP8eEuqWqlqPKG2lbAs6zCKvzXeyotY869a+oClhSY043m8hlfxnND3Gn5Aj9CzsAzTB5xXw1XYhbewBW3ZgjLG8TM+xXf4p6gM0wecU8MX8AH2ogN3he39hhXCLp/BJXyM3qIyTB+wabaH63EEb+ABvsa3uIJBYY/v4jA24XVcxVgRGaYPOEPDFdiPPfgDR3Ecv1fNGcRJvIlXhE22Zc8LxzB9wBkabhbn54jQ73MMzbJwAHeEHa61pGGOgE3TP2zFGpzBCbPrB5Wq79rRWlSG6QPWaFjGkyJ2GLCwXU06RE808M/T39J8NawIP/OWmTH+9NYs9CbiikpRGaYPWKPhBH4SscIohudZ2I7ObHxP/XF++luav08zjB/rWNhtKmdzy/x658owfcCmxSxqxXY8K87RHuHHFpJh+oCL0nAzdmeLfxF36HhRGaYP2LCG3TiAl4Ufc1Lk4ArLMH3AhjRcg0MiP1PGOXyj9i4sZ/1h1nNnmD5g3Rp24328h+W4gWO4KbRdhS6sFjnyB+jDebV51PS3tDh1i+oJG/Ah3hb6DeALXMeL4lxdJ2KNyXzNGC7gIPrzZJg+4LwaLsc2ES/uQIs4N28IrVZnc8pVPzQuYsXz+Ayn1J6p6W9pcTTswjvCjtaJfNq4Kf+zlPUJoVFF1Bh7hW7fC19nej0q/S3N/z5sw058JGLAVlNaVetWETZ5W9QOL4s48VdRs5qrrp/+luZrhy/hE+zDSrXnY0X4Kf3iHYweYXOTtf376suZpr+l+dlhp6jL7xf5bMLeRkQcf1b4KH3C9v6y9F5bQQAfa/gUNmbjCaHPRXyJ08L+hs0e8xWaYfqAjzX8E18J/YZE/feE0G6hOlShGaYPWHMfduBpce8NpMIwfcCl9v9vC+ZpJieVq8YtIo5/OG1OPWdu+mbxnwM+AnLupBTDqzFCAAAAAElFTkSuQmCC\"></td><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAM/SURBVGje7dpdqKVTHAbw33v2PuaMYwyJMdMMhRkpV5SPwsWEGyQlkq8kTIlSXEluXJHccKG4oJSQJCUajTsljY98ZeQzCXMxo5nz4Thnu3jek132x5kZdZaVVbve1lr7ffazn9Za/+f/X42+1vQ99/yzrcGJ2IyNmMYB7ME3mDW+Taxgzr/a6gdsVjJpGmdhOy7GVhyPDv7E13gKrxmvY/1/aVkaNjgN17afM9BtxxZlrU628z7EHfiiNIb1A3aHDXRwHu7FZbKP/oEv8Qm+wkFciUtkfz2hRIb1Aw7UsBH9HsYFbd9uvIp38b2cg2fiGizhN+wrkWH9gAM13Igdot88XsGz+FTWImzCbThHzsRd+LFEhvUDDtTwdFzUPr+MR/FD3/gm3IUb2xfsbuftL5Fh/YADNZyTfXFWtFnWr8E23Cn6Tcu++iQ+LpVh/YAD49J14iGmsBO/42jZW3fgUhwl+j0mOh8slWH9gEO9RadvcJvELteL1yC+/gmJcw6UzLB+wKHeYlFim1txs5yRXfGEeyV+OQnn4rO2r0iG9QMOXYdT4tkfkb21Ef0WJQ5dxIJ4ip14TmKbhdIY1g84dB1Oim+fEO164ivm2+eOnImbJb7ZKn7yPYNzravGsH7AoRrO4g2cIjp9i49kz+xI7vtsXChe43zxi3tkbRbDsH7AkfnSSX/ntveL51hqv9TBcbgcD+FUfIdb8EFJDOsH7I4aXMCvA/p7cibulfzMDaLhetG8KIb1A3aP9AUbcHL7PIeZ0hjWD3hEGq7H1RLP9KQO/FNpDOsHPGwN1+Iq8Y5rJF7dhV9KY1g/4GFpuBZX4H6JSXvi89/0fx3/v6DhsbL+HpAcXCMx6/PG1/BXhWH9gIek4QZch3uwRfSbx0t4XWKa4hjWD7giDSckbrkdN4l/aMR7vIWn8XOpDOsHHKvhlNSc7pZ7NVNt/wLekdrT5yUzrB9wpIbHSOxyn+TVlifPSfzyuOS5R+VHV51h/YAj77Vtx4NSM+xIrWIfXsQzcsftUPRbFYb1Aw7VcJ3UfLeIdjNyh/QFOQPH+cBiGNYPOLJu8bbcw5iRO6Xvy/2L5V+61Dd/ub5YHMP6Af8CPoyf5PRLDe4AAAAASUVORK5C\"></td><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAALdSURBVGje7drJalRBFMbxX3c6iUmIBAPOigpO4Bx16cKVG7e+gvoALnwBn8CFTyFuRFy5VMSFKKKIIqiJEo1mMAMm6XZxbkgkuenQovdSpqCg6K5bX777Tw3nVFcsKZUl7Ya/U6p/adz/WLCy1g7NmFbQjp9lc5i+4KoMO3AUNTzDdE6/NpzHGdzCtzI5TF+wttqX+3FDsLyM1zn9tuMK9uGudYb/uOQyrOAUjuODWCfzysGs37SYk6VymL5gLsNenMUGfMTIKgMcwya8xVTZHKYvmMtwN06Ls8xzjOb02ygY1jAhf88szGH6gisyrGJA7G8/8AQzOQNsxiGx9o5aZ1gWhn04J9bRF1nNK3uwQzD8Kp91YQ7TF1yR4V4xDxt4hE+rPHxA7J11DFuPD8vAsIqT2IlxPJR/TukSZ9Ia5gTr2bI5TF9wGcMenBCxxDu8zPkr27BVMKxiHp3iLNslYsq6+D94j6GiHKYvuIxhN3YJRtMizt8imHSLdbMP/eIsM5B9V8FVEet3ZE7q4kz0EtfxpgiH6QsuYzgn5g6RO7sp9sWaYNOetatZe4PgV8G27PlZjGFQ5OjuiTxBIQ7TF1zGcAy3cVjEFv1+z3s3xLo5Y5FrBV9wX+Tj3md1UJxzxpeMn/4rLUfOuxtHROy+WXCaE9wmRSw/hYu4lA1yB9cEszn59xzpv9Li5yHB57GI7dssgq5ntSH2xQtZuy7ybCPWz6VlYbhQFpitVLrEHK2KmHBIc36FOExfsNbqg90iz00w/Cz2ydI5TF+wZYY9It9NxA9fy+owfcE/moddWXsc38vqMH3Blhl2iviwIe7tJ8rqMH3BlhlWs9oQseFkWR2mL9gyw9mszot7/mb3hoU5TF+wZYZj4s63D680vzcszGH6gi0zHBZxfRueWltcUYjD9AVbZjiCB/Lvp0rjMH3Bpr/zXuhUXdLuEGtnb9b+kn2+lt/3p/9K/7ngLy/ajAWRrgJKAAAAAElFTkSuQmCC\"></td><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAALiSURBVGje5drLi41xGAfwzzHHZdxGo9zJPQvJpVhJFtYUsnApKylSFhR/goWdNbIRssGGFMmChES5NmXILcltGOaMxfNOjuYMZ0ya3/z86u28533Peb/v9/n2PL/f8zy/kqpRqjrv9G/GoH/03P8YsNT3R8QoYw6m4Cke41sKDPMH/K2GIzCxeKuPaEdHca+MYRiNSViGjRiHK9gtdOx3hvkDlnu60YCl2I9GtAkN34k426VfM8ZiDIYX/51WnJd0j8n5mzQdDTuFbt+wAKOKH3/1U5cujTrxHc/wAMfQovacmr9J04qlQzAba7Cz+P5QxNMKPuOt0O4h7ov4+VL4bBIM8wcs/+5mO+5hqvDHFuzBc+Fj7cJX24R/dvjzyN+kaWnYNYZhcPH5Qe21SrIM8wesS8MPws/GYz5u+/v8MX+TpqnhUxFHF2Ed7hRHZSAwzB+wrhy/ETvEXFgW+d9JXBPrmY/q1zN/k6apIczAXqzFUHzCE9zCVaFni9q1mX5lmD9gr2pt07EeqzFXaEmsS2/hMM4Iv0yGYf6Ava6XDhc+uUTU1xZiVnG9VfjqWT3H1vxNmr6G1W86UtS4V2AbZuII9om1bBIM8wesa11aa1TwXtQAXmG58M+mPzw0f5MOHA2rxwxRk6vgkZgfk2GYP2CfNZyATULDVlzCl5QY5g/YJw2bsVnkGxWcws3UGOYP+NcajscWbBe94nOi5/Q+NYb5A/ZawwbMw1ZsEGvTyzgo5sLkGOYPWLeGDWLPxQqx52JZcf0CDuBGqgzzB+wxPxxaHKMwGYuxSmg3WtTaTuMQ7qq/j5G/Sftfw7LoT6wU9ZcposY2QfTxK6J/eBQnRM07aYb5A3aLpU3YJXyuseqNOvAC53Ec18VejOQZ5g/YTcM2XBR5+yTR+20Vml0UucObgcQwf8Ca8+FIET+bRb7+Cq/FHDjgGOYPWFfNu1T1ZiUxL37xa2+i1l7SJBjmD/gDW8eMJrKWQJUAAAAASUVORK5C\"></td><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAALrSURBVGje7drJj0xRFAbwX3W3pg2FkCBpQ2KKIYYEicSUEImVOTbCws7Gxr9iIYYgFhaCRIKFWFhYIcRCGxaChXQjZq3pbovzKopUd1cT6tXVN6nUe+/eut/76ss595xzb0FZK5Rd9/o7reEvzfsfAxb+fIofb95rYO3T/0vrT8MxWIiVQr9jeJcnhukDNv3uD1uwDDuwCu04gg95Y5g+4G/Z4WTswm6Mx3mcwH305I1h+oCD1nAW9mMnPuEwTuFlXhmmDzgoXzoXB7FFrHmHcBJv88wwfcCqNZyOA9iOLhzHaYPTryYM0wesSsPx2Cv8ZwMuCv/5uh4Ypg84oIZN2IA9Iha9J/KHZ/XCMH3AATWcjX0iFu3EGdypJ4bpA/ar4UhsxgoRwN7CZaFl3TBMH7BfDedhG0YI3S7iSdbXmH1KNbZufMsjw/QB+9SwBRsxJ7tvw03hW2eiVcQ6w/EVr/AUj8Ra+SkvDNMH7FPDqULDZmFjbSKvWC3yjJbsbUtFgm58Fhqew1k8zwPD9AEraljAEszPrj8K+9ss/GrBzwWeAoZlfctFLWCUqAG8qTXD9AErajhKxDEjs/sXwnd2ifrMG5Hbd2YTjMMUTBBaFoXeFwxp+A9aRQ3HCRtsFDXsW6Ku3Si0e4X3Yh0sYgG2YhPGCrsc0cfk6f+l+dCwKPLBgljnHuOKiDsbhX1OwmKswTrha5uzMV9wQ9hvzRmmD1hRw1LeUMi+Z2O98JNThY0uyp4Xy8b2inj0Ko6iIw8M0wesuPc0TewprRUid4p1sEnEo01+jkl7hO09FD73vLDd7jwwTB+woh124BKWivVtOCaW9ZfOzXSJuvddXMN1kVt8zBPD9AEravhZ5HijxV5FazawJ+trF1rdFvHOg+xZVx4Zpg/Y7z5+UdRkZoizT1/Efv0Lodlb1elWU4bpA1Z1FqOU01dz9jB3DNMHHGpDbajVvlXtSxvKrptFvtHzy5hq/Gz6ru2fA34Hti6GDloTdx0AAAAASUVORK5C\"></td><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAKDSURBVGje7drPb0xRGMbxz7SlreogFUGT2hEhDQmRWGAhVhIiLIl/QeIPsBdLayshLISEjUSUjRVhY0FiI36kFaSpakPH4r2TTqcV7cy09+aYNzm5N/eeOc8895vz473nltREqea8YnmiY5na/Y8F29GOduQfpaVWrv9Bpe74r0h/aFtxwdJCF8rYgLVYn5V+9GJ1zb+s4BemMYlxfMVoVsbNZ5v+I11xwa76C2VcxEHBrow1ZtnV98VKVmYEywl8wnPcwzP8yNNh+oLzGP7GWHajF51m+ZRqSkfNsSOrt1ow34o9OIRLeCD6ay4O0xdccD7swxYxnvahWzDqNJdZl+DWk9Vbh83YieHs/l1cEGNrLg7TF+xa6OIE3i6ygdq+2Sl478Zl7MWQ4NtmuGzR1WwD1fmQGIenMSW4VcQaZypPh+kLNs2wPrpxGNvEHPgC3/J0mL5gyxnuwEkxT37EU5F35OYwfcGWMuzHKbGeqQh+r/J2mL5gyxiWcACnRR8cFfnhWN4O0xdsGcNBnBPz4Awei/y+/Z5m2aMlDHtwHMdEfvEBt/G5CA7TF2wJw2HRB/vFWvS+hftgLg7TF2ya4SacFXl9CW9wC1+K4jB9waYYrsJRnMgamsRNkRMWxmH6gk0x3I7zGBDrmCe4Y+4+Re4O0xdsmGEZZ7BfjKHvcQ3viuYwfcGGGe4zmwv+xHWMiHemhXKYvmBDDAcEvyHB7CFu4HsRHaYv2BDDXTiSnb/EVYvfq0r/kRafYafYo9+I17ji77lgIRymL9hQPyzjkVjDjIg9w8I6TF9wSd8mVmNQvI8ZFTlFoR2mL7gohtVvn6rn1bXoTF2dxYyn6T/SFRf8A+LUaU8bNcMkAAAAAElFTkSuQmCC\"></td><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAANCSURBVGje7drLa11VFAbw3+21SWNDpbUBG+sTQSOFUnx2ICKiIihSdCKKgiCKk04KBTv3DxAHRaiooCCKKIiCz4oTtT4GPhHEgTaaVlu9NqVNm8TBd0pukmuSCs09bO+Cw3nte7679sd67LV2Q5s02q6nnRlZcYa++z8G7ElPetJ9aSxl0ADWYT3WVM8OYRR/Oj2/W75rW3bABTkcwmbcgKtwIQard4fwMV7EPpyoq4blA3bkcADX4j7cJFw2MVUd2u4/w+P4tK4alg941twHq3AnduByIfkvfIev8XM1bjNuw9W4C99gvI4alg84j8MrsR0j+Bvv4nV8gQM4JvFvBOfj+up6tR6HXQKcxWETW3AFJvAMdmO/2XnLaomPF1f3h/XiYRcBZ3HYQH91Po4/qvNKnJT4N4h7xNdukNzmHbTqqmH5gLM4PIlv8RsuwGO4Bj9gTHzpRXi4en8cL+A9TNZVw/IB5+Wla/AgHpV41xD7mxR/2i92OYFX8AR+rLOG5QPOy2laeE7yzBslNq7DubI+7Bd73YfXxJc2LH2dX/6Udt8O22WVxL8t2Ck5aEPi5Ljw97nkrZ/gSB01LB9w0VrbMHbhfvGh+8WPDqNPfOxPeBIvWzy3KX9Ku+9L2+Vs3F0dK4Wfp/GL1HC2VmMukzx1DG9ZOL8pf0rrw2ED1+EhyXNO4FXJQw+KD90mOerG6tgmNdTf66Rh+YD/yuEGPCA2No0vsUd6FWS9sVv43V59aARr9ThcZunIYVN6FbdW14fxLL6aM25cek9T1TgWz0/Ln9J6cLgWt+Mc4WQv3pZcpv2HW3GHxMpJfC+c1krD8gE7cjiETWJbR/E+fm37h0PSk3pE6qbTEiPfEJutlYblA3bksM9M3XQFLhWumrLmv1nW/+urMS2JlXstXnMrf0rrsT7ciKdwi/B2RGqlTanXDJipwY0Kf8+bsdVaaVg+YEc7PICXpJc4LL2mS6p302JrLXwkezE+lD5xLTUsH7AjhxN4U/i6V2LjgPQpRmWd8YHU10bN7M+opYblAy5Ya+vDedJnGpT8Zqw6Wv7bPuLyp7S3R7gnpy1L2l96Kj89dd0n+2qm5oxZil2WbxbLDvgPKremGL99AToAAAAASUVORK5C\"></td><td style='text-align:center;vertical-align:middle; margin: 0.5em;border:1px #90999f solid;border-collapse:collapse'><img style='max-width: 100px; max-height:100px;display:inline' src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAMMSURBVGje7drNaxVnFAbw3yQxqV9NLaZQq1LF0oqoUQShreJOcKEoUrqxFQRxU+gfINJ9V+5cuRAEQRciuIpY1EUa0vrRYEQRP1HUGiklWjUx18UZyZcmcxd65772hWHmzpy5zzzzcM57zpk3M2JkI44r3s5oeEv/+x4D/j/qf2RFjKbk+4EJbFrQjKeT2KXvFu8ccEINP0Q7VqIRF/AH/hlj14Yt+BIncFJoWQqG6QM2venCJ/gB2zFHiP0Ah7AfN3O7ltxud368HL24URaG6QO+VsOZ2Iaf0YqHeIJPsQvT8Svu4wvsyO1g1iQs0n+ltdcww7eGdenFXtzGBmzKt+Pow/dYlN83iE7hr6VhmD7gOA3b8B3minlvH47gOXrwm5j3bmOd8NcmUYvcwUH0l4lh+oDjNPwK3wi/6kaH0I/QtAO/YzV+EVpnIhc9iD/LxjB9wKaxP5ZiNl6IuHh/xPVGfI6N+BGLhX4VXMIBPCsbw/QBR2nYjPn5yf9wXfhXMz7DGmzF15iWP21F6HYY18rIMH3AURpmop7P8idZiPWiRlyHFSJnJfKXodz+Lk6buLavGcP0AUdpOCBylUHMwE/Cz1pHGA7gKu5hbW7bk58rJcP0AUdp+Bxn8JfwuY+FhkMi17wkcppu7BTz4yC6RJ1RSobpA47LS3uwB5tFLH0q5rmzotd2C8sM5zN9QtPJcpmaMUwfcJyGz4QvnhN5yws8FjlORfheu+jZVHARl8vMMH3A1/bahvBvvo0draI2bDbcl/m7zAzTB2yq9oa5oq/dIHpqXYrH0ZowTB+wKg0zLME84as9qoujNWGYPmBVGk7HKkwVcfSMifvbpWCYPmBVGraJOJqJur7TcC+1tAzTB6xKwwWi3qiI7/lX6oFh+oCFNWwUNeFHomY8hUf1wDB9wMIafiC+2TeIfk2XqB1LzzB9wMIatog1NgNi3dP1emGYPmBhDV+tGe7FMdG7qQuG6QMW1rAf53FU9MTrhmH6gIXWCL/6njhN5DNT8v3QGJsi6/vTf6XvHPAlyZmXHdahEqQAAAAASUVORK5C\"></td></tr></tbody></table><div><small>(a vector displayed as a row to save space)</small></div>"
      ],
      "text/plain": [
       "10-element Array{Array{RGB{Normed{UInt8,8}},2},1}:\n",
       " [RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); … ; RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0)]\n",
       " [RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); … ; RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0)]\n",
       " [RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); … ; RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0)]\n",
       " [RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); … ; RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0)]\n",
       " [RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); … ; RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0)]\n",
       " [RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); … ; RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0)]\n",
       " [RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); … ; RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0)]\n",
       " [RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); … ; RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0)]\n",
       " [RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); … ; RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0)]\n",
       " [RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); … ; RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0); RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0) … RGB{N0f8}(0.0,0.0,0.0) RGB{N0f8}(0.0,0.0,0.0)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Images, Plots\n",
    "imgs = [Images.load(\"pic/mnist/$(pic_num).bmp\") for pic_num in 0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Tuple{Tuple{Float64,Int64},Int64},1}:\n",
       " ((0.6654823831791842, 0), 0) \n",
       " ((0.993387612174138, 1), 1)  \n",
       " ((0.9858610899169266, 2), 2) \n",
       " ((0.5785984109898659, 3), 3) \n",
       " ((0.9983111879986811, 4), 4) \n",
       " ((0.348110867541219, 3), 5)  \n",
       " ((0.996911224857671, 6), 6)  \n",
       " ((0.22664225039243271, 3), 7)\n",
       " ((0.613208233014896, 3), 8)  \n",
       " ((0.9830942506038293, 3), 9) "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process the images\n",
    "img_formatted = map(imgs) do img\n",
    "    img = img .|> rgb -> dot([rgb.r, rgb.g, rgb.b], [1 1 1]) / 3\n",
    "    img = reshape(img', 28*28)\n",
    "end\n",
    "\n",
    "# start predicting\n",
    "[\n",
    "    (predict(img_formatted[idx]), idx-1) for idx in 1:length(img_formatted)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model just works well on some of these hand writting numbers, but provides completely wrong answers on others. The reason could be that samples in mnist are all well processed to be center and size adjusted, while these hand writting images are not. So our test accuracy could gets 94.8% but the model doesn't work well on these numbers above. This is one disadvantage of normal backpropagation NN, and CNN could get over it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
